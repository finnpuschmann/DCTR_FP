Configured CUDA from: /cvmfs/sft.cern.ch/lcg/releases/cuda/12.4-4899e/x86_64-el9-gcc11-opt
GPUs available for tensorflow:
[]
loading data
preparing data
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21634, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 56s - loss: 0.2197 - acc: 0.5252 - val_loss: 0.2163 - val_acc: 0.5268 - lr: 0.0010 - 56s/epoch - 257ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21634 to 0.21557, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2159 - acc: 0.5361 - val_loss: 0.2156 - val_acc: 0.5295 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21557 to 0.21491, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2151 - acc: 0.5416 - val_loss: 0.2149 - val_acc: 0.5361 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21491 to 0.21448, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2147 - acc: 0.5447 - val_loss: 0.2145 - val_acc: 0.5416 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21448 to 0.21424, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 56s - loss: 0.2144 - acc: 0.5461 - val_loss: 0.2142 - val_acc: 0.5514 - lr: 0.0010 - 56s/epoch - 255ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21424 to 0.21402, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2141 - acc: 0.5469 - val_loss: 0.2140 - val_acc: 0.5449 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21402 to 0.21390, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2140 - acc: 0.5472 - val_loss: 0.2139 - val_acc: 0.5491 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21390 to 0.21390, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 55s - loss: 0.2139 - acc: 0.5481 - val_loss: 0.2139 - val_acc: 0.5483 - lr: 0.0010 - 55s/epoch - 250ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21390 to 0.21381, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 53s - loss: 0.2138 - acc: 0.5481 - val_loss: 0.2138 - val_acc: 0.5544 - lr: 0.0010 - 53s/epoch - 244ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21381 to 0.21366, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5487 - val_loss: 0.2137 - val_acc: 0.5495 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 11/100

Epoch 11: val_loss did not improve from 0.21366
219/219 - 54s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5381 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 12/100

Epoch 12: val_loss did not improve from 0.21366
219/219 - 53s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5486 - lr: 0.0010 - 53s/epoch - 241ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21366 to 0.21365, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2136 - acc: 0.5491 - val_loss: 0.2136 - val_acc: 0.5403 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21365 to 0.21362, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2136 - acc: 0.5491 - val_loss: 0.2136 - val_acc: 0.5443 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21362
219/219 - 52s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5536 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 16/100

Epoch 16: val_loss improved from 0.21362 to 0.21355, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2135 - val_acc: 0.5479 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2138 - val_acc: 0.5505 - lr: 0.0010 - 53s/epoch - 244ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5527 - lr: 0.0010 - 53s/epoch - 244ms/step
Epoch 19/100

Epoch 19: val_loss improved from 0.21355 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2135 - val_acc: 0.5514 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21351
219/219 - 52s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5526 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 21/100

Epoch 21: val_loss improved from 0.21351 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5496 - val_loss: 0.2135 - val_acc: 0.5512 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 22/100

Epoch 22: val_loss did not improve from 0.21351

Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 53s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5523 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 23/100

Epoch 23: val_loss improved from 0.21351 to 0.21348, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 55s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5462 - lr: 6.0000e-04 - 55s/epoch - 250ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21348
219/219 - 54s - loss: 0.2134 - acc: 0.5501 - val_loss: 0.2135 - val_acc: 0.5461 - lr: 6.0000e-04 - 54s/epoch - 246ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5434 - lr: 6.0000e-04 - 53s/epoch - 243ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5472 - lr: 6.0000e-04 - 53s/epoch - 241ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21348
219/219 - 54s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5466 - lr: 6.0000e-04 - 54s/epoch - 245ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21348

Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 54s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5509 - lr: 6.0000e-04 - 54s/epoch - 245ms/step
Epoch 29/100

Epoch 29: val_loss improved from 0.21348 to 0.21347, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5494 - lr: 3.6000e-04 - 54s/epoch - 247ms/step
Epoch 30/100

Epoch 30: val_loss improved from 0.21347 to 0.21345, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 55s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5505 - lr: 3.6000e-04 - 55s/epoch - 252ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5474 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5530 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5507 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 34/100

Epoch 34: val_loss improved from 0.21345 to 0.21345, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf

Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 54s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5490 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 35/100

Epoch 35: val_loss improved from 0.21345 to 0.21344, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_1_batchsize_65536.tf
219/219 - 54s - loss: 0.2133 - acc: 0.5507 - val_loss: 0.2134 - val_acc: 0.5511 - lr: 2.1600e-04 - 54s/epoch - 247ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21344
219/219 - 54s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5481 - lr: 2.1600e-04 - 54s/epoch - 246ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21344
219/219 - 53s - loss: 0.2133 - acc: 0.5507 - val_loss: 0.2135 - val_acc: 0.5493 - lr: 2.1600e-04 - 53s/epoch - 242ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2134 - val_acc: 0.5497 - lr: 2.1600e-04 - 52s/epoch - 238ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2134 - val_acc: 0.5489 - lr: 2.1600e-04 - 52s/epoch - 240ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2134 - val_acc: 0.5496 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21344

Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 53s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2134 - val_acc: 0.5481 - lr: 2.1600e-04 - 53s/epoch - 240ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21344
219/219 - 53s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2134 - val_acc: 0.5502 - lr: 1.2960e-04 - 53s/epoch - 244ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21344
219/219 - 54s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2134 - val_acc: 0.5481 - lr: 1.2960e-04 - 54s/epoch - 246ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2134 - val_acc: 0.5488 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2134 - val_acc: 0.5491 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21344
219/219 - 52s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5519 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21344

Epoch 47: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 52s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5481 - lr: 1.2960e-04 - 52s/epoch - 240ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21344
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2134 - val_acc: 0.5478 - lr: 7.7760e-05 - 53s/epoch - 240ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21344
219/219 - 54s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5516 - lr: 7.7760e-05 - 54s/epoch - 246ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21344
Restoring model weights from the end of the best epoch: 35.
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2134 - val_acc: 0.5507 - lr: 7.7760e-05 - 53s/epoch - 240ms/step
Epoch 50: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 1, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21647, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 55s - loss: 0.2225 - acc: 0.5244 - val_loss: 0.2165 - val_acc: 0.5288 - lr: 0.0010 - 55s/epoch - 252ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21647 to 0.21547, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2159 - acc: 0.5356 - val_loss: 0.2155 - val_acc: 0.5322 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21547 to 0.21504, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2152 - acc: 0.5412 - val_loss: 0.2150 - val_acc: 0.5444 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21504 to 0.21459, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2148 - acc: 0.5446 - val_loss: 0.2146 - val_acc: 0.5520 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21459 to 0.21436, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 56s - loss: 0.2144 - acc: 0.5462 - val_loss: 0.2144 - val_acc: 0.5382 - lr: 0.0010 - 56s/epoch - 254ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21436 to 0.21413, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2142 - acc: 0.5469 - val_loss: 0.2141 - val_acc: 0.5469 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21413 to 0.21403, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2141 - acc: 0.5474 - val_loss: 0.2140 - val_acc: 0.5505 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21403 to 0.21392, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2140 - acc: 0.5481 - val_loss: 0.2139 - val_acc: 0.5460 - lr: 0.0010 - 54s/epoch - 249ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21392 to 0.21389, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2139 - acc: 0.5482 - val_loss: 0.2139 - val_acc: 0.5416 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21389 to 0.21389, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 55s - loss: 0.2138 - acc: 0.5483 - val_loss: 0.2139 - val_acc: 0.5398 - lr: 0.0010 - 55s/epoch - 250ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21389 to 0.21378, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 57s - loss: 0.2138 - acc: 0.5484 - val_loss: 0.2138 - val_acc: 0.5521 - lr: 0.0010 - 57s/epoch - 259ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21378 to 0.21371, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 56s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5478 - lr: 0.0010 - 56s/epoch - 254ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21371 to 0.21367, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5494 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 14/100

Epoch 14: val_loss did not improve from 0.21367
219/219 - 53s - loss: 0.2137 - acc: 0.5491 - val_loss: 0.2137 - val_acc: 0.5439 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21367
219/219 - 52s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2138 - val_acc: 0.5424 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.21367
219/219 - 52s - loss: 0.2136 - acc: 0.5491 - val_loss: 0.2137 - val_acc: 0.5464 - lr: 0.0010 - 52s/epoch - 238ms/step
Epoch 17/100

Epoch 17: val_loss improved from 0.21367 to 0.21361, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 55s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2136 - val_acc: 0.5483 - lr: 0.0010 - 55s/epoch - 251ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21361
219/219 - 53s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5405 - lr: 0.0010 - 53s/epoch - 243ms/step
Epoch 19/100

Epoch 19: val_loss improved from 0.21361 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf

Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 54s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2136 - val_acc: 0.5502 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 20/100

Epoch 20: val_loss improved from 0.21360 to 0.21355, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5466 - lr: 6.0000e-04 - 54s/epoch - 246ms/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.21355
219/219 - 52s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5497 - lr: 6.0000e-04 - 52s/epoch - 239ms/step
Epoch 22/100

Epoch 22: val_loss did not improve from 0.21355
219/219 - 52s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5451 - lr: 6.0000e-04 - 52s/epoch - 240ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5455 - lr: 6.0000e-04 - 53s/epoch - 241ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21355
219/219 - 54s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5507 - lr: 6.0000e-04 - 54s/epoch - 246ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5459 - lr: 6.0000e-04 - 53s/epoch - 242ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21355

Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 52s - loss: 0.2134 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5482 - lr: 6.0000e-04 - 52s/epoch - 239ms/step
Epoch 27/100

Epoch 27: val_loss improved from 0.21355 to 0.21354, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5503 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 28/100

Epoch 28: val_loss improved from 0.21354 to 0.21352, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5503 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 29/100

Epoch 29: val_loss improved from 0.21352 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5495 - lr: 3.6000e-04 - 54s/epoch - 247ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21351
219/219 - 54s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5462 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 0.21351
219/219 - 52s - loss: 0.2134 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5457 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21351

Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 52s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5455 - lr: 3.6000e-04 - 52s/epoch - 238ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21351
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5472 - lr: 2.1600e-04 - 52s/epoch - 238ms/step
Epoch 34/100

Epoch 34: val_loss improved from 0.21351 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 54s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5511 - lr: 2.1600e-04 - 54s/epoch - 246ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21351
219/219 - 53s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5469 - lr: 2.1600e-04 - 53s/epoch - 240ms/step
Epoch 36/100

Epoch 36: val_loss improved from 0.21351 to 0.21350, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_2_batchsize_65536.tf
219/219 - 55s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5496 - lr: 2.1600e-04 - 55s/epoch - 250ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21350
219/219 - 53s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5506 - lr: 2.1600e-04 - 53s/epoch - 242ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21350

Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5489 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21350
219/219 - 52s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5460 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21350
219/219 - 52s - loss: 0.2132 - acc: 0.5507 - val_loss: 0.2135 - val_acc: 0.5475 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21350
219/219 - 52s - loss: 0.2132 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5481 - lr: 1.2960e-04 - 52s/epoch - 238ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21350
219/219 - 53s - loss: 0.2132 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5529 - lr: 1.2960e-04 - 53s/epoch - 244ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21350
219/219 - 53s - loss: 0.2132 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5512 - lr: 1.2960e-04 - 53s/epoch - 244ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21350

Epoch 44: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 53s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5471 - lr: 1.2960e-04 - 53s/epoch - 243ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21350
219/219 - 54s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5489 - lr: 7.7760e-05 - 54s/epoch - 246ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21350
219/219 - 53s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5479 - lr: 7.7760e-05 - 53s/epoch - 243ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21350
219/219 - 52s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5493 - lr: 7.7760e-05 - 52s/epoch - 238ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21350
219/219 - 52s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5492 - lr: 7.7760e-05 - 52s/epoch - 238ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21350
219/219 - 54s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5486 - lr: 7.7760e-05 - 54s/epoch - 248ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21350

Epoch 50: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.
219/219 - 53s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5488 - lr: 7.7760e-05 - 53s/epoch - 240ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21350
Restoring model weights from the end of the best epoch: 36.
219/219 - 52s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5492 - lr: 4.6656e-05 - 52s/epoch - 239ms/step
Epoch 51: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 2, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21666, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2239 - acc: 0.5240 - val_loss: 0.2167 - val_acc: 0.5267 - lr: 0.0010 - 55s/epoch - 251ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21666 to 0.21560, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2161 - acc: 0.5368 - val_loss: 0.2156 - val_acc: 0.5491 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21560 to 0.21493, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2152 - acc: 0.5424 - val_loss: 0.2149 - val_acc: 0.5477 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21493 to 0.21460, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2148 - acc: 0.5450 - val_loss: 0.2146 - val_acc: 0.5423 - lr: 0.0010 - 55s/epoch - 253ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21460 to 0.21459, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2145 - acc: 0.5463 - val_loss: 0.2146 - val_acc: 0.5362 - lr: 0.0010 - 55s/epoch - 250ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21459 to 0.21427, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2143 - acc: 0.5468 - val_loss: 0.2143 - val_acc: 0.5440 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21427 to 0.21419, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2141 - acc: 0.5474 - val_loss: 0.2142 - val_acc: 0.5551 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21419 to 0.21397, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2140 - acc: 0.5479 - val_loss: 0.2140 - val_acc: 0.5464 - lr: 0.0010 - 55s/epoch - 249ms/step
Epoch 9/100

Epoch 9: val_loss did not improve from 0.21397
219/219 - 52s - loss: 0.2139 - acc: 0.5482 - val_loss: 0.2140 - val_acc: 0.5400 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21397 to 0.21388, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 56s - loss: 0.2139 - acc: 0.5483 - val_loss: 0.2139 - val_acc: 0.5474 - lr: 0.0010 - 56s/epoch - 257ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21388 to 0.21379, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2138 - acc: 0.5485 - val_loss: 0.2138 - val_acc: 0.5481 - lr: 0.0010 - 55s/epoch - 252ms/step
Epoch 12/100

Epoch 12: val_loss did not improve from 0.21379
219/219 - 53s - loss: 0.2137 - acc: 0.5485 - val_loss: 0.2138 - val_acc: 0.5427 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21379 to 0.21371, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5489 - val_loss: 0.2137 - val_acc: 0.5473 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21371 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2136 - val_acc: 0.5464 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21364
219/219 - 53s - loss: 0.2136 - acc: 0.5492 - val_loss: 0.2138 - val_acc: 0.5520 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 16/100

Epoch 16: val_loss improved from 0.21364 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 55s - loss: 0.2136 - acc: 0.5490 - val_loss: 0.2136 - val_acc: 0.5473 - lr: 0.0010 - 55s/epoch - 250ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21360
219/219 - 54s - loss: 0.2136 - acc: 0.5492 - val_loss: 0.2137 - val_acc: 0.5443 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21360
219/219 - 53s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2136 - val_acc: 0.5459 - lr: 0.0010 - 53s/epoch - 242ms/step
Epoch 19/100

Epoch 19: val_loss did not improve from 0.21360
219/219 - 52s - loss: 0.2136 - acc: 0.5492 - val_loss: 0.2137 - val_acc: 0.5528 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 20/100

Epoch 20: val_loss improved from 0.21360 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf

Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 54s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2136 - val_acc: 0.5502 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 21/100

Epoch 21: val_loss improved from 0.21358 to 0.21357, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5448 - lr: 6.0000e-04 - 54s/epoch - 248ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21357 to 0.21356, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5447 - lr: 6.0000e-04 - 54s/epoch - 248ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.21356
219/219 - 54s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5443 - lr: 6.0000e-04 - 54s/epoch - 247ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21356
219/219 - 53s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5457 - lr: 6.0000e-04 - 53s/epoch - 240ms/step
Epoch 25/100

Epoch 25: val_loss improved from 0.21356 to 0.21352, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5497 - val_loss: 0.2135 - val_acc: 0.5501 - lr: 6.0000e-04 - 54s/epoch - 248ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21352
219/219 - 53s - loss: 0.2134 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5434 - lr: 6.0000e-04 - 53s/epoch - 240ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21352
219/219 - 54s - loss: 0.2135 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5515 - lr: 6.0000e-04 - 54s/epoch - 245ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21352
219/219 - 54s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5481 - lr: 6.0000e-04 - 54s/epoch - 246ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21352
219/219 - 58s - loss: 0.2134 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5520 - lr: 6.0000e-04 - 58s/epoch - 265ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21352
219/219 - 53s - loss: 0.2134 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5469 - lr: 6.0000e-04 - 53s/epoch - 240ms/step
Epoch 31/100

Epoch 31: val_loss improved from 0.21352 to 0.21352, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf

Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 54s - loss: 0.2134 - acc: 0.5498 - val_loss: 0.2135 - val_acc: 0.5471 - lr: 6.0000e-04 - 54s/epoch - 245ms/step
Epoch 32/100

Epoch 32: val_loss improved from 0.21352 to 0.21350, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5469 - lr: 3.6000e-04 - 54s/epoch - 247ms/step
Epoch 33/100

Epoch 33: val_loss improved from 0.21350 to 0.21349, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5491 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2133 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5481 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21349
219/219 - 53s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5485 - lr: 3.6000e-04 - 53s/epoch - 243ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21349
219/219 - 53s - loss: 0.2133 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5524 - lr: 3.6000e-04 - 53s/epoch - 243ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21349

Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5495 - lr: 3.6000e-04 - 52s/epoch - 238ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2133 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5503 - lr: 2.1600e-04 - 52s/epoch - 238ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5505 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5463 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21349
219/219 - 54s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5513 - lr: 2.1600e-04 - 54s/epoch - 245ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21349
219/219 - 54s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5541 - lr: 2.1600e-04 - 54s/epoch - 246ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21349

Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 52s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5502 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 44/100

Epoch 44: val_loss improved from 0.21349 to 0.21348, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_3_batchsize_65536.tf
219/219 - 54s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5497 - lr: 1.2960e-04 - 54s/epoch - 246ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5524 - lr: 1.2960e-04 - 53s/epoch - 240ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5516 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5485 - lr: 1.2960e-04 - 52s/epoch - 238ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21348
219/219 - 54s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5502 - lr: 1.2960e-04 - 54s/epoch - 247ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21348

Epoch 49: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 53s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 1.2960e-04 - 53s/epoch - 240ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5512 - val_loss: 0.2135 - val_acc: 0.5491 - lr: 7.7760e-05 - 52s/epoch - 239ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5512 - val_loss: 0.2135 - val_acc: 0.5494 - lr: 7.7760e-05 - 52s/epoch - 239ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5513 - val_loss: 0.2135 - val_acc: 0.5478 - lr: 7.7760e-05 - 52s/epoch - 239ms/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5512 - val_loss: 0.2135 - val_acc: 0.5506 - lr: 7.7760e-05 - 52s/epoch - 239ms/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5512 - val_loss: 0.2135 - val_acc: 0.5509 - lr: 7.7760e-05 - 53s/epoch - 242ms/step
Epoch 55/100

Epoch 55: val_loss did not improve from 0.21348

Epoch 55: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.
219/219 - 53s - loss: 0.2131 - acc: 0.5513 - val_loss: 0.2135 - val_acc: 0.5501 - lr: 7.7760e-05 - 53s/epoch - 243ms/step
Epoch 56/100

Epoch 56: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2131 - acc: 0.5513 - val_loss: 0.2135 - val_acc: 0.5490 - lr: 4.6656e-05 - 53s/epoch - 240ms/step
Epoch 57/100

Epoch 57: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2131 - acc: 0.5513 - val_loss: 0.2135 - val_acc: 0.5497 - lr: 4.6656e-05 - 52s/epoch - 239ms/step
Epoch 58/100

Epoch 58: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2131 - acc: 0.5514 - val_loss: 0.2135 - val_acc: 0.5481 - lr: 4.6656e-05 - 53s/epoch - 242ms/step
Epoch 59/100

Epoch 59: val_loss did not improve from 0.21348
Restoring model weights from the end of the best epoch: 44.
219/219 - 52s - loss: 0.2131 - acc: 0.5513 - val_loss: 0.2135 - val_acc: 0.5507 - lr: 4.6656e-05 - 52s/epoch - 239ms/step
Epoch 59: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 3, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21679, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 56s - loss: 0.2214 - acc: 0.5250 - val_loss: 0.2168 - val_acc: 0.5229 - lr: 0.0010 - 56s/epoch - 258ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21679 to 0.21553, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 57s - loss: 0.2161 - acc: 0.5357 - val_loss: 0.2155 - val_acc: 0.5470 - lr: 0.0010 - 57s/epoch - 263ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21553 to 0.21478, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 55s - loss: 0.2151 - acc: 0.5431 - val_loss: 0.2148 - val_acc: 0.5400 - lr: 0.0010 - 55s/epoch - 251ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21478 to 0.21430, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2145 - acc: 0.5460 - val_loss: 0.2143 - val_acc: 0.5452 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21430 to 0.21412, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2142 - acc: 0.5473 - val_loss: 0.2141 - val_acc: 0.5478 - lr: 0.0010 - 54s/epoch - 249ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21412 to 0.21398, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2141 - acc: 0.5478 - val_loss: 0.2140 - val_acc: 0.5442 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21398 to 0.21396, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 55s - loss: 0.2139 - acc: 0.5482 - val_loss: 0.2140 - val_acc: 0.5545 - lr: 0.0010 - 55s/epoch - 249ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21396 to 0.21382, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 55s - loss: 0.2139 - acc: 0.5485 - val_loss: 0.2138 - val_acc: 0.5524 - lr: 0.0010 - 55s/epoch - 253ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21382 to 0.21379, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2138 - acc: 0.5488 - val_loss: 0.2138 - val_acc: 0.5423 - lr: 0.0010 - 54s/epoch - 245ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21379 to 0.21377, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5488 - val_loss: 0.2138 - val_acc: 0.5502 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21377 to 0.21367, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5492 - val_loss: 0.2137 - val_acc: 0.5504 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 12/100

Epoch 12: val_loss did not improve from 0.21367
219/219 - 52s - loss: 0.2137 - acc: 0.5492 - val_loss: 0.2138 - val_acc: 0.5454 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21367 to 0.21362, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5495 - val_loss: 0.2136 - val_acc: 0.5491 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 14/100

Epoch 14: val_loss did not improve from 0.21362
219/219 - 54s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2136 - val_acc: 0.5469 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21362
219/219 - 52s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2136 - val_acc: 0.5530 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 16/100

Epoch 16: val_loss improved from 0.21362 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5487 - lr: 0.0010 - 54s/epoch - 248ms/step
Epoch 17/100

Epoch 17: val_loss improved from 0.21360 to 0.21357, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2136 - acc: 0.5496 - val_loss: 0.2136 - val_acc: 0.5479 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21357
219/219 - 52s - loss: 0.2136 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5525 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 19/100

Epoch 19: val_loss did not improve from 0.21357
219/219 - 52s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5485 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21357
219/219 - 53s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5439 - lr: 0.0010 - 53s/epoch - 243ms/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.21357
219/219 - 53s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2136 - val_acc: 0.5522 - lr: 0.0010 - 53s/epoch - 243ms/step
Epoch 22/100

Epoch 22: val_loss did not improve from 0.21357
219/219 - 52s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5488 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.21357

Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 52s - loss: 0.2135 - acc: 0.5497 - val_loss: 0.2136 - val_acc: 0.5463 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 24/100

Epoch 24: val_loss improved from 0.21357 to 0.21354, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5490 - lr: 6.0000e-04 - 54s/epoch - 247ms/step
Epoch 25/100

Epoch 25: val_loss improved from 0.21354 to 0.21353, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5526 - lr: 6.0000e-04 - 54s/epoch - 247ms/step
Epoch 26/100

Epoch 26: val_loss improved from 0.21353 to 0.21353, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 55s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5472 - lr: 6.0000e-04 - 55s/epoch - 249ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21353
219/219 - 53s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5465 - lr: 6.0000e-04 - 53s/epoch - 244ms/step
Epoch 28/100

Epoch 28: val_loss improved from 0.21353 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5500 - lr: 6.0000e-04 - 54s/epoch - 246ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21351

Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 52s - loss: 0.2134 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5434 - lr: 6.0000e-04 - 52s/epoch - 239ms/step
Epoch 30/100

Epoch 30: val_loss improved from 0.21351 to 0.21350, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5526 - lr: 3.6000e-04 - 54s/epoch - 246ms/step
Epoch 31/100

Epoch 31: val_loss improved from 0.21350 to 0.21349, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5491 - lr: 3.6000e-04 - 54s/epoch - 247ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21349
219/219 - 53s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5491 - lr: 3.6000e-04 - 53s/epoch - 241ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21349
219/219 - 54s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 3.6000e-04 - 54s/epoch - 249ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21349
219/219 - 53s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5462 - lr: 3.6000e-04 - 53s/epoch - 240ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21349

Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 54s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5541 - lr: 3.6000e-04 - 54s/epoch - 245ms/step
Epoch 36/100

Epoch 36: val_loss improved from 0.21349 to 0.21348, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_4_batchsize_65536.tf
219/219 - 56s - loss: 0.2133 - acc: 0.5507 - val_loss: 0.2135 - val_acc: 0.5506 - lr: 2.1600e-04 - 56s/epoch - 254ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5468 - lr: 2.1600e-04 - 53s/epoch - 242ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5512 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21348
219/219 - 54s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5513 - lr: 2.1600e-04 - 54s/epoch - 247ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2133 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5452 - lr: 2.1600e-04 - 53s/epoch - 240ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21348

Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 52s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 2.1600e-04 - 52s/epoch - 240ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5508 - val_loss: 0.2135 - val_acc: 0.5493 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5494 - lr: 1.2960e-04 - 53s/epoch - 240ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21348
219/219 - 52s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5480 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21348
219/219 - 54s - loss: 0.2132 - acc: 0.5509 - val_loss: 0.2135 - val_acc: 0.5517 - lr: 1.2960e-04 - 54s/epoch - 244ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5510 - val_loss: 0.2135 - val_acc: 0.5500 - lr: 1.2960e-04 - 53s/epoch - 244ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21348

Epoch 47: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5478 - lr: 1.2960e-04 - 53s/epoch - 240ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5493 - lr: 7.7760e-05 - 53s/epoch - 240ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5512 - val_loss: 0.2135 - val_acc: 0.5476 - lr: 7.7760e-05 - 53s/epoch - 240ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21348
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 7.7760e-05 - 53s/epoch - 242ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21348
Restoring model weights from the end of the best epoch: 36.
219/219 - 53s - loss: 0.2132 - acc: 0.5511 - val_loss: 0.2135 - val_acc: 0.5504 - lr: 7.7760e-05 - 53s/epoch - 241ms/step
Epoch 51: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 4, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21642, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 57s - loss: 0.2205 - acc: 0.5226 - val_loss: 0.2164 - val_acc: 0.5242 - lr: 0.0010 - 57s/epoch - 258ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21642 to 0.21527, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 55s - loss: 0.2158 - acc: 0.5337 - val_loss: 0.2153 - val_acc: 0.5399 - lr: 0.0010 - 55s/epoch - 249ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21527 to 0.21483, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2150 - acc: 0.5417 - val_loss: 0.2148 - val_acc: 0.5335 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21483 to 0.21450, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2146 - acc: 0.5448 - val_loss: 0.2145 - val_acc: 0.5372 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 5/100

Epoch 5: val_loss did not improve from 0.21450
219/219 - 52s - loss: 0.2144 - acc: 0.5462 - val_loss: 0.2145 - val_acc: 0.5328 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21450 to 0.21410, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2142 - acc: 0.5468 - val_loss: 0.2141 - val_acc: 0.5427 - lr: 0.0010 - 54s/epoch - 249ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21410 to 0.21397, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 56s - loss: 0.2140 - acc: 0.5475 - val_loss: 0.2140 - val_acc: 0.5441 - lr: 0.0010 - 56s/epoch - 254ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21397 to 0.21395, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2139 - acc: 0.5477 - val_loss: 0.2140 - val_acc: 0.5411 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21395 to 0.21382, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2139 - acc: 0.5479 - val_loss: 0.2138 - val_acc: 0.5518 - lr: 0.0010 - 54s/epoch - 249ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21382 to 0.21378, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2138 - acc: 0.5481 - val_loss: 0.2138 - val_acc: 0.5527 - lr: 0.0010 - 54s/epoch - 247ms/step
Epoch 11/100

Epoch 11: val_loss did not improve from 0.21378
219/219 - 53s - loss: 0.2138 - acc: 0.5487 - val_loss: 0.2139 - val_acc: 0.5544 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21378 to 0.21372, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2137 - acc: 0.5487 - val_loss: 0.2137 - val_acc: 0.5536 - lr: 0.0010 - 54s/epoch - 248ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21372 to 0.21370, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 56s - loss: 0.2137 - acc: 0.5489 - val_loss: 0.2137 - val_acc: 0.5539 - lr: 0.0010 - 56s/epoch - 254ms/step
Epoch 14/100

Epoch 14: val_loss did not improve from 0.21370
219/219 - 53s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2138 - val_acc: 0.5475 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 15/100

Epoch 15: val_loss improved from 0.21370 to 0.21363, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 55s - loss: 0.2136 - acc: 0.5489 - val_loss: 0.2136 - val_acc: 0.5494 - lr: 0.0010 - 55s/epoch - 251ms/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.21363
219/219 - 52s - loss: 0.2136 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5414 - lr: 0.0010 - 52s/epoch - 239ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21363
219/219 - 53s - loss: 0.2136 - acc: 0.5492 - val_loss: 0.2137 - val_acc: 0.5548 - lr: 0.0010 - 53s/epoch - 243ms/step
Epoch 18/100

Epoch 18: val_loss improved from 0.21363 to 0.21355, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 56s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 0.0010 - 56s/epoch - 255ms/step
Epoch 19/100

Epoch 19: val_loss did not improve from 0.21355
219/219 - 55s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2136 - val_acc: 0.5509 - lr: 0.0010 - 55s/epoch - 251ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2136 - acc: 0.5491 - val_loss: 0.2137 - val_acc: 0.5479 - lr: 0.0010 - 53s/epoch - 242ms/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.21355
219/219 - 53s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5409 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21355 to 0.21351, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2135 - acc: 0.5493 - val_loss: 0.2135 - val_acc: 0.5489 - lr: 0.0010 - 54s/epoch - 246ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.21351
219/219 - 53s - loss: 0.2135 - acc: 0.5492 - val_loss: 0.2136 - val_acc: 0.5469 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21351

Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 53s - loss: 0.2135 - acc: 0.5494 - val_loss: 0.2135 - val_acc: 0.5485 - lr: 0.0010 - 53s/epoch - 240ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21351
219/219 - 53s - loss: 0.2134 - acc: 0.5497 - val_loss: 0.2135 - val_acc: 0.5436 - lr: 6.0000e-04 - 53s/epoch - 243ms/step
Epoch 26/100

Epoch 26: val_loss improved from 0.21351 to 0.21349, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 55s - loss: 0.2134 - acc: 0.5496 - val_loss: 0.2135 - val_acc: 0.5489 - lr: 6.0000e-04 - 55s/epoch - 253ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2134 - acc: 0.5497 - val_loss: 0.2135 - val_acc: 0.5452 - lr: 6.0000e-04 - 52s/epoch - 240ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2134 - acc: 0.5497 - val_loss: 0.2135 - val_acc: 0.5448 - lr: 6.0000e-04 - 52s/epoch - 239ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21349
219/219 - 52s - loss: 0.2134 - acc: 0.5498 - val_loss: 0.2135 - val_acc: 0.5463 - lr: 6.0000e-04 - 52s/epoch - 239ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21349

Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 52s - loss: 0.2134 - acc: 0.5498 - val_loss: 0.2135 - val_acc: 0.5436 - lr: 6.0000e-04 - 52s/epoch - 240ms/step
Epoch 31/100

Epoch 31: val_loss improved from 0.21349 to 0.21346, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 54s - loss: 0.2134 - acc: 0.5500 - val_loss: 0.2135 - val_acc: 0.5467 - lr: 3.6000e-04 - 54s/epoch - 247ms/step
Epoch 32/100

Epoch 32: val_loss improved from 0.21346 to 0.21345, saving model to ./saved_models/DCTR_NNLO_wgt_org_loss_mse_5_batchsize_65536.tf
219/219 - 57s - loss: 0.2134 - acc: 0.5501 - val_loss: 0.2134 - val_acc: 0.5496 - lr: 3.6000e-04 - 57s/epoch - 259ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5504 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5500 - val_loss: 0.2135 - val_acc: 0.5498 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5501 - val_loss: 0.2135 - val_acc: 0.5480 - lr: 3.6000e-04 - 52s/epoch - 239ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21345

Epoch 36: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 53s - loss: 0.2133 - acc: 0.5500 - val_loss: 0.2135 - val_acc: 0.5508 - lr: 3.6000e-04 - 53s/epoch - 240ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5503 - val_loss: 0.2135 - val_acc: 0.5483 - lr: 2.1600e-04 - 52s/epoch - 240ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21345
219/219 - 53s - loss: 0.2133 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5497 - lr: 2.1600e-04 - 53s/epoch - 244ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21345
219/219 - 53s - loss: 0.2133 - acc: 0.5502 - val_loss: 0.2135 - val_acc: 0.5537 - lr: 2.1600e-04 - 53s/epoch - 241ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2133 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5469 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21345
219/219 - 53s - loss: 0.2133 - acc: 0.5504 - val_loss: 0.2135 - val_acc: 0.5486 - lr: 2.1600e-04 - 53s/epoch - 240ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21345

Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 52s - loss: 0.2133 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5497 - lr: 2.1600e-04 - 52s/epoch - 239ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2132 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5482 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21345
219/219 - 53s - loss: 0.2132 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5489 - lr: 1.2960e-04 - 53s/epoch - 242ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21345
219/219 - 54s - loss: 0.2132 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5469 - lr: 1.2960e-04 - 54s/epoch - 245ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21345
219/219 - 52s - loss: 0.2132 - acc: 0.5505 - val_loss: 0.2135 - val_acc: 0.5515 - lr: 1.2960e-04 - 52s/epoch - 239ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21345
Restoring model weights from the end of the best epoch: 32.
219/219 - 52s - loss: 0.2132 - acc: 0.5506 - val_loss: 0.2135 - val_acc: 0.5476 - lr: 1.2960e-04 - 52s/epoch - 240ms/step
Epoch 47: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 5, batch_size = 65536
