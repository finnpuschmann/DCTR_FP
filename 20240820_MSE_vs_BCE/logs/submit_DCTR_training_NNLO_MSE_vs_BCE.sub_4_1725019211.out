Configured CUDA from: /cvmfs/sft.cern.ch/lcg/releases/cuda/12.4-4899e/x86_64-el9-gcc11-opt
GPUs available for tensorflow:
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
loading data
preparing data
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21671, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 13s - loss: 0.2235 - acc: 0.5235 - val_loss: 0.2167 - val_acc: 0.5274 - lr: 0.0010 - 13s/epoch - 57ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21671 to 0.21565, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2161 - acc: 0.5343 - val_loss: 0.2156 - val_acc: 0.5284 - lr: 0.0010 - 11s/epoch - 48ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21565 to 0.21492, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2152 - acc: 0.5425 - val_loss: 0.2149 - val_acc: 0.5455 - lr: 0.0010 - 11s/epoch - 48ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21492 to 0.21450, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2147 - acc: 0.5457 - val_loss: 0.2145 - val_acc: 0.5437 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21450 to 0.21429, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2144 - acc: 0.5465 - val_loss: 0.2143 - val_acc: 0.5464 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21429 to 0.21410, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2141 - acc: 0.5474 - val_loss: 0.2141 - val_acc: 0.5420 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21410 to 0.21400, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2140 - acc: 0.5479 - val_loss: 0.2140 - val_acc: 0.5511 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21400 to 0.21396, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2139 - acc: 0.5484 - val_loss: 0.2140 - val_acc: 0.5484 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21396 to 0.21391, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5489 - val_loss: 0.2139 - val_acc: 0.5553 - lr: 0.0010 - 11s/epoch - 48ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21391 to 0.21383, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5489 - val_loss: 0.2138 - val_acc: 0.5500 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21383 to 0.21380, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5492 - val_loss: 0.2138 - val_acc: 0.5513 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21380 to 0.21375, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5522 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21375 to 0.21373, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5489 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 14/100

Epoch 14: val_loss did not improve from 0.21373
219/219 - 9s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5513 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 15/100

Epoch 15: val_loss improved from 0.21373 to 0.21369, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5497 - val_loss: 0.2137 - val_acc: 0.5509 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5496 - val_loss: 0.2138 - val_acc: 0.5556 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2137 - val_acc: 0.5540 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2137 - val_acc: 0.5519 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 19/100

Epoch 19: val_loss improved from 0.21369 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf

Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 11s - loss: 0.2136 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5482 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 20/100

Epoch 20: val_loss improved from 0.21364 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5487 - lr: 6.0000e-04 - 11s/epoch - 49ms/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.21364
219/219 - 9s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2137 - val_acc: 0.5487 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21364 to 0.21362, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 6.0000e-04 - 11s/epoch - 49ms/step
Epoch 23/100

Epoch 23: val_loss did not improve from 0.21362
219/219 - 9s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5460 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21362
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2137 - val_acc: 0.5469 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21362
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2137 - val_acc: 0.5445 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 26/100

Epoch 26: val_loss improved from 0.21362 to 0.21361, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5504 - val_loss: 0.2136 - val_acc: 0.5495 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21361
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5511 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21361

Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5510 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 29/100

Epoch 29: val_loss improved from 0.21361 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 3.6000e-04 - 11s/epoch - 50ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5466 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 0.21360
219/219 - 10s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 3.6000e-04 - 10s/epoch - 43ms/step
Epoch 32/100

Epoch 32: val_loss improved from 0.21360 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5491 - lr: 3.6000e-04 - 11s/epoch - 51ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2137 - val_acc: 0.5559 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21360

Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5524 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5533 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 37/100

Epoch 37: val_loss improved from 0.21360 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 2.1600e-04 - 11s/epoch - 50ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2134 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5502 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2134 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21359

Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 9s - loss: 0.2134 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5507 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5507 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 43/100

Epoch 43: val_loss improved from 0.21359 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5495 - lr: 1.2960e-04 - 11s/epoch - 52ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5495 - lr: 1.2960e-04 - 9s/epoch - 42ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5494 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21359

Epoch 46: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5502 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 47/100

Epoch 47: val_loss improved from 0.21359 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_1_batchsize_65536.tf
219/219 - 11s - loss: 0.2133 - acc: 0.5515 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 7.7760e-05 - 11s/epoch - 50ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5515 - val_loss: 0.2136 - val_acc: 0.5513 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5505 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5491 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5515 - val_loss: 0.2136 - val_acc: 0.5509 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21359

Epoch 52: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.
219/219 - 9s - loss: 0.2133 - acc: 0.5515 - val_loss: 0.2136 - val_acc: 0.5512 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5509 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5505 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 55/100

Epoch 55: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5517 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 56/100

Epoch 56: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 57/100

Epoch 57: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 58/100

Epoch 58: val_loss did not improve from 0.21359

Epoch 58: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.
219/219 - 9s - loss: 0.2133 - acc: 0.5516 - val_loss: 0.2136 - val_acc: 0.5502 - lr: 4.6656e-05 - 9s/epoch - 43ms/step
Epoch 59/100

Epoch 59: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2132 - acc: 0.5517 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 2.7994e-05 - 9s/epoch - 43ms/step
Epoch 60/100

Epoch 60: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2132 - acc: 0.5517 - val_loss: 0.2136 - val_acc: 0.5509 - lr: 2.7994e-05 - 9s/epoch - 43ms/step
Epoch 61/100

Epoch 61: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2132 - acc: 0.5517 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 2.7994e-05 - 9s/epoch - 43ms/step
Epoch 62/100

Epoch 62: val_loss did not improve from 0.21359
Restoring model weights from the end of the best epoch: 47.
219/219 - 10s - loss: 0.2132 - acc: 0.5517 - val_loss: 0.2136 - val_acc: 0.5507 - lr: 2.7994e-05 - 10s/epoch - 44ms/step
Epoch 62: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 1, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21688, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 12s - loss: 0.2248 - acc: 0.5226 - val_loss: 0.2169 - val_acc: 0.5307 - lr: 0.0010 - 12s/epoch - 54ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21688 to 0.21595, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2163 - acc: 0.5324 - val_loss: 0.2160 - val_acc: 0.5496 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21595 to 0.21512, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2154 - acc: 0.5403 - val_loss: 0.2151 - val_acc: 0.5422 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21512 to 0.21475, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2149 - acc: 0.5442 - val_loss: 0.2147 - val_acc: 0.5406 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21475 to 0.21452, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2146 - acc: 0.5462 - val_loss: 0.2145 - val_acc: 0.5527 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21452 to 0.21428, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2144 - acc: 0.5473 - val_loss: 0.2143 - val_acc: 0.5515 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21428 to 0.21421, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2142 - acc: 0.5480 - val_loss: 0.2142 - val_acc: 0.5421 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21421 to 0.21402, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2140 - acc: 0.5484 - val_loss: 0.2140 - val_acc: 0.5466 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21402 to 0.21398, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2140 - acc: 0.5487 - val_loss: 0.2140 - val_acc: 0.5523 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 10/100

Epoch 10: val_loss did not improve from 0.21398
219/219 - 9s - loss: 0.2139 - acc: 0.5491 - val_loss: 0.2140 - val_acc: 0.5496 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21398 to 0.21386, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5492 - val_loss: 0.2139 - val_acc: 0.5438 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 12/100

Epoch 12: val_loss did not improve from 0.21386
219/219 - 9s - loss: 0.2138 - acc: 0.5493 - val_loss: 0.2139 - val_acc: 0.5507 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21386 to 0.21383, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5494 - val_loss: 0.2138 - val_acc: 0.5522 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21383 to 0.21379, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5495 - val_loss: 0.2138 - val_acc: 0.5539 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21379
219/219 - 9s - loss: 0.2137 - acc: 0.5496 - val_loss: 0.2138 - val_acc: 0.5456 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 16/100

Epoch 16: val_loss improved from 0.21379 to 0.21374, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5496 - val_loss: 0.2137 - val_acc: 0.5537 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 17/100

Epoch 17: val_loss improved from 0.21374 to 0.21371, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5498 - val_loss: 0.2137 - val_acc: 0.5483 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21371
219/219 - 9s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2138 - val_acc: 0.5475 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 19/100

Epoch 19: val_loss improved from 0.21371 to 0.21369, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2136 - acc: 0.5498 - val_loss: 0.2137 - val_acc: 0.5502 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2137 - val_acc: 0.5503 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 21/100

Epoch 21: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2137 - val_acc: 0.5527 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21369 to 0.21369, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf

Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 11s - loss: 0.2136 - acc: 0.5498 - val_loss: 0.2137 - val_acc: 0.5539 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 23/100

Epoch 23: val_loss improved from 0.21369 to 0.21368, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2137 - val_acc: 0.5526 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21368
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2137 - val_acc: 0.5488 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 25/100

Epoch 25: val_loss improved from 0.21368 to 0.21361, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5511 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21361
219/219 - 9s - loss: 0.2135 - acc: 0.5504 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21361
219/219 - 10s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2137 - val_acc: 0.5540 - lr: 6.0000e-04 - 10s/epoch - 44ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21361
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5460 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 29/100

Epoch 29: val_loss improved from 0.21361 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5526 - lr: 6.0000e-04 - 11s/epoch - 52ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5512 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 31/100

Epoch 31: val_loss improved from 0.21360 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf

Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 11s - loss: 0.2135 - acc: 0.5504 - val_loss: 0.2136 - val_acc: 0.5525 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 32/100

Epoch 32: val_loss improved from 0.21359 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2136 - val_acc: 0.5524 - lr: 3.6000e-04 - 11s/epoch - 51ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5519 - lr: 3.6000e-04 - 9s/epoch - 42ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5538 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5492 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5483 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21358

Epoch 37: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 39/100

Epoch 39: val_loss improved from 0.21358 to 0.21355, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_2_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5505 - lr: 2.1600e-04 - 11s/epoch - 49ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5477 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5548 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2134 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5500 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21355

Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 9s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5511 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5493 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5526 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5485 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5492 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21355

Epoch 49: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 10s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5510 - lr: 1.2960e-04 - 10s/epoch - 44ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5490 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5517 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21355
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5486 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.21355
219/219 - 10s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 7.7760e-05 - 10s/epoch - 43ms/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.21355
Restoring model weights from the end of the best epoch: 39.
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 54: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 2, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21668, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 12s - loss: 0.2221 - acc: 0.5231 - val_loss: 0.2167 - val_acc: 0.5307 - lr: 0.0010 - 12s/epoch - 54ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21668 to 0.21569, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2160 - acc: 0.5342 - val_loss: 0.2157 - val_acc: 0.5464 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21569 to 0.21498, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2152 - acc: 0.5417 - val_loss: 0.2150 - val_acc: 0.5416 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21498 to 0.21453, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2147 - acc: 0.5454 - val_loss: 0.2145 - val_acc: 0.5521 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21453 to 0.21432, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2144 - acc: 0.5470 - val_loss: 0.2143 - val_acc: 0.5494 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21432 to 0.21418, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2142 - acc: 0.5476 - val_loss: 0.2142 - val_acc: 0.5523 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21418 to 0.21397, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2140 - acc: 0.5479 - val_loss: 0.2140 - val_acc: 0.5501 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21397 to 0.21392, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2139 - acc: 0.5481 - val_loss: 0.2139 - val_acc: 0.5446 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21392 to 0.21383, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5482 - val_loss: 0.2138 - val_acc: 0.5494 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 10/100

Epoch 10: val_loss did not improve from 0.21383
219/219 - 9s - loss: 0.2138 - acc: 0.5484 - val_loss: 0.2138 - val_acc: 0.5454 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 11/100

Epoch 11: val_loss did not improve from 0.21383
219/219 - 9s - loss: 0.2138 - acc: 0.5485 - val_loss: 0.2138 - val_acc: 0.5444 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21383 to 0.21375, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5486 - val_loss: 0.2137 - val_acc: 0.5464 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 13/100

Epoch 13: val_loss did not improve from 0.21375
219/219 - 9s - loss: 0.2137 - acc: 0.5491 - val_loss: 0.2138 - val_acc: 0.5476 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21375 to 0.21370, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5490 - val_loss: 0.2137 - val_acc: 0.5499 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21370
219/219 - 9s - loss: 0.2137 - acc: 0.5491 - val_loss: 0.2137 - val_acc: 0.5440 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.21370
219/219 - 9s - loss: 0.2136 - acc: 0.5492 - val_loss: 0.2137 - val_acc: 0.5491 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 17/100

Epoch 17: val_loss improved from 0.21370 to 0.21365, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2136 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5505 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 18/100

Epoch 18: val_loss did not improve from 0.21365
219/219 - 9s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5516 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 19/100

Epoch 19: val_loss did not improve from 0.21365
219/219 - 9s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5452 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21365

Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 9s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5495 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 21/100

Epoch 21: val_loss improved from 0.21365 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5443 - lr: 6.0000e-04 - 11s/epoch - 49ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21364 to 0.21361, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5480 - lr: 6.0000e-04 - 11s/epoch - 49ms/step
Epoch 23/100

Epoch 23: val_loss improved from 0.21361 to 0.21361, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5523 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21361
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2138 - val_acc: 0.5421 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21361
219/219 - 9s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5525 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 26/100

Epoch 26: val_loss improved from 0.21361 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5463 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2137 - val_acc: 0.5429 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5483 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5539 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 0.21359
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5452 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21359

Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 10s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5515 - lr: 6.0000e-04 - 10s/epoch - 44ms/step
Epoch 33/100

Epoch 33: val_loss improved from 0.21359 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5493 - lr: 3.6000e-04 - 11s/epoch - 51ms/step
Epoch 34/100

Epoch 34: val_loss improved from 0.21359 to 0.21359, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5539 - lr: 3.6000e-04 - 11s/epoch - 49ms/step
Epoch 35/100

Epoch 35: val_loss improved from 0.21359 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5475 - lr: 3.6000e-04 - 11s/epoch - 50ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5523 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5459 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21358

Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5528 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 39/100

Epoch 39: val_loss improved from 0.21358 to 0.21356, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_3_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5510 - lr: 2.1600e-04 - 11s/epoch - 50ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5534 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5492 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5535 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21356

Epoch 44: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5474 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5492 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5510 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5517 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5504 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21356

Epoch 50: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 10s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5461 - lr: 1.2960e-04 - 10s/epoch - 43ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.21356
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.21356
Restoring model weights from the end of the best epoch: 39.
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5518 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 54: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 3, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21670, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 12s - loss: 0.2249 - acc: 0.5267 - val_loss: 0.2167 - val_acc: 0.5397 - lr: 0.0010 - 12s/epoch - 54ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21670 to 0.21557, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2160 - acc: 0.5392 - val_loss: 0.2156 - val_acc: 0.5400 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21557 to 0.21497, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2152 - acc: 0.5445 - val_loss: 0.2150 - val_acc: 0.5450 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21497 to 0.21445, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2147 - acc: 0.5470 - val_loss: 0.2145 - val_acc: 0.5468 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21445 to 0.21426, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2143 - acc: 0.5480 - val_loss: 0.2143 - val_acc: 0.5421 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21426 to 0.21415, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2141 - acc: 0.5484 - val_loss: 0.2141 - val_acc: 0.5522 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21415 to 0.21393, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2140 - acc: 0.5487 - val_loss: 0.2139 - val_acc: 0.5469 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21393 to 0.21392, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2139 - acc: 0.5488 - val_loss: 0.2139 - val_acc: 0.5518 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 9/100

Epoch 9: val_loss did not improve from 0.21392
219/219 - 9s - loss: 0.2138 - acc: 0.5489 - val_loss: 0.2140 - val_acc: 0.5437 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21392 to 0.21380, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5492 - val_loss: 0.2138 - val_acc: 0.5501 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21380 to 0.21380, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5492 - val_loss: 0.2138 - val_acc: 0.5503 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21380 to 0.21379, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5494 - val_loss: 0.2138 - val_acc: 0.5518 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21379 to 0.21372, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5493 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21372 to 0.21372, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5492 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21372
219/219 - 9s - loss: 0.2137 - acc: 0.5495 - val_loss: 0.2138 - val_acc: 0.5531 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 16/100

Epoch 16: val_loss improved from 0.21372 to 0.21369, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5481 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21369
219/219 - 9s - loss: 0.2136 - acc: 0.5496 - val_loss: 0.2138 - val_acc: 0.5584 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 18/100

Epoch 18: val_loss improved from 0.21369 to 0.21368, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 12s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2137 - val_acc: 0.5532 - lr: 0.0010 - 12s/epoch - 55ms/step
Epoch 19/100

Epoch 19: val_loss improved from 0.21368 to 0.21366, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 15s - loss: 0.2136 - acc: 0.5497 - val_loss: 0.2137 - val_acc: 0.5459 - lr: 0.0010 - 15s/epoch - 67ms/step
Epoch 20/100

Epoch 20: val_loss did not improve from 0.21366
219/219 - 9s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2137 - val_acc: 0.5481 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 21/100

Epoch 21: val_loss improved from 0.21366 to 0.21365, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 14s - loss: 0.2136 - acc: 0.5496 - val_loss: 0.2137 - val_acc: 0.5495 - lr: 0.0010 - 14s/epoch - 64ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21365 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf

Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 17s - loss: 0.2136 - acc: 0.5499 - val_loss: 0.2136 - val_acc: 0.5453 - lr: 0.0010 - 17s/epoch - 79ms/step
Epoch 23/100

Epoch 23: val_loss improved from 0.21364 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 14s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5481 - lr: 6.0000e-04 - 14s/epoch - 63ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5468 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5515 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 27/100

Epoch 27: val_loss improved from 0.21360 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 14s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5513 - lr: 6.0000e-04 - 14s/epoch - 64ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5505 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2137 - val_acc: 0.5562 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5471 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 31/100

Epoch 31: val_loss did not improve from 0.21358
219/219 - 10s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 6.0000e-04 - 10s/epoch - 44ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5534 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21358

Epoch 33: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 9s - loss: 0.2135 - acc: 0.5503 - val_loss: 0.2136 - val_acc: 0.5528 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 34/100

Epoch 34: val_loss improved from 0.21358 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 12s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 3.6000e-04 - 12s/epoch - 57ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5545 - lr: 3.6000e-04 - 9s/epoch - 42ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5471 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 37/100

Epoch 37: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5549 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5521 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21358

Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5497 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 40/100

Epoch 40: val_loss improved from 0.21358 to 0.21357, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_4_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5512 - lr: 2.1600e-04 - 11s/epoch - 52ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5470 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5540 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21357

Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 9s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5538 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5515 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21357
219/219 - 10s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5530 - lr: 1.2960e-04 - 10s/epoch - 43ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5490 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5496 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21357

Epoch 51: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5504 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21357
219/219 - 10s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5509 - lr: 7.7760e-05 - 10s/epoch - 44ms/step
Epoch 53/100

Epoch 53: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5516 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 54/100

Epoch 54: val_loss did not improve from 0.21357
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5512 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 55/100

Epoch 55: val_loss did not improve from 0.21357
Restoring model weights from the end of the best epoch: 40.
219/219 - 10s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5511 - lr: 7.7760e-05 - 10s/epoch - 44ms/step
Epoch 55: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 4, batch_size = 65536
Epoch 1/100

Epoch 1: val_loss improved from inf to 0.21648, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 12s - loss: 0.2231 - acc: 0.5242 - val_loss: 0.2165 - val_acc: 0.5336 - lr: 0.0010 - 12s/epoch - 55ms/step
Epoch 2/100

Epoch 2: val_loss improved from 0.21648 to 0.21529, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2158 - acc: 0.5391 - val_loss: 0.2153 - val_acc: 0.5445 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 3/100

Epoch 3: val_loss improved from 0.21529 to 0.21480, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2150 - acc: 0.5455 - val_loss: 0.2148 - val_acc: 0.5552 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 4/100

Epoch 4: val_loss improved from 0.21480 to 0.21454, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2145 - acc: 0.5474 - val_loss: 0.2145 - val_acc: 0.5572 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 5/100

Epoch 5: val_loss improved from 0.21454 to 0.21427, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2142 - acc: 0.5483 - val_loss: 0.2143 - val_acc: 0.5511 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 6/100

Epoch 6: val_loss improved from 0.21427 to 0.21404, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2141 - acc: 0.5487 - val_loss: 0.2140 - val_acc: 0.5512 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 7/100

Epoch 7: val_loss improved from 0.21404 to 0.21392, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2139 - acc: 0.5490 - val_loss: 0.2139 - val_acc: 0.5518 - lr: 0.0010 - 11s/epoch - 50ms/step
Epoch 8/100

Epoch 8: val_loss improved from 0.21392 to 0.21389, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2139 - acc: 0.5490 - val_loss: 0.2139 - val_acc: 0.5461 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 9/100

Epoch 9: val_loss improved from 0.21389 to 0.21384, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5489 - val_loss: 0.2138 - val_acc: 0.5520 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 10/100

Epoch 10: val_loss improved from 0.21384 to 0.21381, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2138 - acc: 0.5490 - val_loss: 0.2138 - val_acc: 0.5452 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 11/100

Epoch 11: val_loss improved from 0.21381 to 0.21375, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2138 - val_acc: 0.5517 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 12/100

Epoch 12: val_loss improved from 0.21375 to 0.21375, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5491 - val_loss: 0.2137 - val_acc: 0.5471 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 13/100

Epoch 13: val_loss improved from 0.21375 to 0.21371, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5520 - lr: 0.0010 - 11s/epoch - 52ms/step
Epoch 14/100

Epoch 14: val_loss improved from 0.21371 to 0.21368, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2137 - acc: 0.5493 - val_loss: 0.2137 - val_acc: 0.5522 - lr: 0.0010 - 11s/epoch - 49ms/step
Epoch 15/100

Epoch 15: val_loss did not improve from 0.21368
219/219 - 9s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5537 - lr: 0.0010 - 9s/epoch - 42ms/step
Epoch 16/100

Epoch 16: val_loss did not improve from 0.21368
219/219 - 9s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5507 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 17/100

Epoch 17: val_loss did not improve from 0.21368
219/219 - 9s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2137 - val_acc: 0.5525 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 18/100

Epoch 18: val_loss improved from 0.21368 to 0.21367, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2136 - acc: 0.5494 - val_loss: 0.2137 - val_acc: 0.5517 - lr: 0.0010 - 11s/epoch - 51ms/step
Epoch 19/100

Epoch 19: val_loss did not improve from 0.21367

Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.
219/219 - 9s - loss: 0.2136 - acc: 0.5495 - val_loss: 0.2138 - val_acc: 0.5562 - lr: 0.0010 - 9s/epoch - 43ms/step
Epoch 20/100

Epoch 20: val_loss improved from 0.21367 to 0.21365, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5550 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 21/100

Epoch 21: val_loss improved from 0.21365 to 0.21364, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2136 - val_acc: 0.5512 - lr: 6.0000e-04 - 11s/epoch - 49ms/step
Epoch 22/100

Epoch 22: val_loss improved from 0.21364 to 0.21363, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5498 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 23/100

Epoch 23: val_loss improved from 0.21363 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5495 - lr: 6.0000e-04 - 11s/epoch - 50ms/step
Epoch 24/100

Epoch 24: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5487 - lr: 6.0000e-04 - 9s/epoch - 42ms/step
Epoch 25/100

Epoch 25: val_loss did not improve from 0.21360
219/219 - 10s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5461 - lr: 6.0000e-04 - 10s/epoch - 44ms/step
Epoch 26/100

Epoch 26: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5499 - val_loss: 0.2137 - val_acc: 0.5448 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 27/100

Epoch 27: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2135 - acc: 0.5500 - val_loss: 0.2137 - val_acc: 0.5523 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 28/100

Epoch 28: val_loss did not improve from 0.21360
219/219 - 10s - loss: 0.2135 - acc: 0.5501 - val_loss: 0.2136 - val_acc: 0.5472 - lr: 6.0000e-04 - 10s/epoch - 43ms/step
Epoch 29/100

Epoch 29: val_loss did not improve from 0.21360

Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.
219/219 - 9s - loss: 0.2135 - acc: 0.5502 - val_loss: 0.2136 - val_acc: 0.5472 - lr: 6.0000e-04 - 9s/epoch - 43ms/step
Epoch 30/100

Epoch 30: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5504 - val_loss: 0.2136 - val_acc: 0.5522 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 31/100

Epoch 31: val_loss improved from 0.21360 to 0.21360, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5490 - lr: 3.6000e-04 - 11s/epoch - 51ms/step
Epoch 32/100

Epoch 32: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5494 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 33/100

Epoch 33: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5505 - val_loss: 0.2136 - val_acc: 0.5541 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 34/100

Epoch 34: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5507 - val_loss: 0.2136 - val_acc: 0.5539 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 35/100

Epoch 35: val_loss did not improve from 0.21360

Epoch 35: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.
219/219 - 9s - loss: 0.2134 - acc: 0.5506 - val_loss: 0.2136 - val_acc: 0.5507 - lr: 3.6000e-04 - 9s/epoch - 43ms/step
Epoch 36/100

Epoch 36: val_loss did not improve from 0.21360
219/219 - 9s - loss: 0.2134 - acc: 0.5508 - val_loss: 0.2136 - val_acc: 0.5458 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 37/100

Epoch 37: val_loss improved from 0.21360 to 0.21358, saving model to ./saved_models/DCTR_NNLO_wgt_np_mse_loss_mse_5_batchsize_65536.tf
219/219 - 11s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 2.1600e-04 - 11s/epoch - 50ms/step
Epoch 38/100

Epoch 38: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5522 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 39/100

Epoch 39: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2134 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5508 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 40/100

Epoch 40: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5518 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 41/100

Epoch 41: val_loss did not improve from 0.21358

Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.
219/219 - 9s - loss: 0.2133 - acc: 0.5509 - val_loss: 0.2136 - val_acc: 0.5486 - lr: 2.1600e-04 - 9s/epoch - 43ms/step
Epoch 42/100

Epoch 42: val_loss did not improve from 0.21358
219/219 - 10s - loss: 0.2133 - acc: 0.5510 - val_loss: 0.2136 - val_acc: 0.5532 - lr: 1.2960e-04 - 10s/epoch - 44ms/step
Epoch 43/100

Epoch 43: val_loss did not improve from 0.21358
219/219 - 10s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5522 - lr: 1.2960e-04 - 10s/epoch - 43ms/step
Epoch 44/100

Epoch 44: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5476 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 45/100

Epoch 45: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5514 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 46/100

Epoch 46: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5511 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 47/100

Epoch 47: val_loss did not improve from 0.21358

Epoch 47: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5499 - lr: 1.2960e-04 - 9s/epoch - 43ms/step
Epoch 48/100

Epoch 48: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5495 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 49/100

Epoch 49: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5512 - val_loss: 0.2136 - val_acc: 0.5498 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 50/100

Epoch 50: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5492 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 51/100

Epoch 51: val_loss did not improve from 0.21358
219/219 - 9s - loss: 0.2133 - acc: 0.5514 - val_loss: 0.2136 - val_acc: 0.5503 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 52/100

Epoch 52: val_loss did not improve from 0.21358
Restoring model weights from the end of the best epoch: 37.
219/219 - 9s - loss: 0.2133 - acc: 0.5513 - val_loss: 0.2136 - val_acc: 0.5501 - lr: 7.7760e-05 - 9s/epoch - 43ms/step
Epoch 52: early stopping
clearing keras session and collecting garbage
finished training: loss = 'mse', run = 5, batch_size = 65536
