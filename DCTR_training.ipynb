{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 20 10:48:15 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   49C    P8    31W / 250W |   7044MiB /  8192MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# import standard modules and DCTR\n",
    "import os\n",
    "os.system('for a in /sys/bus/pci/devices/*; do echo 0 | tee -a $a/numa_node>/dev/null; done') # get rid of NUMA node warnings: https://github.com/tensorflow/tensorflow/issues/42738\n",
    "os.system('nvidia-smi') # make sure nvidia gpu acceleration is working\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity('ERROR') # get rid of some warnings that don't impact anything\n",
    "# make sure GPU usage is enabled\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# import DCTR\n",
    "import DCTR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POWHEG hvq all particles x0_nrm.shape: (10000000, 3, 9)\n",
      "POWHEG hvq all particles x0_plt.shape: (30000000, 3, 9)\n",
      "POWHEG hvq all particles x0_plt_nrm.shape: (30000000, 3, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-normed Datasets: MiNNLO: X1 | POWHEG hvq: X0\n",
    "# only contain tt-pair; every event has order: \n",
    "# tt-pair, top, anti-top\n",
    "# every particle has arguments: \n",
    "# [pt, y, phi, mass, eta, E, PID, w, theta]\n",
    "# [0 , 1, 2  , 3   , 4  , 5, 6  , 7, 8    ]\n",
    "\n",
    "\n",
    "# POWHEG hvq\n",
    "x0_nrm = []\n",
    "x0_nrm = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/01-02_normed_converted_lhe.npz', i=3)[:int(1e7)]\n",
    "print('POWHEG hvq all particles x0_nrm.shape: '+str(x0_nrm.shape))\n",
    "\n",
    "# plotting data; different from training data\n",
    "x0_plt = []\n",
    "x0_plt = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/03-04_converted_lhe.npz', i=3)[:int(3e7)]\n",
    "print('POWHEG hvq all particles x0_plt.shape: '+str(x0_plt.shape))\n",
    "\n",
    "x0_plt_nrm = []\n",
    "x0_plt_nrm = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/03-04_normed_converted_lhe.npz', i=3)[:int(3e7)]\n",
    "print('POWHEG hvq all particles x0_plt_nrm.shape: '+str(x0_plt_nrm.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiNNLO all particles x1_nrm.shape: (9553938, 3, 9)\n",
      "MiNNLO all particles x1_plt.shape: (9553938, 3, 9)\n"
     ]
    }
   ],
   "source": [
    "# MiNNLO\n",
    "x1_nrm = []\n",
    "x1_nrm = DCTR.load_dataset(f'{data_dir}/MiNNLO/converted_with_13TeV_NLO/normed_converted_lhe.npz', i=3)\n",
    "print('MiNNLO all particles x1_nrm.shape: '+str(x1_nrm.shape))\n",
    "\n",
    "# plotting data\n",
    "x1_plt = []\n",
    "x1_plt = DCTR.load_dataset(f'{data_dir}/MiNNLO/converted_with_13TeV_NLO/converted_lhe.npz', i=3)\n",
    "print('MiNNLO all particles x1_plt.shape: '+str(x1_plt.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normalized event generator weights\n",
    "x0_wgt = x0_nrm[:, 0, 7].copy()\n",
    "x0_wgt /= np.mean(x0_wgt) # adjust so mean is 1\n",
    "\n",
    "x0_plt_wgt = x0_plt_nrm[:, 0, 7].copy()\n",
    "x0_plt_wgt /= np.mean(x0_plt_wgt) \n",
    "\n",
    "x1_wgt = x1_nrm[:, 0, 7].copy()\n",
    "x1_wgt /= np.mean(x1_wgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete eta (pseudorapidity) and Energy -> Train only with [pt, y, phi, m, PID]\n",
    "\n",
    "# delete energy\n",
    "x0_nrm = np.delete(x0_nrm, 5, -1)\n",
    "x0_plt_nrm = np.delete(x0_plt_nrm, 5, -1)\n",
    "x1_nrm = np.delete(x1_nrm, 5, -1)\n",
    "\n",
    "# delete eta\n",
    "x0_nrm = np.delete(x0_nrm, 4, -1)\n",
    "x0_plt_nrm = np.delete(x0_plt_nrm, 4, -1)\n",
    "x1_nrm = np.delete(x1_nrm, 4, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 10:53:43.176483: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 10:53:43.832776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 425 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2024-03-20 10:53:43.863020: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1759854480 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# prep arrays for training\n",
    "x_train, x_val, y_train, y_val, wgt_train, wgt_val = DCTR.prep_arrays(x0_nrm, x1_nrm, val=0.25)\n",
    "\n",
    "# bring into shape for training loop\n",
    "train_data = (x_train, y_train, x_val, y_val, wgt_train, wgt_val)\n",
    "plt_data = (x0_plt , x0_plt_nrm, x1_plt, x1_wgt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 0\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:02:25.423240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2024-03-10 09:02:37.106260: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2119 of run 0 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:03:23.425631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2120 of run 1 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:04:19.618212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2119 of run 2 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:05:15.591072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2119 of run 3 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:06:11.728631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2120 of run 4 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 0 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:07:08.044616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2120 of run 5 of super_epoch 0 with batch_size 65536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:08:05.587469: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-10 09:08:05.964368: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-10 09:08:12.849483: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-10 09:08:13.212981: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 0\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_3/s-0_b-65536_r-3.tf\n",
      "with chi2 7.7468 and loss 0.2119\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:13:06.386733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2122 of run 0 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:14:03.339318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2122 of run 1 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:14:59.910413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2122 of run 2 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:15:56.732468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2122 of run 3 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:16:53.620819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2121 of run 4 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 0 with batch_size 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:17:49.894435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2122 of run 5 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 0\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_4/s-0_b-131072_r-4.tf\n",
      "with chi2 4.9728 and loss 0.2121\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:23:48.559851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2126 of run 0 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:24:46.126650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2129 of run 1 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:25:43.302804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2126 of run 2 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:26:40.911633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2125 of run 3 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:27:38.515421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2126 of run 4 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 0 with batch_size 262144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 09:28:35.833038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6212 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2129 of run 5 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 0\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_0/s-0_b-262144_r-0.tf\n",
      "with chi2 7.2374 and loss 0.2126\n",
      "\n",
      "\n",
      "finished super_epoch 0 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_4/s-0_b-131072_r-4.tfwith chi2 4.9728 and loss 0.2121\n",
      "starting super_epoch 1\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_4/s-0_b-131072_r-4.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2117 of run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2118 of run 5 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 1\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "with chi2 4.1339 and loss 0.2117\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_4/s-0_b-131072_r-4.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2118 of run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2118 of run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2117 of run 5 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 1\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "with chi2 4.0325 and loss 0.2118\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_0/run_4/s-0_b-131072_r-4.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2118 of run 0 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2118 of run 1 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2118 of run 2 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2118 of run 3 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      " best loss 0.2118 of run 4 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0012000000569969416.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.000720000034198165.\n",
      "\n",
      " best loss 0.2118 of run 5 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 1\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_5/s-1_b-262144_r-5.tf\n",
      "with chi2 5.0840 and loss 0.2118\n",
      "\n",
      "\n",
      "finished super_epoch 1 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_1/s-1_b-131072_r-1.tfwith chi2 4.0325 and loss 0.2118\n",
      "starting super_epoch 2\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 2\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_5/s-2_b-65536_r-5.tf\n",
      "with chi2 3.3524 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 2\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_0/s-2_b-131072_r-0.tf\n",
      "with chi2 2.9540 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0004320000065490603.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 2\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "with chi2 3.0560 and loss 0.2116\n",
      "\n",
      "\n",
      "finished super_epoch 2 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_0/s-2_b-131072_r-0.tfwith chi2 2.9540 and loss 0.2116\n",
      "starting super_epoch 3\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_0/s-2_b-131072_r-0.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 3\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_2/s-3_b-65536_r-2.tf\n",
      "with chi2 2.8657 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_0/s-2_b-131072_r-0.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 3\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_3/s-3_b-131072_r-3.tf\n",
      "with chi2 2.6113 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_2/run_0/s-2_b-131072_r-0.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00025920000043697653.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00015551999676972626.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 3\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_1/s-3_b-262144_r-1.tf\n",
      "with chi2 2.6945 and loss 0.2116\n",
      "\n",
      "\n",
      "finished super_epoch 3 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_3/s-3_b-131072_r-3.tfwith chi2 2.6113 and loss 0.2116\n",
      "starting super_epoch 4\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_3/s-3_b-131072_r-3.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 4\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "with chi2 2.2413 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_3/s-3_b-131072_r-3.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 4\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "with chi2 2.1068 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_3/run_3/s-3_b-131072_r-3.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.331199980806559e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 5.598720163106918e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 4\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_0/s-4_b-262144_r-0.tf\n",
      "with chi2 2.4568 and loss 0.2116\n",
      "\n",
      "\n",
      "finished super_epoch 4 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 5\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 5\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_5/run_0/s-5_b-65536_r-0.tf\n",
      "with chi2 2.4718 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 5\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_5/run_1/s-5_b-131072_r-1.tf\n",
      "with chi2 2.2554 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 5\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_5/run_1/s-5_b-262144_r-1.tf\n",
      "with chi2 2.4748 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 5 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 6\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 6\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_6/run_5/s-6_b-65536_r-5.tf\n",
      "with chi2 2.2636 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 6\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_6/run_1/s-6_b-131072_r-1.tf\n",
      "with chi2 2.6366 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 6\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_6/run_4/s-6_b-262144_r-4.tf\n",
      "with chi2 2.5180 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 6 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 7\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 7\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_7/run_1/s-7_b-65536_r-1.tf\n",
      "with chi2 2.3049 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 7\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_7/run_5/s-7_b-131072_r-5.tf\n",
      "with chi2 2.4817 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 7\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_7/run_5/s-7_b-262144_r-5.tf\n",
      "with chi2 2.4762 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 7 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 8\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 8\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_8/run_2/s-8_b-65536_r-2.tf\n",
      "with chi2 2.2976 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 8\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_8/run_4/s-8_b-131072_r-4.tf\n",
      "with chi2 2.4069 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 8\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_8/run_1/s-8_b-262144_r-1.tf\n",
      "with chi2 2.5765 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 8 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 9\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 9\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_9/run_5/s-9_b-65536_r-5.tf\n",
      "with chi2 2.2825 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 9\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_9/run_4/s-9_b-131072_r-4.tf\n",
      "with chi2 2.3000 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 9\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_9/run_4/s-9_b-262144_r-4.tf\n",
      "with chi2 2.4582 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 9 with 6 runs each with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tfwith chi2 2.1068 and loss 0.2116\n",
      "starting super_epoch 10\n",
      "\n",
      "starting training with batch_size: 65536 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 65536\n",
      "in super epoch 10\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_10/run_2/s-10_b-65536_r-2.tf\n",
      "with chi2 2.3341 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 131072\n",
      "in super epoch 10\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_10/run_4/s-10_b-131072_r-4.tf\n",
      "with chi2 2.4562 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 6 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 262144\n",
      "in super epoch 10\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_10/run_3/s-10_b-262144_r-3.tf\n",
      "with chi2 2.4512 and loss 0.2116\n",
      "super_patiece reached. Stopping training.\n",
      "\n",
      "\n",
      "\n",
      "finished loop of 30 super_epochs\n",
      "with batch_sizes:[65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "with chi2 2.1068 and loss 0.2116\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lowest_chi2_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mtrain_data, plt_data, weights=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mbatch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03minput_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mPhi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.002\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# chi2 is actually reduced chi2 -> 1 would be optimal\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 289\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    286\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[1;32m    287\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow([model])\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_list, \u001b[43mlowest_chi2_list\u001b[49m, lowest_loss_list\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lowest_chi2_list' is not defined"
     ]
    }
   ],
   "source": [
    "# second session\n",
    "\n",
    "K.clear_session()\n",
    "'''\n",
    "train_data, plt_data, model=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\n",
    "batch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \n",
    "input_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \n",
    "Phi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\n",
    "'''\n",
    "# train_loop(train_data, plt_data, batch_sizes=[8*8192, 16*8192, 32*8192], repeat=6, super_epochs=30, \n",
    "           train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310', epochs=6, dropout=0.05, learning_rate=0.002)\n",
    "\n",
    "# chi2 is actually reduced chi2 -> 1 would be optimal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 30\n",
      "\n",
      "starting training with batch_size: 16384 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 4 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 5 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 6 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 6 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 7 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 7 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 8 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "starting run 9 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 30 with batch_size 16384\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 16384\n",
      "in super epoch 30\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "with chi2 2.0807 and loss 0.2116\n",
      "starting training with batch_size: 32768 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 5 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 6 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 7 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 8 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 8 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "starting run 9 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 9 of super_epoch 30 with batch_size 32768\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 32768\n",
      "in super epoch 30\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_1/s-30_b-32768_r-1.tf\n",
      "with chi2 2.1112 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 8 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "starting run 9 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 30 with batch_size 65536\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 65536\n",
      "in super epoch 30\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_1/s-30_b-65536_r-1.tf\n",
      "with chi2 2.2414 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 8 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "starting run 9 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 30 with batch_size 131072\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 131072\n",
      "in super epoch 30\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_0/s-30_b-131072_r-0.tf\n",
      "with chi2 2.4518 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\n",
      "starting run 0 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 7 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 8 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "starting run 9 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 3.359232141519897e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.015539284911938e-05.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 30 with batch_size 262144\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 262144\n",
      "in super epoch 30\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_9/s-30_b-262144_r-9.tf\n",
      "with chi2 2.3525 and loss 0.2115\n",
      "\n",
      "\n",
      "finished super_epoch 30 with 10 runs each with batch_sizes:[16384, 32768, 65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tfwith chi2 2.0807 and loss 0.2116\n",
      "starting super_epoch 31\n",
      "\n",
      "starting training with batch_size: 16384 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 4 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 5 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 6 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 7 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 8 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "starting run 9 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 31 with batch_size 16384\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 16384\n",
      "in super epoch 31\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_31/run_4/s-31_b-16384_r-4.tf\n",
      "with chi2 2.1474 and loss 0.2115\n",
      "starting training with batch_size: 32768 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 5 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 6 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 7 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 8 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "starting run 9 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 31 with batch_size 32768\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 32768\n",
      "in super epoch 31\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_31/run_7/s-31_b-32768_r-7.tf\n",
      "with chi2 2.2663 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 8 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "starting run 9 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 31 with batch_size 65536\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 65536\n",
      "in super epoch 31\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_31/run_1/s-31_b-65536_r-1.tf\n",
      "with chi2 2.3965 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 8 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "starting run 9 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 31 with batch_size 131072\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 131072\n",
      "in super epoch 31\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_31/run_7/s-31_b-131072_r-7.tf\n",
      "with chi2 2.3466 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 7 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 8 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "starting run 9 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 31 with batch_size 262144\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 262144\n",
      "in super epoch 31\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_31/run_7/s-31_b-262144_r-7.tf\n",
      "with chi2 2.4247 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 31 with 10 runs each with batch_sizes:[16384, 32768, 65536, 131072, 262144]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tfwith chi2 2.0807 and loss 0.2116\n",
      "starting super_epoch 32\n",
      "\n",
      "starting training with batch_size: 16384 and 10 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 4 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 5 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 6 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 7 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 8 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 32 with batch_size 16384\n",
      "\n",
      "starting run 9 of super_epoch 32 with batch_size 16384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mtrain_data, plt_data, model=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mbatch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03minput_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mPhi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# chi2 is actually reduced chi2 -> 1 would be optimal\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 216\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    215\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 165\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_super_epoch_choose_best\u001b[39m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    161\u001b[0m                                   input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, Phi_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m128\u001b[39m), F_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m), loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    162\u001b[0m                                   Phi_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m), F_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m), output_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     dctr, model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 147\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    144\u001b[0m     dctr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# loss_val = dctr.evaluate(x_val, y_val, sample_weight=pd.Series(wgt_val).to_frame('w_v'), batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:634\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, wgt_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[1;32m    625\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, savePath\u001b[38;5;241m=\u001b[39mcurrentPath, saveLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDCTR_training\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    method to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    allows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    plots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1435\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1432\u001b[0m   val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m   epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1435\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_logs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1436\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m epoch_logs\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:416\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    414\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 416\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1388\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m-> 1388\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1443\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_weights(\n\u001b[1;32m   1441\u001b[0m         filepath, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m   1442\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1445\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:2384\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \n\u001b[1;32m   2344\u001b[0m \u001b[38;5;124;03mPlease see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2383\u001b[0m \u001b[38;5;66;03m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[0;32m-> 2384\u001b[0m \u001b[43msave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2385\u001b[0m \u001b[43m                \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/save.py:151\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m generic_utils\u001b[38;5;241m.\u001b[39mSharedObjectSavingScope():\n\u001b[0;32m--> 151\u001b[0m     \u001b[43msaved_model_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                          \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_traces\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/save.py:93\u001b[0m, in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mdeprecated_internal_learning_phase_scope(\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mkeras_option_scope(save_traces):\n\u001b[0;32m---> 93\u001b[0m     saved_nodes, node_paths \u001b[38;5;241m=\u001b[39m \u001b[43msave_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_and_return_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m   \u001b[38;5;66;03m# Save all metadata to a separate file in the SavedModel directory.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m   metadata \u001b[38;5;241m=\u001b[39m generate_keras_metadata(saved_nodes, node_paths)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1369\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1365\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m saved_model_pb2\u001b[38;5;241m.\u001b[39mSavedModel()\n\u001b[1;32m   1366\u001b[0m meta_graph_def \u001b[38;5;241m=\u001b[39m saved_model\u001b[38;5;241m.\u001b[39mmeta_graphs\u001b[38;5;241m.\u001b[39madd()\n\u001b[1;32m   1368\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1369\u001b[0m     \u001b[43m_build_meta_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1370\u001b[0m saved_model\u001b[38;5;241m.\u001b[39msaved_model_schema_version \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1371\u001b[0m     constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;66;03m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1536\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;124;03m  asset_info: `_AssetInfo` tuple containing external assets in the `obj`.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save_context\u001b[38;5;241m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1536\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_meta_graph_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_graph_def\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:1477\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1475\u001b[0m checkpoint_graph_view \u001b[38;5;241m=\u001b[39m _AugmentedGraphView(obj)\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1477\u001b[0m   signatures \u001b[38;5;241m=\u001b[39m \u001b[43msignature_serialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_function_to_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_graph_view\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m signatures, wrapped_functions \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1481\u001b[0m     signature_serialization\u001b[38;5;241m.\u001b[39mcanonicalize_signatures(signatures))\n\u001b[1;32m   1482\u001b[0m signature_serialization\u001b[38;5;241m.\u001b[39mvalidate_saveable_view(checkpoint_graph_view)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/signature_serialization.py:103\u001b[0m, in \u001b[0;36mfind_function_to_export\u001b[0;34m(saveable_view)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# TODO(b/205014194): Discuss removing this behaviour. It can lead to WTFs when\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# a user decides to annotate more functions with tf.function and suddenly\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# serving that model way later in the process stops working.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m possible_signatures \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m children:\n\u001b[1;32m    104\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, (def_function\u001b[38;5;241m.\u001b[39mFunction, defun\u001b[38;5;241m.\u001b[39mConcreteFunction)):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py:133\u001b[0m, in \u001b[0;36m_AugmentedGraphView.list_children\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lists children of `obj` for SavedModel.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children_cache:\n\u001b[1;32m    132\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children_cache[obj] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m--> 133\u001b[0m       \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AugmentedGraphView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_children\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m          \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m          \u001b[49m\u001b[43msave_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSaveType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVEDMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialization_cache\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children_cache[obj]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    138\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m base\u001b[38;5;241m.\u001b[39mTrackableReference(name, child)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/graph_view.py:256\u001b[0m, in \u001b[0;36mObjectGraphView.list_children\u001b[0;34m(self, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    254\u001b[0m obj\u001b[38;5;241m.\u001b[39m_maybe_initialize_trackable()\n\u001b[1;32m    255\u001b[0m children \u001b[38;5;241m=\u001b[39m [base\u001b[38;5;241m.\u001b[39mTrackableReference(name, ref) \u001b[38;5;28;01mfor\u001b[39;00m name, ref\n\u001b[0;32m--> 256\u001b[0m             \u001b[38;5;129;01min\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trackable_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# GraphView objects may define children of the root object that are not\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# actually attached, e.g. a Checkpoint object's save_counter.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attached_dependencies:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py:1479\u001b[0m, in \u001b[0;36mTrackable._trackable_children\u001b[0;34m(self, save_type, **kwargs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m save_type \u001b[38;5;241m==\u001b[39m SaveType\u001b[38;5;241m.\u001b[39mSAVEDMODEL:\n\u001b[1;32m   1478\u001b[0m   cache \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1479\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_legacy_saved_model_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1481\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected format passed to `_trackable_children`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1482\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py:1499\u001b[0m, in \u001b[0;36mTrackable._get_legacy_saved_model_children\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fn, core_types\u001b[38;5;241m.\u001b[39mGenericFunction):\n\u001b[0;32m-> 1499\u001b[0m     \u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_all_concrete_functions_for_serialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# Retrieve children that are only included when exporting SavedModel.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m extra_dependencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_extra_dependencies_for_serialization(\n\u001b[1;32m   1503\u001b[0m     serialization_cache)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:1220\u001b[0m, in \u001b[0;36mFunction._list_all_concrete_functions_for_serialization\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1218\u001b[0m concrete_functions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m args, kwargs \u001b[38;5;129;01min\u001b[39;00m seen_signatures:\n\u001b[0;32m-> 1220\u001b[0m   concrete_functions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_functions\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:1264\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:1255\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m   \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m   1254\u001b[0m   \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m-> 1255\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1256\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1257\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1259\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:3036\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3034\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 3036\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3037\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m   3038\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   3039\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:3292\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3289\u001b[0m       args, kwargs, flat_args, filtered_flat_args)\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd_call_context(cache_key\u001b[38;5;241m.\u001b[39mcall_context)\n\u001b[0;32m-> 3292\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                          graph_function)\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3125\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3127\u001b[0m ]\n\u001b[1;32m   3128\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3129\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3142\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3143\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:1161\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1161\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m   1166\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:677\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    674\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 677\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/save_impl.py:572\u001b[0m, in \u001b[0;36mlayer_call_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\u001b[38;5;241m.\u001b[39menter(\n\u001b[1;32m    568\u001b[0m     layer, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[1;32m    569\u001b[0m     saving\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m    571\u001b[0m       layer\u001b[38;5;241m.\u001b[39m_compute_dtype_object):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m _restore_layer_losses(original_losses)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/utils.py:168\u001b[0m, in \u001b[0;36mmaybe_add_training_arg.<locals>.wrap_with_training_arg\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m   set_training_arg(training, training_arg_index, args, kwargs)\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontrol_flow_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_training_and_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace_training_and_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/control_flow_util.py:105\u001b[0m, in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pred, tf\u001b[38;5;241m.\u001b[39mVariable):\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcond(\n\u001b[1;32m    104\u001b[0m       pred, true_fn\u001b[38;5;241m=\u001b[39mtrue_fn, false_fn\u001b[38;5;241m=\u001b[39mfalse_fn, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfalse_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/smart_cond.py:55\u001b[0m, in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m true_fn()\n\u001b[1;32m     54\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfalse_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m control_flow_ops\u001b[38;5;241m.\u001b[39mcond(pred, true_fn\u001b[38;5;241m=\u001b[39mtrue_fn, false_fn\u001b[38;5;241m=\u001b[39mfalse_fn,\n\u001b[1;32m     58\u001b[0m                                name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/utils.py:170\u001b[0m, in \u001b[0;36mmaybe_add_training_arg.<locals>.wrap_with_training_arg.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m   set_training_arg(training, training_arg_index, args, kwargs)\n\u001b[1;32m    166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m control_flow_util\u001b[38;5;241m.\u001b[39msmart_cond(\n\u001b[1;32m    169\u001b[0m     training, \u001b[38;5;28;01mlambda\u001b[39;00m: replace_training_and_call(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mreplace_training_and_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/utils.py:166\u001b[0m, in \u001b[0;36mmaybe_add_training_arg.<locals>.wrap_with_training_arg.<locals>.replace_training_and_call\u001b[0;34m(training)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_training_and_call\u001b[39m(training):\n\u001b[1;32m    165\u001b[0m   set_training_arg(training, training_arg_index, args, kwargs)\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/save_impl.py:652\u001b[0m, in \u001b[0;36m_extract_outputs_from_fn.<locals>.call\u001b[0;34m(inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 652\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_return_conditional_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/save_impl.py:610\u001b[0m, in \u001b[0;36mLayerCall.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    609\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_trace(args, kwargs)\n\u001b[0;32m--> 610\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1867\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1864\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_override_gradient_function(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1865\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function(),\n\u001b[1;32m   1866\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatefulPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function()}):\n\u001b[0;32m-> 1867\u001b[0m     flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mforward_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_tangents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1868\u001b[0m forward_backward\u001b[38;5;241m.\u001b[39mrecord(flat_outputs)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:527\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_control_captures):\n\u001b[1;32m    521\u001b[0m       \u001b[38;5;66;03m# The caller must use record_operation to record this operation in the\u001b[39;00m\n\u001b[1;32m    522\u001b[0m       \u001b[38;5;66;03m# eager case, so we enforce the same requirement for the non-eager\u001b[39;00m\n\u001b[1;32m    523\u001b[0m       \u001b[38;5;66;03m# case by explicitly pausing recording. We don't have a gradient\u001b[39;00m\n\u001b[1;32m    524\u001b[0m       \u001b[38;5;66;03m# registered for PartitionedCall, so recording this operation confuses\u001b[39;00m\n\u001b[1;32m    525\u001b[0m       \u001b[38;5;66;03m# forwardprop code (GradientTape manages to ignore it).\u001b[39;00m\n\u001b[1;32m    526\u001b[0m       \u001b[38;5;28;01mwith\u001b[39;00m tape\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m--> 527\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitioned_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexecutor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, func_graph_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph_outputs):\n\u001b[1;32m    536\u001b[0m   handle_data_util\u001b[38;5;241m.\u001b[39mcopy_handle_data(func_graph_output, outputs[i])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/functional_ops.py:1190\u001b[0m, in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# The generated binding returns an empty list for functions that don't\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# return any Tensors, hence the need to use `create_op` directly.\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m args \u001b[38;5;241m=\u001b[39m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   1191\u001b[0m tin_attr \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39mattr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue\u001b[38;5;241m.\u001b[39mListValue(\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m[x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mas_datatype_enum \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]))\n\u001b[1;32m   1194\u001b[0m tout_attr \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39mattr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue\u001b[38;5;241m.\u001b[39mListValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtout))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/functional_ops.py:1190\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1186\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# The generated binding returns an empty list for functions that don't\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# return any Tensors, hence the need to use `create_op` directly.\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   1191\u001b[0m tin_attr \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39mattr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue\u001b[38;5;241m.\u001b[39mListValue(\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m[x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mas_datatype_enum \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]))\n\u001b[1;32m   1194\u001b[0m tout_attr \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39mattr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue\u001b[38;5;241m.\u001b[39mListValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtout))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1656\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mbuilding_function:\n\u001b[1;32m   1654\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to capture an EagerTensor without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1655\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding a function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1659\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:729\u001b[0m, in \u001b[0;36mFuncGraph.capture\u001b[0;34m(self, tensor, name, shape)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture_eager_tensor(tensor, name)\n\u001b[1;32m    728\u001b[0m   \u001b[38;5;66;03m# Large EagerTensors and resources are captured with Placeholder ops\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    731\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:764\u001b[0m, in \u001b[0;36mFuncGraph._capture_helper\u001b[0;34m(self, tensor, name, shape)\u001b[0m\n\u001b[1;32m    762\u001b[0m capture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mid\u001b[39m(tensor))\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capture \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[43m_create_substitute_placeholder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m   \u001b[38;5;66;03m# Record the composite device as an attribute to the placeholder.\u001b[39;00m\n\u001b[1;32m    767\u001b[0m   \u001b[38;5;66;03m# This attribute would be propogated into the arg_attr of the FunctionDef.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m   \u001b[38;5;66;03m# Currently, a packed eager tensor is always placed on a CompositeDevice.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, ops\u001b[38;5;241m.\u001b[39mEagerTensor) \u001b[38;5;129;01mand\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_packed:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:1305\u001b[0m, in \u001b[0;36m_create_substitute_placeholder\u001b[0;34m(value, name, dtype, shape)\u001b[0m\n\u001b[1;32m   1303\u001b[0m   shape \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies(\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1305\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_placeholder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m handle_data_util\u001b[38;5;241m.\u001b[39mcopy_handle_data(value, placeholder)\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m placeholder\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/graph_only_ops.py:34\u001b[0m, in \u001b[0;36mgraph_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m     33\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape}\n\u001b[0;32m---> 34\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlaceholder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m result, \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:693\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    691\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[1;32m    692\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:3776\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 3776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3783\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3785\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:2171\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2169\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m op_def \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2170\u001b[0m     op_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_get_op_def(node_def\u001b[38;5;241m.\u001b[39mop)\n\u001b[0;32m-> 2171\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2173\u001b[0m   name \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   2175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traceback \u001b[38;5;241m=\u001b[39m tf_stack\u001b[38;5;241m.\u001b[39mextract_stack_for_node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:2002\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1998\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_AddControlInput(op_desc, control_input\u001b[38;5;241m.\u001b[39m_c_op)\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;66;03m# pylint: enable=protected-access\u001b[39;00m\n\u001b[1;32m   2000\u001b[0m \n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# Add attrs\u001b[39;00m\n\u001b[0;32m-> 2002\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, attr_value \u001b[38;5;129;01min\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mattr\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2003\u001b[0m   serialized \u001b[38;5;241m=\u001b[39m attr_value\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m   2004\u001b[0m   \u001b[38;5;66;03m# TODO(skyewm): this creates and deletes a new TF_Status for every attr.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m   \u001b[38;5;66;03m# It might be worth creating a convenient way to re-use the same status.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/_collections_abc.py:743\u001b[0m, in \u001b[0;36mItemsView.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping:\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m (key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping[key])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# second session continuation\n",
    "\n",
    "K.clear_session()\n",
    "'''\n",
    "train_data, plt_data, model=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\n",
    "batch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \n",
    "input_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \n",
    "Phi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\n",
    "'''\n",
    "# train_loop(train_data, plt_data, batch_sizes=[2*8192, 4*8192, 8*8192, 16*8192, 32*8192], repeat=10, super_epochs=10, starting_super_epoch = 30,\n",
    "           model = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_4/run_5/s-4_b-131072_r-5.tf',\n",
    "           train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310', epochs=10, dropout=0.1, learning_rate=0.001)\n",
    "\n",
    "# chi2 is actually reduced chi2 -> 1 would be optimal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 33\n",
      "\n",
      "starting training with batch_size: 8192 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 33 with batch_size 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 09:07:08.094500: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 6 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 7 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 8 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "starting run 9 of super_epoch 33 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 33 with batch_size 8192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 09:34:45.703244: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 09:34:46.084054: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 09:34:52.996159: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 09:34:53.377724: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 8192\n",
      "in super epoch 33\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "with chi2 2.2136 and loss 0.2115\n",
      "starting training with batch_size: 16384 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 4 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 5 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 6 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 7 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 8 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "starting run 9 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 33 with batch_size 16384\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 16384\n",
      "in super epoch 33\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_5/s-33_b-16384_r-5.tf\n",
      "with chi2 2.2517 and loss 0.2115\n",
      "starting training with batch_size: 32768 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 5 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 6 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 7 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 8 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "starting run 9 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 33 with batch_size 32768\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 32768\n",
      "in super epoch 33\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_7/s-33_b-32768_r-7.tf\n",
      "with chi2 2.3293 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 8 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "starting run 9 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 7.041496064630337e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 33 with batch_size 65536\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 65536\n",
      "in super epoch 33\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_4/s-33_b-65536_r-4.tf\n",
      "with chi2 2.2562 and loss 0.2115\n",
      "\n",
      "\n",
      "finished super_epoch 33 with 10 runs each with batch_sizes:[8192, 16384, 32768, 65536]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tfwith chi2 2.2136 and loss 0.2115\n",
      "starting super_epoch 34\n",
      "\n",
      "starting training with batch_size: 8192 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "starting run 0 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 6 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 7 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 8 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "starting run 9 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 34 with batch_size 8192\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 8192\n",
      "in super epoch 34\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_34/run_3/s-34_b-8192_r-3.tf\n",
      "with chi2 2.3590 and loss 0.2115\n",
      "starting training with batch_size: 16384 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "starting run 0 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 4 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 5 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 6 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 7 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 8 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "starting run 9 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 34 with batch_size 16384\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 16384\n",
      "in super epoch 34\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_34/run_1/s-34_b-16384_r-1.tf\n",
      "with chi2 2.3214 and loss 0.2115\n",
      "starting training with batch_size: 32768 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "starting run 0 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 5 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 6 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 7 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 8 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "starting run 9 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 34 with batch_size 32768\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 32768\n",
      "in super epoch 34\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_34/run_9/s-34_b-32768_r-9.tf\n",
      "with chi2 2.3883 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "starting run 0 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 5 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 6 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 7 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 8 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 8 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "starting run 9 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 9 of super_epoch 34 with batch_size 65536\n",
      "\n",
      "calculating stats for 10 models\n",
      "\n",
      "finished 10 runs of batch_size 65536\n",
      "in super epoch 34\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_34/run_3/s-34_b-65536_r-3.tf\n",
      "with chi2 2.4004 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      "finished super_epoch 34 with 10 runs each with batch_sizes:[8192, 16384, 32768, 65536]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tfwith chi2 2.2136 and loss 0.2115\n",
      "starting super_epoch 35\n",
      "\n",
      "starting training with batch_size: 8192 and 15 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_33/run_2/s-33_b-8192_r-2.tf\n",
      "starting run 0 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.141239423915976e-06.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 2.4113084691634865e-06.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 35 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 35 with batch_size 8192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 216\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    215\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 165\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_super_epoch_choose_best\u001b[39m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    161\u001b[0m                                   input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, Phi_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m128\u001b[39m), F_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m), loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    162\u001b[0m                                   Phi_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m), F_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m), output_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     dctr, model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 147\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    144\u001b[0m     dctr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# loss_val = dctr.evaluate(x_val, y_val, sample_weight=pd.Series(wgt_val).to_frame('w_v'), batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m    151\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:634\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, wgt_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[1;32m    625\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, savePath\u001b[38;5;241m=\u001b[39mcurrentPath, saveLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDCTR_training\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    method to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    allows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    plots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1408\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1409\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1419\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[0;32m-> 1420\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m   1433\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1716\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1715\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1716\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1717\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1718\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(train_data, plt_data, batch_sizes=[8192, 2*8192, 4*8192, 8*8192], repeat=10, super_epochs=15, starting_super_epoch = 33, lowest_chi2 = 3,\n",
    "           model = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf',\n",
    "           train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310', epochs=15, dropout=0.15, learning_rate=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 37\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 37 with batch_size 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 14:11:28.774413: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 37 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 37 with batch_size 4096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 14:22:40.296996: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 14:22:41.017236: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 14:22:48.438809: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-11 14:22:49.287432: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 37\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "with chi2 2.0931 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
      "starting run 0 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.209323527291417e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 7.255941272887867e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 37 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 37\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_0/s-37_b-8192_r-0.tf\n",
      "with chi2 2.2309 and loss 0.2116\n",
      "\n",
      "\n",
      "finished super_epoch 37 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 38\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 38 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 38\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_38/run_2/s-38_b-4096_r-2.tf\n",
      "with chi2 2.2255 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 38 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 38\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_38/run_5/s-38_b-8192_r-5.tf\n",
      "with chi2 2.2897 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 38 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 39\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 39 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 39\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_39/run_4/s-39_b-4096_r-4.tf\n",
      "with chi2 2.2480 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 39 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 39\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_39/run_0/s-39_b-8192_r-0.tf\n",
      "with chi2 2.2733 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 39 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 40\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 40 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 40\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_40/run_5/s-40_b-4096_r-5.tf\n",
      "with chi2 2.3467 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 40 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 40\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_40/run_3/s-40_b-8192_r-3.tf\n",
      "with chi2 2.2642 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 40 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 41\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 41 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 41\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_41/run_0/s-41_b-4096_r-0.tf\n",
      "with chi2 2.1216 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 41 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 41\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_41/run_5/s-41_b-8192_r-5.tf\n",
      "with chi2 2.2843 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to 7e-05\n",
      "\n",
      "\n",
      "finished super_epoch 41 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 42\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 42 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 42\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_42/run_5/s-42_b-4096_r-5.tf\n",
      "with chi2 2.3320 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 42 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 42\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_42/run_0/s-42_b-8192_r-0.tf\n",
      "with chi2 2.2427 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to 4.899999999999999e-05\n",
      "\n",
      "\n",
      "finished super_epoch 42 with 6 runs each with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tfwith chi2 2.0931 and loss 0.2116\n",
      "starting super_epoch 43\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "starting run 4 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "starting run 5 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 43 with batch_size 4096\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 4096\n",
      "in super epoch 43\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_43/run_5/s-43_b-4096_r-5.tf\n",
      "with chi2 2.3185 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting with weights from model: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "starting run 0 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "starting run 4 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "starting run 5 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 4.3535648728720845e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.6121388145838864e-06.\n",
      "\n",
      " best loss 0.2116 of run 5 of super_epoch 43 with batch_size 8192\n",
      "\n",
      "calculating stats for 6 models\n",
      "\n",
      "finished 6 runs of batch_size 8192\n",
      "in super epoch 43\n",
      "with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_43/run_3/s-43_b-8192_r-3.tf\n",
      "with chi2 2.3088 and loss 0.2116\n",
      "super_patiece reached. Stopping training.\n",
      "\n",
      "\n",
      "\n",
      "finished loop of 7 super_epochs\n",
      "with batch_sizes:[4096, 8192]\n",
      "best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n",
      "with chi2 2.0931 and loss 0.2116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf'],\n",
       " [2.0930774819210742],\n",
       " [0.21157675981521606])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop(train_data, plt_data, batch_sizes=[4096, 8192], repeat=6, super_epochs=7, starting_super_epoch = 37, lowest_chi2 = 2.1,\n",
    "           model = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf', \n",
    "           train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310', epochs=8, dropout=0.02, learning_rate=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models\n",
    "# 2.0807: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_30/run_6/s-30_b-16384_r-6.tf\n",
    "# 2.0931: /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_20240310/super_epoch_37/run_1/s-37_b-4096_r-1.tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 0\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 0 with batch_size 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 14:18:28.675571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6153 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " best loss 0.2119 of run 0 of super_epoch 0 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 32768\n",
      "\n",
      " best loss 0.2119 of run 1 of super_epoch 0 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 32768\n",
      "\n",
      " best loss 0.2120 of run 2 of super_epoch 0 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 32768\n",
      "\n",
      " best loss 0.2120 of run 3 of super_epoch 0 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 32768\n",
      "\n",
      " best loss 0.2119 of run 4 of super_epoch 0 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 0\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_0/run_1/s-0_b-32768_r-1.tf\n",
      "    with chi2 12.3689 and loss 0.2119\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 0 with batch_size 65536\n",
      "\n",
      " best loss 0.2123 of run 0 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 65536\n",
      "\n",
      " best loss 0.2121 of run 1 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 65536\n",
      "\n",
      " best loss 0.2121 of run 2 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 65536\n",
      "\n",
      " best loss 0.2122 of run 3 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 65536\n",
      "\n",
      " best loss 0.2121 of run 4 of super_epoch 0 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 0\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_0/run_3/s-0_b-65536_r-3.tf\n",
      "    with chi2 9.0144 and loss 0.2122\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 0 with batch_size 131072\n",
      "\n",
      " best loss 0.2128 of run 0 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 131072\n",
      "\n",
      " best loss 0.2130 of run 1 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 131072\n",
      "\n",
      " best loss 0.2128 of run 2 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 131072\n",
      "\n",
      " best loss 0.2131 of run 3 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 131072\n",
      "\n",
      " best loss 0.2131 of run 4 of super_epoch 0 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 0\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_0/run_0/s-0_b-131072_r-0.tf\n",
      "    with chi2 22.0552 and loss 0.2128\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 0 with batch_size 262144\n",
      "\n",
      " best loss 0.2141 of run 0 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 0 with batch_size 262144\n",
      "\n",
      " best loss 0.2143 of run 1 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 0 with batch_size 262144\n",
      "\n",
      " best loss 0.2140 of run 2 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 0 with batch_size 262144\n",
      "\n",
      " best loss 0.2143 of run 3 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 0 with batch_size 262144\n",
      "\n",
      " best loss 0.2141 of run 4 of super_epoch 0 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 0\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_0/run_2/s-0_b-262144_r-2.tf\n",
      "    with chi2 110.1722 and loss 0.2140\n",
      "\n",
      "\n",
      " finished super_epoch 0 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_0/run_3/s-0_b-65536_r-3.tf\n",
      "    with chi2 9.0144 and loss 0.2122\n",
      "starting super_epoch 1\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 1 with batch_size 32768\n",
      "\n",
      " best loss 0.2118 of run 0 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 1 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 3 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 4 of super_epoch 1 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 1\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_1/run_3/s-1_b-32768_r-3.tf\n",
      "    with chi2 5.5679 and loss 0.2118\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 1\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_1/run_0/s-1_b-65536_r-0.tf\n",
      "    with chi2 6.2262 and loss 0.2117\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2117 of run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 1\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_1/run_2/s-1_b-131072_r-2.tf\n",
      "    with chi2 4.9503 and loss 0.2117\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 0 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 1 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 2 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 3 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "\n",
      " best loss 0.2118 of run 4 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 1\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_1/run_2/s-1_b-262144_r-2.tf\n",
      "    with chi2 8.8992 and loss 0.2118\n",
      "\n",
      "\n",
      " finished super_epoch 1 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_1/run_2/s-1_b-131072_r-2.tf\n",
      "    with chi2 4.9503 and loss 0.2117\n",
      "starting super_epoch 2\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 0 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 1 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 4 of super_epoch 2 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 2\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_2/run_0/s-2_b-32768_r-0.tf\n",
      "    with chi2 7.0313 and loss 0.2117\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2117 of run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2117 of run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 2\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_2/run_0/s-2_b-65536_r-0.tf\n",
      "    with chi2 6.6682 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      " best loss 0.2117 of run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 2\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_2/run_2/s-2_b-131072_r-2.tf\n",
      "    with chi2 6.3876 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 2\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "    with chi2 4.6886 and loss 0.2116\n",
      "\n",
      "\n",
      " finished super_epoch 2 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "    with chi2 4.6886 and loss 0.2116\n",
      "starting super_epoch 3\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 3\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_3/run_1/s-3_b-32768_r-1.tf\n",
      "    with chi2 5.4983 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 3\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_3/run_2/s-3_b-65536_r-2.tf\n",
      "    with chi2 4.7611 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 3\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_3/run_4/s-3_b-131072_r-4.tf\n",
      "    with chi2 4.1599 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 3\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_3/run_0/s-3_b-262144_r-0.tf\n",
      "    with chi2 4.2119 and loss 0.2116\n",
      "\n",
      "\n",
      " finished super_epoch 3 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_3/run_4/s-3_b-131072_r-4.tf\n",
      "    with chi2 4.1599 and loss 0.2116\n",
      "starting super_epoch 4\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 4\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_4/s-4_b-32768_r-4.tf\n",
      "    with chi2 4.9318 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 4\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 4\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_1/s-4_b-131072_r-1.tf\n",
      "    with chi2 3.4741 and loss 0.2116\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.6655999904032795e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.799360081553459e-05.\n",
      "\n",
      " best loss 0.2116 of run 4 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 4\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_1/s-4_b-262144_r-1.tf\n",
      "    with chi2 3.5928 and loss 0.2116\n",
      "\n",
      "\n",
      " finished super_epoch 4 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 5\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 5 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 5\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_5/run_2/s-5_b-32768_r-2.tf\n",
      "    with chi2 3.1910 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 5\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_5/run_4/s-5_b-65536_r-4.tf\n",
      "    with chi2 3.6122 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 5\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_5/run_3/s-5_b-131072_r-3.tf\n",
      "    with chi2 3.8359 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 5\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_5/run_1/s-5_b-262144_r-1.tf\n",
      "    with chi2 4.0120 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 5 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 6\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 6 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 6\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_6/run_1/s-6_b-32768_r-1.tf\n",
      "    with chi2 3.7381 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 6\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_6/run_3/s-6_b-65536_r-3.tf\n",
      "    with chi2 3.7000 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 6\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_6/run_0/s-6_b-131072_r-0.tf\n",
      "    with chi2 3.6535 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 6\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_6/run_4/s-6_b-262144_r-4.tf\n",
      "    with chi2 3.9001 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 6 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 7\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 7 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 7\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_7/run_1/s-7_b-32768_r-1.tf\n",
      "    with chi2 3.5690 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 7\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_7/run_4/s-7_b-65536_r-4.tf\n",
      "    with chi2 3.7348 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 7\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_7/run_0/s-7_b-131072_r-0.tf\n",
      "    with chi2 3.5741 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 7\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_7/run_2/s-7_b-262144_r-2.tf\n",
      "    with chi2 3.9510 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 7 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 8\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 8 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 8\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_8/run_4/s-8_b-32768_r-4.tf\n",
      "    with chi2 3.8039 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 8\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_8/run_3/s-8_b-65536_r-3.tf\n",
      "    with chi2 4.4585 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 8\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_8/run_2/s-8_b-131072_r-2.tf\n",
      "    with chi2 3.6753 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 8 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 8\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_8/run_2/s-8_b-262144_r-2.tf\n",
      "    with chi2 4.0613 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 8 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 9\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 9 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 9\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_9/run_0/s-9_b-32768_r-0.tf\n",
      "    with chi2 3.7059 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 9\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_9/run_0/s-9_b-65536_r-0.tf\n",
      "    with chi2 3.8568 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 9\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_9/run_1/s-9_b-131072_r-1.tf\n",
      "    with chi2 4.2341 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 9 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 9\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_9/run_4/s-9_b-262144_r-4.tf\n",
      "    with chi2 3.5405 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 9 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 10\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 10 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 10\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_10/run_0/s-10_b-32768_r-0.tf\n",
      "    with chi2 4.2548 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 10\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_10/run_2/s-10_b-65536_r-2.tf\n",
      "    with chi2 4.1973 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 10\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_10/run_2/s-10_b-131072_r-2.tf\n",
      "    with chi2 3.5909 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 10 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 10\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_10/run_1/s-10_b-262144_r-1.tf\n",
      "    with chi2 3.9846 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 10 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 11\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 11 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 11\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_11/run_2/s-11_b-32768_r-2.tf\n",
      "    with chi2 4.4917 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 11\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_11/run_0/s-11_b-65536_r-0.tf\n",
      "    with chi2 3.5805 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 11\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_11/run_1/s-11_b-131072_r-1.tf\n",
      "    with chi2 3.8675 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 11 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 11\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_11/run_1/s-11_b-262144_r-1.tf\n",
      "    with chi2 3.9534 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 11 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 12\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 12 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 12\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_12/run_2/s-12_b-32768_r-2.tf\n",
      "    with chi2 4.0220 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 12\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_12/run_4/s-12_b-65536_r-4.tf\n",
      "    with chi2 3.4218 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 12\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_12/run_0/s-12_b-131072_r-0.tf\n",
      "    with chi2 3.7626 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 12 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 12\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_12/run_2/s-12_b-262144_r-2.tf\n",
      "    with chi2 3.8944 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 12 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 13\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 13 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 13\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_13/run_2/s-13_b-32768_r-2.tf\n",
      "    with chi2 4.1635 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 13\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_13/run_4/s-13_b-65536_r-4.tf\n",
      "    with chi2 4.5486 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 13\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_13/run_1/s-13_b-131072_r-1.tf\n",
      "    with chi2 3.4934 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 13 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 13\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_13/run_4/s-13_b-262144_r-4.tf\n",
      "    with chi2 4.4342 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 13 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "starting super_epoch 14\n",
      "\n",
      "starting training with batch_size: 32768 and 5 epochs\n",
      "starting run 0 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "starting run 4 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 14 with batch_size 32768\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 32768\n",
      "    in super epoch 14\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_14/run_1/s-14_b-32768_r-1.tf\n",
      "    with chi2 4.0160 and loss 0.2115\n",
      "starting training with batch_size: 65536 and 5 epochs\n",
      "starting run 0 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 65536\n",
      "    in super epoch 14\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_14/run_3/s-14_b-65536_r-3.tf\n",
      "    with chi2 3.5686 and loss 0.2115\n",
      "starting training with batch_size: 131072 and 5 epochs\n",
      "starting run 0 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 131072\n",
      "    in super epoch 14\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_14/run_2/s-14_b-131072_r-2.tf\n",
      "    with chi2 3.7980 and loss 0.2115\n",
      "starting training with batch_size: 262144 and 5 epochs\n",
      "starting run 0 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 0 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 1 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 2 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 3 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2115 of run 4 of super_epoch 14 with batch_size 262144\n",
      "\n",
      "calculating stats for 5 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 5 runs of batch_size 262144\n",
      "    in super epoch 14\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_14/run_4/s-14_b-262144_r-4.tf\n",
      "    with chi2 3.8243 and loss 0.2115\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 14 with 5 runs each with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n",
      "\n",
      "\n",
      "\n",
      " finished loop of 15 super_epochs\n",
      "    with batch_sizes:[32768, 65536, 131072, 262144]\n",
      "    best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\n",
      "    with chi2 2.9960 and loss 0.2116\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lowest_chi2_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mtrain_data, plt_data, weights=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mbatch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03minput_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mPhi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 268\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    265\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[1;32m    266\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow([model])\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_model_list, \u001b[43mlowest_chi2_list\u001b[49m, lowest_loss_list\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lowest_chi2_list' is not defined"
     ]
    }
   ],
   "source": [
    "# first session\n",
    "\n",
    "K.clear_session()\n",
    "'''\n",
    "train_data, plt_data, model=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\n",
    "batch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, epochs = 5, starting_super_epoch = 0, \n",
    "input_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \n",
    "Phi_acts=('linear', 'elu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\n",
    "'''\n",
    "# train_loop(train_data, plt_data, batch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=15, epochs=5)\n",
    "\n",
    "# chi2 is actually reduced chi2 -> 1 would be optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 16\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:29:25.603131: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 4096\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 14:36:24.528930: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-07 14:36:24.910526: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-07 14:36:31.871218: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n",
      "2024-03-07 14:36:32.237766: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1800000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 4096\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-4096_r-2.tf\n",
      "    with chi2 2.0312 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 8192\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 8192\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_3/s-16_b-8192_r-3.tf\n",
      "    with chi2 1.8968 and loss 0.2116\n",
      "starting training with batch_size: 16384 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 16384\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 16384\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_1/s-16_b-16384_r-1.tf\n",
      "    with chi2 1.6789 and loss 0.2116\n",
      "starting training with batch_size: 32768 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 32768\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 32768\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_3/s-16_b-32768_r-3.tf\n",
      "    with chi2 2.1437 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
      "    with chi2 1.5110 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 8 epochs\n",
      "starting run 0 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.6796160707599483e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.007769642455969e-05.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 131072\n",
      "    in super epoch 16\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_3/s-16_b-131072_r-3.tf\n",
      "    with chi2 1.9813 and loss 0.2116\n",
      "\n",
      "\n",
      " finished super_epoch 16 with 4 runs each with batch_sizes:[4096, 8192, 16384, 32768, 65536, 131072]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
      "    with chi2 1.5110 and loss 0.2116\n",
      "starting super_epoch 17\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 4096\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 4096\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_2/s-17_b-4096_r-2.tf\n",
      "    with chi2 1.9564 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 8192\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 8192\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_2/s-17_b-8192_r-2.tf\n",
      "    with chi2 1.9360 and loss 0.2116\n",
      "starting training with batch_size: 16384 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 16384\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 16384\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_0/s-17_b-16384_r-0.tf\n",
      "    with chi2 1.6155 and loss 0.2116\n",
      "starting training with batch_size: 32768 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 32768\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 32768\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_0/s-17_b-32768_r-0.tf\n",
      "    with chi2 2.0044 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_2/s-17_b-65536_r-2.tf\n",
      "    with chi2 1.9828 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 8 epochs\n",
      "starting run 0 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 131072\n",
      "    in super epoch 17\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_17/run_0/s-17_b-131072_r-0.tf\n",
      "    with chi2 2.1072 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 17 with 4 runs each with batch_sizes:[4096, 8192, 16384, 32768, 65536, 131072]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
      "    with chi2 1.5110 and loss 0.2116\n",
      "starting super_epoch 18\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 4096\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 4096\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_3/s-18_b-4096_r-3.tf\n",
      "    with chi2 1.9981 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 8192\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 8192\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_2/s-18_b-8192_r-2.tf\n",
      "    with chi2 2.0687 and loss 0.2116\n",
      "starting training with batch_size: 16384 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 16384\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 16384\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_1/s-18_b-16384_r-1.tf\n",
      "    with chi2 2.0305 and loss 0.2116\n",
      "starting training with batch_size: 32768 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 32768\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 32768\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_3/s-18_b-32768_r-3.tf\n",
      "    with chi2 1.8839 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_3/s-18_b-65536_r-3.tf\n",
      "    with chi2 2.0334 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 8 epochs\n",
      "starting run 0 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 18 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 131072\n",
      "    in super epoch 18\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_18/run_1/s-18_b-131072_r-1.tf\n",
      "    with chi2 2.0384 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 18 with 4 runs each with batch_sizes:[4096, 8192, 16384, 32768, 65536, 131072]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
      "    with chi2 1.5110 and loss 0.2116\n",
      "starting super_epoch 19\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 4096\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 4096\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_0/s-19_b-4096_r-0.tf\n",
      "    with chi2 2.1515 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 8192\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 8192\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_1/s-19_b-8192_r-1.tf\n",
      "    with chi2 1.7969 and loss 0.2116\n",
      "starting training with batch_size: 16384 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 16384\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 16384\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_3/s-19_b-16384_r-3.tf\n",
      "    with chi2 1.7122 and loss 0.2116\n",
      "starting training with batch_size: 32768 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 32768\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 32768\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_3/s-19_b-32768_r-3.tf\n",
      "    with chi2 2.0188 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_1/s-19_b-65536_r-1.tf\n",
      "    with chi2 2.0812 and loss 0.2116\n",
      "starting training with batch_size: 131072 and 8 epochs\n",
      "starting run 0 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 19 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 131072\n",
      "    in super epoch 19\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_19/run_0/s-19_b-131072_r-0.tf\n",
      "    with chi2 1.9494 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 19 with 4 runs each with batch_sizes:[4096, 8192, 16384, 32768, 65536, 131072]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
      "    with chi2 1.5110 and loss 0.2116\n",
      "starting super_epoch 20\n",
      "\n",
      "starting training with batch_size: 4096 and 8 epochs\n",
      "starting run 0 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "starting run 1 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "starting run 2 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "starting run 3 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 20 with batch_size 4096\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 4096\n",
      "    in super epoch 20\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_20/run_1/s-20_b-4096_r-1.tf\n",
      "    with chi2 2.0994 and loss 0.2116\n",
      "starting training with batch_size: 8192 and 8 epochs\n",
      "starting run 0 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "starting run 1 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "starting run 2 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "starting run 3 of super_epoch 20 with batch_size 8192\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 20 with batch_size 8192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 18:10:27.295449: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.68GiB (rounded to 1800000000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-03-07 18:10:27.299047: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2024-03-07 18:10:27.299066: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 103, Chunks in use: 103. 25.8KiB allocated for chunks. 25.8KiB in use in bin. 508B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299071: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 43, Chunks in use: 42. 22.2KiB allocated for chunks. 21.8KiB in use in bin. 17.9KiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299075: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 14, Chunks in use: 8. 16.5KiB allocated for chunks. 9.0KiB in use in bin. 6.5KiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299080: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 9, Chunks in use: 7. 21.5KiB allocated for chunks. 15.8KiB in use in bin. 13.7KiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299084: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299088: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299093: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299098: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 24, Chunks in use: 20. 1.11MiB allocated for chunks. 934.5KiB in use in bin. 857.8KiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299102: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 18, Chunks in use: 15. 1.29MiB allocated for chunks. 1.03MiB in use in bin. 837.1KiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299106: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 1, Chunks in use: 0. 138.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299110: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299114: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299118: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299122: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299125: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299129: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299133: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299136: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299140: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299145: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299149: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 4, Chunks in use: 3. 5.93GiB allocated for chunks. 5.03GiB in use in bin. 5.03GiB client-requested in use in bin.\n",
      "2024-03-07 18:10:27.299154: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 1.68GiB was 256.00MiB, Chunk State: \n",
      "2024-03-07 18:10:27.299161: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 921.10MiB | Requested Size: 256.0KiB | in_use: 0 | bin_num: 20, prev:   Size: 64.0KiB | Requested Size: 64.0KiB | in_use: 1 | bin_num: -1\n",
      "2024-03-07 18:10:27.299165: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 6368591872\n",
      "2024-03-07 18:10:27.299171: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000000 of size 1280 next 1\n",
      "2024-03-07 18:10:27.299174: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000500 of size 256 next 2\n",
      "2024-03-07 18:10:27.299177: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000600 of size 256 next 3\n",
      "2024-03-07 18:10:27.299181: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000700 of size 256 next 4\n",
      "2024-03-07 18:10:27.299184: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000800 of size 256 next 193\n",
      "2024-03-07 18:10:27.299187: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000900 of size 256 next 5\n",
      "2024-03-07 18:10:27.299190: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000a00 of size 256 next 8\n",
      "2024-03-07 18:10:27.299193: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000b00 of size 256 next 9\n",
      "2024-03-07 18:10:27.299197: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000c00 of size 256 next 36\n",
      "2024-03-07 18:10:27.299200: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000d00 of size 256 next 254\n",
      "2024-03-07 18:10:27.299203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000e00 of size 256 next 217\n",
      "2024-03-07 18:10:27.299206: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000f00 of size 256 next 15\n",
      "2024-03-07 18:10:27.299209: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42001000 of size 256 next 16\n",
      "2024-03-07 18:10:27.299212: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42001100 of size 256 next 17\n",
      "2024-03-07 18:10:27.299215: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42001200 of size 52736 next 975\n",
      "2024-03-07 18:10:27.299219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e000 of size 512 next 201\n",
      "2024-03-07 18:10:27.299222: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e200 of size 512 next 1053\n",
      "2024-03-07 18:10:27.299226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e400 of size 256 next 1002\n",
      "2024-03-07 18:10:27.299229: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e500 of size 512 next 1123\n",
      "2024-03-07 18:10:27.299232: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e700 of size 768 next 929\n",
      "2024-03-07 18:10:27.299235: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ea00 of size 512 next 918\n",
      "2024-03-07 18:10:27.299238: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ec00 of size 512 next 1049\n",
      "2024-03-07 18:10:27.299242: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ee00 of size 512 next 1112\n",
      "2024-03-07 18:10:27.299245: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f000 of size 256 next 250\n",
      "2024-03-07 18:10:27.299255: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f100 of size 1792 next 1059\n",
      "2024-03-07 18:10:27.299258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f800 of size 512 next 955\n",
      "2024-03-07 18:10:27.299261: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fa00 of size 256 next 998\n",
      "2024-03-07 18:10:27.299264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fb00 of size 256 next 169\n",
      "2024-03-07 18:10:27.299268: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fc00 of size 256 next 208\n",
      "2024-03-07 18:10:27.299271: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fd00 of size 256 next 93\n",
      "2024-03-07 18:10:27.299274: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fe00 of size 768 next 860\n",
      "2024-03-07 18:10:27.299277: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010100 of size 256 next 903\n",
      "2024-03-07 18:10:27.299280: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010200 of size 256 next 263\n",
      "2024-03-07 18:10:27.299284: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010300 of size 62464 next 11\n",
      "2024-03-07 18:10:27.299288: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f700 of size 256 next 23\n",
      "2024-03-07 18:10:27.299291: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f800 of size 256 next 24\n",
      "2024-03-07 18:10:27.299294: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f900 of size 256 next 25\n",
      "2024-03-07 18:10:27.299297: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fa00 of size 256 next 26\n",
      "2024-03-07 18:10:27.299300: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fb00 of size 256 next 210\n",
      "2024-03-07 18:10:27.299303: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fc00 of size 256 next 30\n",
      "2024-03-07 18:10:27.299307: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fd00 of size 256 next 31\n",
      "2024-03-07 18:10:27.299310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fe00 of size 256 next 257\n",
      "2024-03-07 18:10:27.299313: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201ff00 of size 256 next 82\n",
      "2024-03-07 18:10:27.299316: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020000 of size 256 next 69\n",
      "2024-03-07 18:10:27.299319: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020100 of size 256 next 27\n",
      "2024-03-07 18:10:27.299322: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020200 of size 3072 next 1090\n",
      "2024-03-07 18:10:27.299326: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020e00 of size 256 next 1067\n",
      "2024-03-07 18:10:27.299329: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020f00 of size 256 next 1031\n",
      "2024-03-07 18:10:27.299332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021000 of size 256 next 89\n",
      "2024-03-07 18:10:27.299335: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021100 of size 256 next 1021\n",
      "2024-03-07 18:10:27.299338: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021200 of size 256 next 990\n",
      "2024-03-07 18:10:27.299341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021300 of size 256 next 889\n",
      "2024-03-07 18:10:27.299345: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021400 of size 2816 next 48\n",
      "2024-03-07 18:10:27.299348: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021f00 of size 256 next 49\n",
      "2024-03-07 18:10:27.299351: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022000 of size 256 next 976\n",
      "2024-03-07 18:10:27.299354: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022100 of size 256 next 840\n",
      "2024-03-07 18:10:27.299358: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022200 of size 1024 next 931\n",
      "2024-03-07 18:10:27.299361: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022600 of size 256 next 812\n",
      "2024-03-07 18:10:27.299364: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022700 of size 512 next 858\n",
      "2024-03-07 18:10:27.299367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022900 of size 512 next 102\n",
      "2024-03-07 18:10:27.299370: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022b00 of size 512 next 1107\n",
      "2024-03-07 18:10:27.299374: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022d00 of size 512 next 172\n",
      "2024-03-07 18:10:27.299377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022f00 of size 512 next 1130\n",
      "2024-03-07 18:10:27.299380: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023100 of size 512 next 1028\n",
      "2024-03-07 18:10:27.299383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023300 of size 1024 next 109\n",
      "2024-03-07 18:10:27.299386: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023700 of size 256 next 1124\n",
      "2024-03-07 18:10:27.299389: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023800 of size 256 next 944\n",
      "2024-03-07 18:10:27.299392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023900 of size 256 next 1008\n",
      "2024-03-07 18:10:27.299396: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023a00 of size 256 next 1137\n",
      "2024-03-07 18:10:27.299399: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023b00 of size 256 next 68\n",
      "2024-03-07 18:10:27.299402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023c00 of size 256 next 959\n",
      "2024-03-07 18:10:27.299405: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023d00 of size 256 next 40\n",
      "2024-03-07 18:10:27.299408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023e00 of size 256 next 85\n",
      "2024-03-07 18:10:27.299411: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023f00 of size 256 next 39\n",
      "2024-03-07 18:10:27.299414: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024000 of size 256 next 38\n",
      "2024-03-07 18:10:27.299418: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024100 of size 256 next 83\n",
      "2024-03-07 18:10:27.299421: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024200 of size 40192 next 240\n",
      "2024-03-07 18:10:27.299424: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4202df00 of size 51200 next 919\n",
      "2024-03-07 18:10:27.299427: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4203a700 of size 65536 next 1134\n",
      "2024-03-07 18:10:27.299430: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4204a700 of size 51200 next 925\n",
      "2024-03-07 18:10:27.299434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42056f00 of size 40192 next 939\n",
      "2024-03-07 18:10:27.299437: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060c00 of size 256 next 1055\n",
      "2024-03-07 18:10:27.299440: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060d00 of size 256 next 205\n",
      "2024-03-07 18:10:27.299443: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060e00 of size 256 next 1011\n",
      "2024-03-07 18:10:27.299446: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060f00 of size 256 next 157\n",
      "2024-03-07 18:10:27.299449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061000 of size 256 next 80\n",
      "2024-03-07 18:10:27.299452: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061100 of size 256 next 203\n",
      "2024-03-07 18:10:27.299456: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061200 of size 256 next 985\n",
      "2024-03-07 18:10:27.299459: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061300 of size 256 next 897\n",
      "2024-03-07 18:10:27.299462: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061400 of size 256 next 783\n",
      "2024-03-07 18:10:27.299465: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061500 of size 256 next 921\n",
      "2024-03-07 18:10:27.299469: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061600 of size 256 next 1126\n",
      "2024-03-07 18:10:27.299472: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061700 of size 256 next 90\n",
      "2024-03-07 18:10:27.299475: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061800 of size 256 next 249\n",
      "2024-03-07 18:10:27.299478: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061900 of size 256 next 952\n",
      "2024-03-07 18:10:27.299481: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061a00 of size 256 next 1136\n",
      "2024-03-07 18:10:27.299484: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061b00 of size 512 next 965\n",
      "2024-03-07 18:10:27.299487: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061d00 of size 1024 next 1121\n",
      "2024-03-07 18:10:27.299491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062100 of size 2048 next 128\n",
      "2024-03-07 18:10:27.299494: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062900 of size 256 next 994\n",
      "2024-03-07 18:10:27.299497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062a00 of size 256 next 1000\n",
      "2024-03-07 18:10:27.299500: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062b00 of size 256 next 943\n",
      "2024-03-07 18:10:27.299503: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062c00 of size 256 next 1103\n",
      "2024-03-07 18:10:27.299506: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062d00 of size 256 next 14\n",
      "2024-03-07 18:10:27.299509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062e00 of size 256 next 206\n",
      "2024-03-07 18:10:27.299513: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062f00 of size 512 next 171\n",
      "2024-03-07 18:10:27.299516: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063100 of size 512 next 873\n",
      "2024-03-07 18:10:27.299519: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063300 of size 256 next 227\n",
      "2024-03-07 18:10:27.299522: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063400 of size 256 next 236\n",
      "2024-03-07 18:10:27.299525: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063500 of size 256 next 885\n",
      "2024-03-07 18:10:27.299528: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42063600 of size 512 next 856\n",
      "2024-03-07 18:10:27.299531: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063800 of size 512 next 1138\n",
      "2024-03-07 18:10:27.299535: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063a00 of size 512 next 10\n",
      "2024-03-07 18:10:27.299538: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063c00 of size 512 next 1054\n",
      "2024-03-07 18:10:27.299541: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063e00 of size 256 next 1076\n",
      "2024-03-07 18:10:27.299544: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063f00 of size 256 next 245\n",
      "2024-03-07 18:10:27.299547: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42064000 of size 1280 next 880\n",
      "2024-03-07 18:10:27.299550: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064500 of size 256 next 996\n",
      "2024-03-07 18:10:27.299553: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064600 of size 256 next 13\n",
      "2024-03-07 18:10:27.299557: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064700 of size 256 next 951\n",
      "2024-03-07 18:10:27.299560: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064800 of size 256 next 145\n",
      "2024-03-07 18:10:27.299563: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064900 of size 512 next 44\n",
      "2024-03-07 18:10:27.299566: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064b00 of size 1024 next 1106\n",
      "2024-03-07 18:10:27.299570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064f00 of size 2048 next 37\n",
      "2024-03-07 18:10:27.299573: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42065700 of size 3072 next 999\n",
      "2024-03-07 18:10:27.299576: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066300 of size 256 next 158\n",
      "2024-03-07 18:10:27.299579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066400 of size 256 next 936\n",
      "2024-03-07 18:10:27.299582: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066500 of size 256 next 139\n",
      "2024-03-07 18:10:27.299585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066600 of size 256 next 113\n",
      "2024-03-07 18:10:27.299588: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066700 of size 256 next 140\n",
      "2024-03-07 18:10:27.299592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066800 of size 256 next 1009\n",
      "2024-03-07 18:10:27.299595: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066900 of size 256 next 1133\n",
      "2024-03-07 18:10:27.299598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066a00 of size 256 next 58\n",
      "2024-03-07 18:10:27.299601: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066b00 of size 256 next 142\n",
      "2024-03-07 18:10:27.299604: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42066c00 of size 1536 next 937\n",
      "2024-03-07 18:10:27.299607: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067200 of size 512 next 251\n",
      "2024-03-07 18:10:27.299610: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067400 of size 512 next 229\n",
      "2024-03-07 18:10:27.299614: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42067600 of size 1280 next 165\n",
      "2024-03-07 18:10:27.299617: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067b00 of size 512 next 1069\n",
      "2024-03-07 18:10:27.299620: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067d00 of size 512 next 243\n",
      "2024-03-07 18:10:27.299623: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067f00 of size 512 next 1125\n",
      "2024-03-07 18:10:27.299626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068100 of size 256 next 847\n",
      "2024-03-07 18:10:27.299629: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068200 of size 256 next 233\n",
      "2024-03-07 18:10:27.299632: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42068300 of size 1280 next 20\n",
      "2024-03-07 18:10:27.299636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068800 of size 512 next 155\n",
      "2024-03-07 18:10:27.299639: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068a00 of size 512 next 830\n",
      "2024-03-07 18:10:27.299642: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068c00 of size 256 next 1034\n",
      "2024-03-07 18:10:27.299645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42068d00 of size 1024 next 953\n",
      "2024-03-07 18:10:27.299648: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069100 of size 512 next 1066\n",
      "2024-03-07 18:10:27.299651: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069300 of size 512 next 177\n",
      "2024-03-07 18:10:27.299655: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069500 of size 768 next 1052\n",
      "2024-03-07 18:10:27.299658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069800 of size 256 next 1003\n",
      "2024-03-07 18:10:27.299661: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42069900 of size 52224 next 283\n",
      "2024-03-07 18:10:27.299664: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42076500 of size 51200 next 1070\n",
      "2024-03-07 18:10:27.299668: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42082d00 of size 78848 next 163\n",
      "2024-03-07 18:10:27.299672: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42096100 of size 67584 next 1108\n",
      "2024-03-07 18:10:27.299675: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420a6900 of size 2048 next 190\n",
      "2024-03-07 18:10:27.299678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420a7100 of size 78336 next 176\n",
      "2024-03-07 18:10:27.299681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420ba300 of size 51200 next 883\n",
      "2024-03-07 18:10:27.299685: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420c6b00 of size 65536 next 1043\n",
      "2024-03-07 18:10:27.299688: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420d6b00 of size 105728 next 1064\n",
      "2024-03-07 18:10:27.299691: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420f0800 of size 40192 next 909\n",
      "2024-03-07 18:10:27.299694: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420fa500 of size 49920 next 96\n",
      "2024-03-07 18:10:27.299697: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106800 of size 256 next 1085\n",
      "2024-03-07 18:10:27.299700: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106900 of size 256 next 1029\n",
      "2024-03-07 18:10:27.299704: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106a00 of size 512 next 144\n",
      "2024-03-07 18:10:27.299707: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106c00 of size 512 next 119\n",
      "2024-03-07 18:10:27.299710: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106e00 of size 512 next 218\n",
      "2024-03-07 18:10:27.299713: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42107000 of size 2816 next 221\n",
      "2024-03-07 18:10:27.299716: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107b00 of size 65536 next 269\n",
      "2024-03-07 18:10:27.299719: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42117b00 of size 40192 next 1032\n",
      "2024-03-07 18:10:27.299723: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42121800 of size 51200 next 120\n",
      "2024-03-07 18:10:27.299726: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c4212e000 of size 142080 next 252\n",
      "2024-03-07 18:10:27.299729: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42150b00 of size 256 next 1071\n",
      "2024-03-07 18:10:27.299732: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c42150c00 of size 91392 next 164\n",
      "2024-03-07 18:10:27.299735: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42167100 of size 65536 next 33\n",
      "2024-03-07 18:10:27.299738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42177100 of size 51200 next 179\n",
      "2024-03-07 18:10:27.299742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42183900 of size 40192 next 184\n",
      "2024-03-07 18:10:27.299745: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0c4218d600 of size 91392 next 1104\n",
      "2024-03-07 18:10:27.299748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c421a3b00 of size 65536 next 1063\n",
      "2024-03-07 18:10:27.299751: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c421b3b00 of size 51200 next 246\n",
      "2024-03-07 18:10:27.299755: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c421c0300 of size 1800000000 next 920\n",
      "2024-03-07 18:10:27.299758: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad65d500 of size 51200 next 1056\n",
      "2024-03-07 18:10:27.299762: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad669d00 of size 40192 next 175\n",
      "2024-03-07 18:10:27.299765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0cad673a00 of size 91392 next 181\n",
      "2024-03-07 18:10:27.299768: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad689f00 of size 65536 next 926\n",
      "2024-03-07 18:10:27.299771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad699f00 of size 51200 next 899\n",
      "2024-03-07 18:10:27.299774: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad6a6700 of size 1800000000 next 234\n",
      "2024-03-07 18:10:27.299778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b43900 of size 65536 next 150\n",
      "2024-03-07 18:10:27.299781: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b53900 of size 51200 next 893\n",
      "2024-03-07 18:10:27.299784: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b60100 of size 1024 next 84\n",
      "2024-03-07 18:10:27.299787: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b60500 of size 2048 next 949\n",
      "2024-03-07 18:10:27.299790: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b60d00 of size 78336 next 141\n",
      "2024-03-07 18:10:27.299793: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d18b73f00 of size 51712 next 971\n",
      "2024-03-07 18:10:27.299797: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b80900 of size 256 next 972\n",
      "2024-03-07 18:10:27.299800: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b80a00 of size 512 next 989\n",
      "2024-03-07 18:10:27.299803: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b80c00 of size 65536 next 92\n",
      "2024-03-07 18:10:27.299806: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b90c00 of size 40192 next 1051\n",
      "2024-03-07 18:10:27.299809: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18b9a900 of size 1800000000 next 114\n",
      "2024-03-07 18:10:27.299812: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037b00 of size 512 next 124\n",
      "2024-03-07 18:10:27.299816: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037d00 of size 512 next 978\n",
      "2024-03-07 18:10:27.299819: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037f00 of size 51200 next 187\n",
      "2024-03-07 18:10:27.299822: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044700 of size 1024 next 262\n",
      "2024-03-07 18:10:27.299825: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d84044b00 of size 1280 next 957\n",
      "2024-03-07 18:10:27.299828: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045000 of size 2048 next 277\n",
      "2024-03-07 18:10:27.299831: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045800 of size 512 next 267\n",
      "2024-03-07 18:10:27.299834: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045a00 of size 512 next 110\n",
      "2024-03-07 18:10:27.299838: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045c00 of size 79872 next 911\n",
      "2024-03-07 18:10:27.299841: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059400 of size 512 next 59\n",
      "2024-03-07 18:10:27.299844: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059600 of size 256 next 45\n",
      "2024-03-07 18:10:27.299847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059700 of size 256 next 122\n",
      "2024-03-07 18:10:27.299850: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d84059800 of size 50688 next 1078\n",
      "2024-03-07 18:10:27.299853: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84065e00 of size 256 next 846\n",
      "2024-03-07 18:10:27.299857: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84065f00 of size 512 next 46\n",
      "2024-03-07 18:10:27.299860: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84066100 of size 65536 next 1102\n",
      "2024-03-07 18:10:27.299864: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d84076100 of size 965844736 next 18446744073709551615\n",
      "2024-03-07 18:10:27.299867: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2024-03-07 18:10:27.299871: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 103 Chunks of size 256 totalling 25.8KiB\n",
      "2024-03-07 18:10:27.299875: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 39 Chunks of size 512 totalling 19.5KiB\n",
      "2024-03-07 18:10:27.299878: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 768 totalling 2.2KiB\n",
      "2024-03-07 18:10:27.299882: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 6 Chunks of size 1024 totalling 6.0KiB\n",
      "2024-03-07 18:10:27.299885: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-03-07 18:10:27.299889: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB\n",
      "2024-03-07 18:10:27.299892: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 2048 totalling 10.0KiB\n",
      "2024-03-07 18:10:27.299896: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2816 totalling 2.8KiB\n",
      "2024-03-07 18:10:27.299899: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\n",
      "2024-03-07 18:10:27.299903: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 7 Chunks of size 40192 totalling 274.8KiB\n",
      "2024-03-07 18:10:27.299907: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 49920 totalling 48.8KiB\n",
      "2024-03-07 18:10:27.299910: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 11 Chunks of size 51200 totalling 550.0KiB\n",
      "2024-03-07 18:10:27.299914: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 62464 totalling 61.0KiB\n",
      "2024-03-07 18:10:27.299917: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 65536 totalling 576.0KiB\n",
      "2024-03-07 18:10:27.299921: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 67584 totalling 66.0KiB\n",
      "2024-03-07 18:10:27.299924: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 78336 totalling 153.0KiB\n",
      "2024-03-07 18:10:27.299928: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 78848 totalling 77.0KiB\n",
      "2024-03-07 18:10:27.299931: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 79872 totalling 78.0KiB\n",
      "2024-03-07 18:10:27.299935: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 105728 totalling 103.2KiB\n",
      "2024-03-07 18:10:27.299939: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 1800000000 totalling 5.03GiB\n",
      "2024-03-07 18:10:27.299942: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 5.03GiB\n",
      "2024-03-07 18:10:27.299945: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 6368591872 memory_limit_: 6368591872 available bytes: 0 curr_region_allocation_bytes_: 12737183744\n",
      "2024-03-07 18:10:27.299951: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                      6368591872\n",
      "InUse:                      5402109440\n",
      "MaxInUse:                   6038536704\n",
      "NumAllocs:                   102668390\n",
      "MaxAllocSize:               1800646400\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-03-07 18:10:27.299958: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *************************************************************************************_______________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# continue training with best weughts from first session as baseline\u001b[39;00m\n\u001b[1;32m      3\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_patience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 211\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 166\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_super_epoch_choose_best\u001b[39m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m                                   input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, Phi_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m128\u001b[39m), F_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m), loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    158\u001b[0m                                   Phi_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m), F_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m), output_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     dctr, model_list, loss_list \u001b[38;5;241m=\u001b[39m train_super_epoch(model, train_data, batch_size, repeat, train_dir \u001b[38;5;241m=\u001b[39m train_dir, input_dim\u001b[38;5;241m=\u001b[39minput_dim, \n\u001b[1;32m    162\u001b[0m                                                               Phi_sizes \u001b[38;5;241m=\u001b[39m Phi_sizes, F_sizes \u001b[38;5;241m=\u001b[39m F_sizes, loss \u001b[38;5;241m=\u001b[39m loss, dropout\u001b[38;5;241m=\u001b[39mdropout, l2_reg\u001b[38;5;241m=\u001b[39ml2_reg,\n\u001b[1;32m    163\u001b[0m                                                               Phi_acts\u001b[38;5;241m=\u001b[39mPhi_acts, F_acts\u001b[38;5;241m=\u001b[39mF_acts, output_act\u001b[38;5;241m=\u001b[39moutput_act, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[1;32m    164\u001b[0m                                                               epochs \u001b[38;5;241m=\u001b[39m epochs, super_epoch \u001b[38;5;241m=\u001b[39m super_epoch)\n\u001b[0;32m--> 166\u001b[0m     rwgt_list\u001b[38;5;241m=\u001b[39m \u001b[43mget_rwgt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# stats\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     mae_mean_list, chi2_mean_list, p_mean_list \u001b[38;5;241m=\u001b[39m calc_stats(rwgt_list, plt_data)\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mget_rwgt\u001b[0;34m(model_list, x0_plt_nrm)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[1;32m     32\u001b[0m     dctr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[0;32m---> 33\u001b[0m     rwgt \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     rwgt_list\u001b[38;5;241m.\u001b[39mappend(rwgt)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rwgt_list\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:675\u001b[0m, in \u001b[0;36mpredict_weights\u001b[0;34m(dctr, X, batch_size, clip, verbose)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_weights\u001b[39m(dctr, X, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    669\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    generates weights for reweighing X0 to X1: weights_0\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03m                  and for reweighing X1 to X0: weights_1\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;124;03m    from the predictions made by DCTR\u001b[39;00m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;124;03m    and returns the reweighing arrays\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     predics \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m     weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdivide(predics[:,\u001b[38;5;241m1\u001b[39m], (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mpredics[:,\u001b[38;5;241m1\u001b[39m]), out\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros_like(predics[:,\u001b[38;5;241m1\u001b[39m]), where\u001b[38;5;241m=\u001b[39m(predics[:,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m1.0\u001b[39m )\n\u001b[1;32m    679\u001b[0m     weights \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(weights) \u001b[38;5;66;03m# adjust weights so that mean is 1\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# continue training with best weughts from first session as baseline\n",
    "\n",
    "K.clear_session()\n",
    "# train_loop(train_data, plt_data, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10',model='/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train/super_epoch_4/run_0/s-4_b-65536_r-0.tf', \n",
    "           lowest_chi2 = 5, repeat=4, super_epochs=35, starting_super_epoch=16, super_patience = 3, learning_rate=0.0005, dropout = 0.1, \n",
    "           batch_sizes=[4096, 8192, 2*8192, 4*8192, 8*8192, 16*8192])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 20\n",
      "\n",
      "starting training with batch_size: 49152 and 8 epochs\n",
      "starting run 0 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "starting run 1 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "starting run 2 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "starting run 3 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 20 with batch_size 49152\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 49152\n",
      "    in super epoch 20\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_20/run_1/s-20_b-49152_r-1.tf\n",
      "    with chi2 2.0278 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 20 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 20\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_20/run_0/s-20_b-65536_r-0.tf\n",
      "    with chi2 1.8601 and loss 0.2116\n",
      "starting training with batch_size: 98304 and 8 epochs\n",
      "starting run 0 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "starting run 1 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "starting run 2 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "starting run 3 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 6.046617636457085e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 3.6279706364439334e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 20 with batch_size 98304\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 98304\n",
      "    in super epoch 20\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_20/run_1/s-20_b-98304_r-1.tf\n",
      "    with chi2 1.9110 and loss 0.2116\n",
      "\n",
      "\n",
      " finished super_epoch 20 with 4 runs each with batch_sizes:[49152, 65536, 98304]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_20/run_0/s-20_b-65536_r-0.tf\n",
      "    with chi2 1.8601 and loss 0.2116\n",
      "starting super_epoch 21\n",
      "\n",
      "starting training with batch_size: 49152 and 8 epochs\n",
      "starting run 0 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "starting run 1 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "starting run 2 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "starting run 3 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 21 with batch_size 49152\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 49152\n",
      "    in super epoch 21\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_21/run_0/s-21_b-49152_r-0.tf\n",
      "    with chi2 2.0190 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 21 with batch_size 65536\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 65536\n",
      "    in super epoch 21\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_21/run_2/s-21_b-65536_r-2.tf\n",
      "    with chi2 2.0842 and loss 0.2116\n",
      "starting training with batch_size: 98304 and 8 epochs\n",
      "starting run 0 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "starting run 1 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "starting run 2 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "starting run 3 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 21 with batch_size 98304\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 98304\n",
      "    in super epoch 21\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_21/run_3/s-21_b-98304_r-3.tf\n",
      "    with chi2 2.0604 and loss 0.2116\n",
      "no improvement, lowering learnng_rate to {learning_rate}\n",
      "\n",
      "\n",
      " finished super_epoch 21 with 4 runs each with batch_sizes:[49152, 65536, 98304]\n",
      "   best model/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_20/run_0/s-20_b-65536_r-0.tf\n",
      "    with chi2 1.8601 and loss 0.2116\n",
      "starting super_epoch 22\n",
      "\n",
      "starting training with batch_size: 49152 and 8 epochs\n",
      "starting run 0 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 0 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "starting run 1 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 1 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "starting run 2 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 2 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "starting run 3 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 2.1767824364360423e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.3060694072919432e-06.\n",
      "\n",
      " best loss 0.2116 of run 3 of super_epoch 22 with batch_size 49152\n",
      "\n",
      "calculating stats for 4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:829: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chi2 = np.nansum(np.power(n_list[0] - n, 2)/(np.power(uncert, 2) + np.power(uncert_list[0], 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finished 4 runs of batch_size 49152\n",
      "    in super epoch 22\n",
      "   with best model /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20/super_epoch_22/run_2/s-22_b-49152_r-2.tf\n",
      "    with chi2 1.9880 and loss 0.2116\n",
      "starting training with batch_size: 65536 and 8 epochs\n",
      "starting run 0 of super_epoch 22 with batch_size 65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 20:07:42.341349: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.00MiB (rounded to 26214400)requested by op gradient_tape/model_8/output/MatMul/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2024-03-07 20:07:42.341415: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2024-03-07 20:07:42.341430: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 181, Chunks in use: 181. 45.2KiB allocated for chunks. 45.2KiB in use in bin. 912B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341440: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 91, Chunks in use: 90. 47.8KiB allocated for chunks. 47.2KiB in use in bin. 38.4KiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341450: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 16, Chunks in use: 16. 17.0KiB allocated for chunks. 17.0KiB in use in bin. 12.7KiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341459: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 15, Chunks in use: 15. 32.0KiB allocated for chunks. 32.0KiB in use in bin. 29.3KiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341468: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341477: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341486: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 1, Chunks in use: 0. 20.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341495: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 41, Chunks in use: 41. 1.92MiB allocated for chunks. 1.92MiB in use in bin. 1.73MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341504: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 35, Chunks in use: 34. 2.41MiB allocated for chunks. 2.35MiB in use in bin. 1.81MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341512: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341520: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341528: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 3, Chunks in use: 2. 2.00MiB allocated for chunks. 1.25MiB in use in bin. 1.25MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341536: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341545: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.75MiB allocated for chunks. 3.75MiB in use in bin. 3.75MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341554: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 0. 7.75MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341562: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341572: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 6, Chunks in use: 5. 152.67MiB allocated for chunks. 129.74MiB in use in bin. 125.00MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341582: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 5, Chunks in use: 5. 160.00MiB allocated for chunks. 160.00MiB in use in bin. 160.00MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341592: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 5, Chunks in use: 5. 438.00MiB allocated for chunks. 438.00MiB in use in bin. 438.00MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341602: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 154.00MiB allocated for chunks. 154.00MiB in use in bin. 111.89MiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341611: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 3. 5.03GiB allocated for chunks. 5.03GiB in use in bin. 5.03GiB client-requested in use in bin.\n",
      "2024-03-07 20:07:42.341620: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 25.00MiB was 16.00MiB, Chunk State: \n",
      "2024-03-07 20:07:42.341637: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 22.93MiB | Requested Size: 256.0KiB | in_use: 0 | bin_num: 16, prev:   Size: 3.75MiB | Requested Size: 3.75MiB | in_use: 1 | bin_num: -1, next:   Size: 84.8KiB | Requested Size: 64.0KiB | in_use: 1 | bin_num: -1\n",
      "2024-03-07 20:07:42.341644: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 6368591872\n",
      "2024-03-07 20:07:42.341653: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000000 of size 1280 next 1\n",
      "2024-03-07 20:07:42.341660: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000500 of size 256 next 2\n",
      "2024-03-07 20:07:42.341667: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000600 of size 256 next 3\n",
      "2024-03-07 20:07:42.341674: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000700 of size 256 next 4\n",
      "2024-03-07 20:07:42.341681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000800 of size 256 next 193\n",
      "2024-03-07 20:07:42.341688: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000900 of size 256 next 5\n",
      "2024-03-07 20:07:42.341694: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000a00 of size 256 next 8\n",
      "2024-03-07 20:07:42.341701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000b00 of size 256 next 9\n",
      "2024-03-07 20:07:42.341708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000c00 of size 256 next 973\n",
      "2024-03-07 20:07:42.341715: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000d00 of size 256 next 1118\n",
      "2024-03-07 20:07:42.341721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000e00 of size 256 next 924\n",
      "2024-03-07 20:07:42.341728: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42000f00 of size 256 next 15\n",
      "2024-03-07 20:07:42.341735: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42001000 of size 256 next 16\n",
      "2024-03-07 20:07:42.341742: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42001100 of size 256 next 17\n",
      "2024-03-07 20:07:42.341748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42001200 of size 52736 next 975\n",
      "2024-03-07 20:07:42.341756: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e000 of size 512 next 201\n",
      "2024-03-07 20:07:42.341762: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e200 of size 512 next 1053\n",
      "2024-03-07 20:07:42.341769: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e400 of size 256 next 1002\n",
      "2024-03-07 20:07:42.341776: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e500 of size 512 next 1123\n",
      "2024-03-07 20:07:42.341783: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200e700 of size 768 next 929\n",
      "2024-03-07 20:07:42.341790: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ea00 of size 512 next 918\n",
      "2024-03-07 20:07:42.341797: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ec00 of size 512 next 1049\n",
      "2024-03-07 20:07:42.341804: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200ee00 of size 512 next 1112\n",
      "2024-03-07 20:07:42.341811: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f000 of size 256 next 250\n",
      "2024-03-07 20:07:42.341818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f100 of size 1792 next 1059\n",
      "2024-03-07 20:07:42.341825: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200f800 of size 512 next 955\n",
      "2024-03-07 20:07:42.341831: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fa00 of size 256 next 998\n",
      "2024-03-07 20:07:42.341838: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fb00 of size 256 next 169\n",
      "2024-03-07 20:07:42.341845: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fc00 of size 256 next 208\n",
      "2024-03-07 20:07:42.341852: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fd00 of size 256 next 93\n",
      "2024-03-07 20:07:42.341859: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4200fe00 of size 768 next 860\n",
      "2024-03-07 20:07:42.341865: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010100 of size 256 next 1085\n",
      "2024-03-07 20:07:42.341872: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010200 of size 256 next 263\n",
      "2024-03-07 20:07:42.341879: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42010300 of size 62464 next 11\n",
      "2024-03-07 20:07:42.341886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f700 of size 256 next 23\n",
      "2024-03-07 20:07:42.341893: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f800 of size 256 next 24\n",
      "2024-03-07 20:07:42.341899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201f900 of size 256 next 25\n",
      "2024-03-07 20:07:42.341906: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fa00 of size 256 next 26\n",
      "2024-03-07 20:07:42.341913: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fb00 of size 256 next 210\n",
      "2024-03-07 20:07:42.341920: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fc00 of size 256 next 30\n",
      "2024-03-07 20:07:42.341927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fd00 of size 256 next 31\n",
      "2024-03-07 20:07:42.341936: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201fe00 of size 256 next 257\n",
      "2024-03-07 20:07:42.341945: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4201ff00 of size 256 next 82\n",
      "2024-03-07 20:07:42.341956: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020000 of size 256 next 69\n",
      "2024-03-07 20:07:42.341965: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020100 of size 256 next 27\n",
      "2024-03-07 20:07:42.341976: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020200 of size 3072 next 1090\n",
      "2024-03-07 20:07:42.341986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42020e00 of size 512 next 899\n",
      "2024-03-07 20:07:42.341994: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021000 of size 768 next 990\n",
      "2024-03-07 20:07:42.342004: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021300 of size 256 next 889\n",
      "2024-03-07 20:07:42.342014: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021400 of size 2816 next 48\n",
      "2024-03-07 20:07:42.342023: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42021f00 of size 256 next 49\n",
      "2024-03-07 20:07:42.342032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022000 of size 256 next 1066\n",
      "2024-03-07 20:07:42.342041: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022100 of size 256 next 840\n",
      "2024-03-07 20:07:42.342051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022200 of size 1024 next 931\n",
      "2024-03-07 20:07:42.342060: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022600 of size 256 next 812\n",
      "2024-03-07 20:07:42.342069: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022700 of size 512 next 858\n",
      "2024-03-07 20:07:42.342079: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022900 of size 512 next 102\n",
      "2024-03-07 20:07:42.342086: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022b00 of size 512 next 1107\n",
      "2024-03-07 20:07:42.342096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022d00 of size 512 next 172\n",
      "2024-03-07 20:07:42.342106: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42022f00 of size 512 next 1130\n",
      "2024-03-07 20:07:42.342115: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023100 of size 512 next 1028\n",
      "2024-03-07 20:07:42.342125: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023300 of size 1024 next 109\n",
      "2024-03-07 20:07:42.342134: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023700 of size 256 next 1124\n",
      "2024-03-07 20:07:42.342142: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023800 of size 256 next 944\n",
      "2024-03-07 20:07:42.342153: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023900 of size 256 next 1008\n",
      "2024-03-07 20:07:42.342160: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023a00 of size 256 next 1137\n",
      "2024-03-07 20:07:42.342169: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023b00 of size 256 next 68\n",
      "2024-03-07 20:07:42.342178: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023c00 of size 256 next 959\n",
      "2024-03-07 20:07:42.342186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023d00 of size 256 next 40\n",
      "2024-03-07 20:07:42.342195: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023e00 of size 256 next 85\n",
      "2024-03-07 20:07:42.342203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42023f00 of size 256 next 39\n",
      "2024-03-07 20:07:42.342213: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024000 of size 256 next 38\n",
      "2024-03-07 20:07:42.342224: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024100 of size 256 next 83\n",
      "2024-03-07 20:07:42.342234: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42024200 of size 40192 next 240\n",
      "2024-03-07 20:07:42.342243: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4202df00 of size 51200 next 919\n",
      "2024-03-07 20:07:42.342250: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4203a700 of size 65536 next 1134\n",
      "2024-03-07 20:07:42.342257: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4204a700 of size 51200 next 925\n",
      "2024-03-07 20:07:42.342264: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42056f00 of size 40192 next 939\n",
      "2024-03-07 20:07:42.342271: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060c00 of size 256 next 1055\n",
      "2024-03-07 20:07:42.342278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060d00 of size 256 next 205\n",
      "2024-03-07 20:07:42.342285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060e00 of size 256 next 1011\n",
      "2024-03-07 20:07:42.342293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42060f00 of size 256 next 157\n",
      "2024-03-07 20:07:42.342303: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061000 of size 256 next 1017\n",
      "2024-03-07 20:07:42.342318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061100 of size 256 next 1146\n",
      "2024-03-07 20:07:42.342328: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061200 of size 256 next 908\n",
      "2024-03-07 20:07:42.342335: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061300 of size 256 next 1187\n",
      "2024-03-07 20:07:42.342343: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061400 of size 256 next 206\n",
      "2024-03-07 20:07:42.342353: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061500 of size 256 next 216\n",
      "2024-03-07 20:07:42.342362: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061600 of size 256 next 1093\n",
      "2024-03-07 20:07:42.342372: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061700 of size 256 next 60\n",
      "2024-03-07 20:07:42.342382: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061800 of size 1024 next 1106\n",
      "2024-03-07 20:07:42.342392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061c00 of size 256 next 947\n",
      "2024-03-07 20:07:42.342402: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061d00 of size 256 next 203\n",
      "2024-03-07 20:07:42.342410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061e00 of size 256 next 141\n",
      "2024-03-07 20:07:42.342420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42061f00 of size 256 next 1164\n",
      "2024-03-07 20:07:42.342430: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062000 of size 256 next 228\n",
      "2024-03-07 20:07:42.342439: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062100 of size 256 next 1166\n",
      "2024-03-07 20:07:42.342447: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062200 of size 256 next 1042\n",
      "2024-03-07 20:07:42.342456: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062300 of size 256 next 248\n",
      "2024-03-07 20:07:42.342463: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062400 of size 256 next 177\n",
      "2024-03-07 20:07:42.342470: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062500 of size 256 next 156\n",
      "2024-03-07 20:07:42.342477: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062600 of size 256 next 272\n",
      "2024-03-07 20:07:42.342484: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062700 of size 256 next 1143\n",
      "2024-03-07 20:07:42.342491: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062800 of size 256 next 1100\n",
      "2024-03-07 20:07:42.342498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062900 of size 256 next 171\n",
      "2024-03-07 20:07:42.342506: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062a00 of size 256 next 964\n",
      "2024-03-07 20:07:42.342516: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062b00 of size 1024 next 1178\n",
      "2024-03-07 20:07:42.342525: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42062f00 of size 256 next 303\n",
      "2024-03-07 20:07:42.342535: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063000 of size 256 next 1226\n",
      "2024-03-07 20:07:42.342545: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063100 of size 256 next 847\n",
      "2024-03-07 20:07:42.342555: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063200 of size 256 next 930\n",
      "2024-03-07 20:07:42.342566: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063300 of size 256 next 227\n",
      "2024-03-07 20:07:42.342576: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063400 of size 256 next 236\n",
      "2024-03-07 20:07:42.342586: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063500 of size 256 next 885\n",
      "2024-03-07 20:07:42.342596: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063600 of size 256 next 862\n",
      "2024-03-07 20:07:42.342605: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063700 of size 256 next 953\n",
      "2024-03-07 20:07:42.342615: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063800 of size 256 next 1129\n",
      "2024-03-07 20:07:42.342625: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063900 of size 256 next 72\n",
      "2024-03-07 20:07:42.342634: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063a00 of size 256 next 246\n",
      "2024-03-07 20:07:42.342644: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063b00 of size 512 next 669\n",
      "2024-03-07 20:07:42.342653: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063d00 of size 256 next 976\n",
      "2024-03-07 20:07:42.342663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063e00 of size 256 next 1231\n",
      "2024-03-07 20:07:42.342671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42063f00 of size 256 next 1026\n",
      "2024-03-07 20:07:42.342681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064000 of size 512 next 977\n",
      "2024-03-07 20:07:42.342692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064200 of size 768 next 880\n",
      "2024-03-07 20:07:42.342702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064500 of size 256 next 996\n",
      "2024-03-07 20:07:42.342712: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064600 of size 256 next 13\n",
      "2024-03-07 20:07:42.342723: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064700 of size 256 next 951\n",
      "2024-03-07 20:07:42.342733: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42064800 of size 2048 next 988\n",
      "2024-03-07 20:07:42.342743: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42065000 of size 512 next 921\n",
      "2024-03-07 20:07:42.342753: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42065200 of size 512 next 997\n",
      "2024-03-07 20:07:42.342763: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42065400 of size 2048 next 1194\n",
      "2024-03-07 20:07:42.342773: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42065c00 of size 512 next 132\n",
      "2024-03-07 20:07:42.342783: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42065e00 of size 512 next 44\n",
      "2024-03-07 20:07:42.342793: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066000 of size 512 next 901\n",
      "2024-03-07 20:07:42.342800: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066200 of size 512 next 1168\n",
      "2024-03-07 20:07:42.342807: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066400 of size 256 next 1001\n",
      "2024-03-07 20:07:42.342814: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066500 of size 256 next 1111\n",
      "2024-03-07 20:07:42.342821: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066600 of size 256 next 113\n",
      "2024-03-07 20:07:42.342829: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066700 of size 256 next 140\n",
      "2024-03-07 20:07:42.342839: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066800 of size 256 next 1009\n",
      "2024-03-07 20:07:42.342849: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066900 of size 256 next 1133\n",
      "2024-03-07 20:07:42.342857: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066a00 of size 256 next 58\n",
      "2024-03-07 20:07:42.342865: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066b00 of size 256 next 142\n",
      "2024-03-07 20:07:42.342872: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066c00 of size 512 next 244\n",
      "2024-03-07 20:07:42.342882: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42066e00 of size 512 next 942\n",
      "2024-03-07 20:07:42.342890: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067000 of size 256 next 1015\n",
      "2024-03-07 20:07:42.342897: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067100 of size 256 next 1148\n",
      "2024-03-07 20:07:42.342905: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067200 of size 256 next 41\n",
      "2024-03-07 20:07:42.342915: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067300 of size 512 next 1195\n",
      "2024-03-07 20:07:42.342923: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067500 of size 1024 next 902\n",
      "2024-03-07 20:07:42.342930: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42067900 of size 2048 next 771\n",
      "2024-03-07 20:07:42.342937: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068100 of size 512 next 970\n",
      "2024-03-07 20:07:42.342945: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068300 of size 1024 next 900\n",
      "2024-03-07 20:07:42.342954: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068700 of size 512 next 1022\n",
      "2024-03-07 20:07:42.342963: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068900 of size 768 next 830\n",
      "2024-03-07 20:07:42.342972: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068c00 of size 256 next 1034\n",
      "2024-03-07 20:07:42.342982: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068d00 of size 256 next 1169\n",
      "2024-03-07 20:07:42.342992: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068e00 of size 256 next 1196\n",
      "2024-03-07 20:07:42.343001: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42068f00 of size 256 next 917\n",
      "2024-03-07 20:07:42.343012: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069000 of size 256 next 247\n",
      "2024-03-07 20:07:42.343021: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069100 of size 256 next 214\n",
      "2024-03-07 20:07:42.343032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069200 of size 512 next 1023\n",
      "2024-03-07 20:07:42.343041: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069400 of size 512 next 1007\n",
      "2024-03-07 20:07:42.343051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069600 of size 256 next 989\n",
      "2024-03-07 20:07:42.343067: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069700 of size 256 next 1052\n",
      "2024-03-07 20:07:42.343077: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069800 of size 256 next 1003\n",
      "2024-03-07 20:07:42.343087: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42069900 of size 52224 next 283\n",
      "2024-03-07 20:07:42.343098: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42076500 of size 51200 next 1070\n",
      "2024-03-07 20:07:42.343109: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42082d00 of size 78848 next 163\n",
      "2024-03-07 20:07:42.343119: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42096100 of size 512 next 1047\n",
      "2024-03-07 20:07:42.343128: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42096300 of size 67072 next 1108\n",
      "2024-03-07 20:07:42.343137: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420a6900 of size 2048 next 190\n",
      "2024-03-07 20:07:42.343147: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420a7100 of size 78336 next 176\n",
      "2024-03-07 20:07:42.343156: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420ba300 of size 51200 next 883\n",
      "2024-03-07 20:07:42.343165: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420c6b00 of size 65536 next 1043\n",
      "2024-03-07 20:07:42.343174: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420d6b00 of size 105728 next 1064\n",
      "2024-03-07 20:07:42.343184: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420f0800 of size 40192 next 909\n",
      "2024-03-07 20:07:42.343193: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c420fa500 of size 49920 next 96\n",
      "2024-03-07 20:07:42.343203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106800 of size 256 next 189\n",
      "2024-03-07 20:07:42.343211: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106900 of size 256 next 1029\n",
      "2024-03-07 20:07:42.343220: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106a00 of size 512 next 144\n",
      "2024-03-07 20:07:42.343229: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106c00 of size 512 next 119\n",
      "2024-03-07 20:07:42.343239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42106e00 of size 512 next 218\n",
      "2024-03-07 20:07:42.343247: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107000 of size 256 next 613\n",
      "2024-03-07 20:07:42.343256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107100 of size 256 next 1072\n",
      "2024-03-07 20:07:42.343266: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107200 of size 256 next 961\n",
      "2024-03-07 20:07:42.343275: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107300 of size 256 next 1063\n",
      "2024-03-07 20:07:42.343284: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107400 of size 256 next 1144\n",
      "2024-03-07 20:07:42.343293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107500 of size 256 next 73\n",
      "2024-03-07 20:07:42.343301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107600 of size 256 next 1004\n",
      "2024-03-07 20:07:42.343310: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107700 of size 256 next 1136\n",
      "2024-03-07 20:07:42.343320: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107800 of size 256 next 36\n",
      "2024-03-07 20:07:42.343329: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107900 of size 256 next 866\n",
      "2024-03-07 20:07:42.343338: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107a00 of size 256 next 221\n",
      "2024-03-07 20:07:42.343346: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42107b00 of size 65536 next 269\n",
      "2024-03-07 20:07:42.343353: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42117b00 of size 40192 next 1032\n",
      "2024-03-07 20:07:42.343360: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42121800 of size 51200 next 120\n",
      "2024-03-07 20:07:42.343367: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4212e000 of size 65536 next 1110\n",
      "2024-03-07 20:07:42.343374: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c4213e000 of size 76544 next 252\n",
      "2024-03-07 20:07:42.343381: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42150b00 of size 256 next 1071\n",
      "2024-03-07 20:07:42.343390: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0c42150c00 of size 1800000000 next 145\n",
      "2024-03-07 20:07:42.343400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0cad5ede00 of size 1800000000 next 981\n",
      "2024-03-07 20:07:42.343410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d18a8b000 of size 1801112320 next 114\n",
      "2024-03-07 20:07:42.343420: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037b00 of size 512 next 124\n",
      "2024-03-07 20:07:42.343428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037d00 of size 512 next 978\n",
      "2024-03-07 20:07:42.343438: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84037f00 of size 51200 next 187\n",
      "2024-03-07 20:07:42.343448: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044700 of size 1024 next 262\n",
      "2024-03-07 20:07:42.343457: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044b00 of size 256 next 1035\n",
      "2024-03-07 20:07:42.343467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044c00 of size 256 next 1197\n",
      "2024-03-07 20:07:42.343476: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044d00 of size 256 next 111\n",
      "2024-03-07 20:07:42.343484: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044e00 of size 256 next 1065\n",
      "2024-03-07 20:07:42.343492: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84044f00 of size 256 next 957\n",
      "2024-03-07 20:07:42.343503: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045000 of size 2048 next 277\n",
      "2024-03-07 20:07:42.343512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045800 of size 512 next 267\n",
      "2024-03-07 20:07:42.343521: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045a00 of size 512 next 110\n",
      "2024-03-07 20:07:42.343533: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84045c00 of size 79872 next 911\n",
      "2024-03-07 20:07:42.343541: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059400 of size 512 next 59\n",
      "2024-03-07 20:07:42.343551: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059600 of size 256 next 45\n",
      "2024-03-07 20:07:42.343561: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059700 of size 256 next 122\n",
      "2024-03-07 20:07:42.343571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84059800 of size 50688 next 1078\n",
      "2024-03-07 20:07:42.343581: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84065e00 of size 256 next 846\n",
      "2024-03-07 20:07:42.343592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84065f00 of size 512 next 46\n",
      "2024-03-07 20:07:42.343602: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84066100 of size 65536 next 1102\n",
      "2024-03-07 20:07:42.343612: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d84076100 of size 161483008 next 21\n",
      "2024-03-07 20:07:42.343619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8da76a00 of size 40192 next 1135\n",
      "2024-03-07 20:07:42.343626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8da80700 of size 76544 next 1058\n",
      "2024-03-07 20:07:42.343636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8da93200 of size 65536 next 131\n",
      "2024-03-07 20:07:42.343644: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daa3200 of size 40192 next 971\n",
      "2024-03-07 20:07:42.343652: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daacf00 of size 61184 next 255\n",
      "2024-03-07 20:07:42.343659: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dabbe00 of size 52224 next 940\n",
      "2024-03-07 20:07:42.343669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dac8a00 of size 51200 next 181\n",
      "2024-03-07 20:07:42.343677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dad5200 of size 55040 next 95\n",
      "2024-03-07 20:07:42.343686: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dae2900 of size 512 next 243\n",
      "2024-03-07 20:07:42.343694: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dae2b00 of size 65536 next 1201\n",
      "2024-03-07 20:07:42.343701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daf2b00 of size 512 next 912\n",
      "2024-03-07 20:07:42.343709: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daf2d00 of size 51200 next 1167\n",
      "2024-03-07 20:07:42.343717: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daff500 of size 1024 next 1101\n",
      "2024-03-07 20:07:42.343727: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daff900 of size 256 next 158\n",
      "2024-03-07 20:07:42.343737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daffa00 of size 256 next 1046\n",
      "2024-03-07 20:07:42.343746: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daffb00 of size 256 next 1193\n",
      "2024-03-07 20:07:42.343756: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daffc00 of size 512 next 1229\n",
      "2024-03-07 20:07:42.343765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8daffe00 of size 2048 next 1036\n",
      "2024-03-07 20:07:42.343775: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00600 of size 512 next 33\n",
      "2024-03-07 20:07:42.343785: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00800 of size 512 next 1060\n",
      "2024-03-07 20:07:42.343795: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00a00 of size 256 next 1151\n",
      "2024-03-07 20:07:42.343805: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00b00 of size 256 next 1103\n",
      "2024-03-07 20:07:42.343818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00c00 of size 256 next 1083\n",
      "2024-03-07 20:07:42.343829: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00d00 of size 256 next 139\n",
      "2024-03-07 20:07:42.343839: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db00e00 of size 512 next 202\n",
      "2024-03-07 20:07:42.343849: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db01000 of size 512 next 80\n",
      "2024-03-07 20:07:42.343859: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db01200 of size 64000 next 1173\n",
      "2024-03-07 20:07:42.343869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db10c00 of size 65536 next 84\n",
      "2024-03-07 20:07:42.343877: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db20c00 of size 256 next 234\n",
      "2024-03-07 20:07:42.343884: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db20d00 of size 512 next 1191\n",
      "2024-03-07 20:07:42.343891: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db20f00 of size 65536 next 175\n",
      "2024-03-07 20:07:42.343898: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db30f00 of size 40192 next 949\n",
      "2024-03-07 20:07:42.343906: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db3ac00 of size 60416 next 32\n",
      "2024-03-07 20:07:42.343914: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db49800 of size 512 next 845\n",
      "2024-03-07 20:07:42.343924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db49a00 of size 512 next 967\n",
      "2024-03-07 20:07:42.343933: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db49c00 of size 512 next 78\n",
      "2024-03-07 20:07:42.343944: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db49e00 of size 512 next 1096\n",
      "2024-03-07 20:07:42.343954: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db4a000 of size 51200 next 118\n",
      "2024-03-07 20:07:42.343963: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db56800 of size 1024 next 160\n",
      "2024-03-07 20:07:42.343970: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db56c00 of size 256 next 6\n",
      "2024-03-07 20:07:42.343978: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db56d00 of size 256 next 241\n",
      "2024-03-07 20:07:42.343987: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db56e00 of size 256 next 969\n",
      "2024-03-07 20:07:42.343996: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db56f00 of size 256 next 1147\n",
      "2024-03-07 20:07:42.344006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db57000 of size 256 next 890\n",
      "2024-03-07 20:07:42.344016: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db57100 of size 2048 next 231\n",
      "2024-03-07 20:07:42.344025: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db57900 of size 512 next 10\n",
      "2024-03-07 20:07:42.344045: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db57b00 of size 512 next 1037\n",
      "2024-03-07 20:07:42.344055: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db57d00 of size 79872 next 89\n",
      "2024-03-07 20:07:42.344065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db6b500 of size 512 next 703\n",
      "2024-03-07 20:07:42.344073: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db6b700 of size 256 next 127\n",
      "2024-03-07 20:07:42.344080: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db6b800 of size 256 next 982\n",
      "2024-03-07 20:07:42.344088: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db6b900 of size 50688 next 1031\n",
      "2024-03-07 20:07:42.344097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db77f00 of size 256 next 876\n",
      "2024-03-07 20:07:42.344107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db78000 of size 512 next 233\n",
      "2024-03-07 20:07:42.344117: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db78200 of size 65536 next 211\n",
      "2024-03-07 20:07:42.344126: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db88200 of size 512 next 14\n",
      "2024-03-07 20:07:42.344136: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db88400 of size 51200 next 180\n",
      "2024-03-07 20:07:42.344146: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db94c00 of size 1024 next 1122\n",
      "2024-03-07 20:07:42.344155: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db95000 of size 512 next 630\n",
      "2024-03-07 20:07:42.344162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db95200 of size 768 next 260\n",
      "2024-03-07 20:07:42.344172: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db95500 of size 2048 next 107\n",
      "2024-03-07 20:07:42.344182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db95d00 of size 512 next 903\n",
      "2024-03-07 20:07:42.344192: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db95f00 of size 512 next 1155\n",
      "2024-03-07 20:07:42.344203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8db96100 of size 79872 next 1159\n",
      "2024-03-07 20:07:42.344213: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dba9900 of size 512 next 309\n",
      "2024-03-07 20:07:42.344221: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dba9b00 of size 256 next 1138\n",
      "2024-03-07 20:07:42.344231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dba9c00 of size 256 next 1054\n",
      "2024-03-07 20:07:42.344239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dba9d00 of size 50688 next 90\n",
      "2024-03-07 20:07:42.344248: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbb6300 of size 256 next 814\n",
      "2024-03-07 20:07:42.344258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbb6400 of size 512 next 937\n",
      "2024-03-07 20:07:42.344268: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbb6600 of size 65536 next 915\n",
      "2024-03-07 20:07:42.344278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbc6600 of size 40192 next 995\n",
      "2024-03-07 20:07:42.344288: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbd0300 of size 66048 next 261\n",
      "2024-03-07 20:07:42.344297: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe0500 of size 1024 next 20\n",
      "2024-03-07 20:07:42.344306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe0900 of size 512 next 1223\n",
      "2024-03-07 20:07:42.344315: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe0b00 of size 768 next 984\n",
      "2024-03-07 20:07:42.344325: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe0e00 of size 2048 next 29\n",
      "2024-03-07 20:07:42.344335: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe1600 of size 512 next 884\n",
      "2024-03-07 20:07:42.344343: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe1800 of size 512 next 972\n",
      "2024-03-07 20:07:42.344352: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbe1a00 of size 79872 next 1067\n",
      "2024-03-07 20:07:42.344360: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbf5200 of size 512 next 936\n",
      "2024-03-07 20:07:42.344368: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbf5400 of size 256 next 920\n",
      "2024-03-07 20:07:42.344376: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbf5500 of size 256 next 1131\n",
      "2024-03-07 20:07:42.344387: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dbf5600 of size 50688 next 1095\n",
      "2024-03-07 20:07:42.344396: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc01c00 of size 256 next 1177\n",
      "2024-03-07 20:07:42.344405: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc01d00 of size 512 next 1192\n",
      "2024-03-07 20:07:42.344414: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc01f00 of size 65536 next 161\n",
      "2024-03-07 20:07:42.344422: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc11f00 of size 40192 next 1082\n",
      "2024-03-07 20:07:42.344430: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc1bc00 of size 65536 next 1230\n",
      "2024-03-07 20:07:42.344439: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc2bc00 of size 65536 next 1025\n",
      "2024-03-07 20:07:42.344449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc3bc00 of size 51200 next 965\n",
      "2024-03-07 20:07:42.344458: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc48400 of size 40192 next 887\n",
      "2024-03-07 20:07:42.344467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d8dc52100 of size 65536 next 897\n",
      "2024-03-07 20:07:42.344475: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc62100 of size 65536 next 1174\n",
      "2024-03-07 20:07:42.344485: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc72100 of size 51200 next 281\n",
      "2024-03-07 20:07:42.344494: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc7e900 of size 40192 next 948\n",
      "2024-03-07 20:07:42.344506: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc88600 of size 91392 next 196\n",
      "2024-03-07 20:07:42.344514: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dc9eb00 of size 65536 next 864\n",
      "2024-03-07 20:07:42.344524: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dcaeb00 of size 51200 next 952\n",
      "2024-03-07 20:07:42.344532: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8dcbb300 of size 786432 next 821\n",
      "2024-03-07 20:07:42.344539: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d8dd7b300 of size 8126464 next 242\n",
      "2024-03-07 20:07:42.344546: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8e53b300 of size 524288 next 92\n",
      "2024-03-07 20:07:42.344555: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d8e5bb300 of size 786432 next 186\n",
      "2024-03-07 20:07:42.344565: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d8e67b300 of size 3932160 next 1175\n",
      "2024-03-07 20:07:42.344575: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d8ea3b300 of size 24045568 next 960\n",
      "2024-03-07 20:07:42.344585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90129b00 of size 86784 next 927\n",
      "2024-03-07 20:07:42.344594: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013ee00 of size 512 next 155\n",
      "2024-03-07 20:07:42.344604: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013f000 of size 512 next 875\n",
      "2024-03-07 20:07:42.344612: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013f200 of size 768 next 943\n",
      "2024-03-07 20:07:42.344619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013f500 of size 256 next 1227\n",
      "2024-03-07 20:07:42.344626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013f600 of size 2048 next 1220\n",
      "2024-03-07 20:07:42.344633: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013fe00 of size 256 next 1142\n",
      "2024-03-07 20:07:42.344641: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9013ff00 of size 256 next 245\n",
      "2024-03-07 20:07:42.344650: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90140000 of size 256 next 910\n",
      "2024-03-07 20:07:42.344657: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90140100 of size 1024 next 693\n",
      "2024-03-07 20:07:42.344664: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90140500 of size 256 next 1126\n",
      "2024-03-07 20:07:42.344671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90140600 of size 2048 next 886\n",
      "2024-03-07 20:07:42.344678: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90140e00 of size 512 next 974\n",
      "2024-03-07 20:07:42.344685: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141000 of size 512 next 906\n",
      "2024-03-07 20:07:42.344692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141200 of size 512 next 165\n",
      "2024-03-07 20:07:42.344702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141400 of size 512 next 249\n",
      "2024-03-07 20:07:42.344709: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141600 of size 512 next 1120\n",
      "2024-03-07 20:07:42.344716: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141800 of size 512 next 1163\n",
      "2024-03-07 20:07:42.344725: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141a00 of size 1024 next 1161\n",
      "2024-03-07 20:07:42.344733: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141e00 of size 256 next 923\n",
      "2024-03-07 20:07:42.344740: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90141f00 of size 256 next 1074\n",
      "2024-03-07 20:07:42.344748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142000 of size 256 next 1098\n",
      "2024-03-07 20:07:42.344757: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142100 of size 256 next 270\n",
      "2024-03-07 20:07:42.344764: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142200 of size 256 next 123\n",
      "2024-03-07 20:07:42.344771: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142300 of size 256 next 57\n",
      "2024-03-07 20:07:42.344778: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142400 of size 256 next 204\n",
      "2024-03-07 20:07:42.344785: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142500 of size 256 next 179\n",
      "2024-03-07 20:07:42.344792: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142600 of size 256 next 63\n",
      "2024-03-07 20:07:42.344802: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142700 of size 512 next 207\n",
      "2024-03-07 20:07:42.344809: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142900 of size 512 next 1204\n",
      "2024-03-07 20:07:42.344816: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142b00 of size 512 next 81\n",
      "2024-03-07 20:07:42.344823: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90142d00 of size 768 next 1056\n",
      "2024-03-07 20:07:42.344830: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90143000 of size 512 next 86\n",
      "2024-03-07 20:07:42.344837: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90143200 of size 512 next 1128\n",
      "2024-03-07 20:07:42.344847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90143400 of size 512 next 135\n",
      "2024-03-07 20:07:42.344854: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90143600 of size 256 next 136\n",
      "2024-03-07 20:07:42.344862: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90143700 of size 2304 next 783\n",
      "2024-03-07 20:07:42.344871: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90144000 of size 256 next 856\n",
      "2024-03-07 20:07:42.344879: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90144100 of size 512 next 251\n",
      "2024-03-07 20:07:42.344886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90144300 of size 1024 next 138\n",
      "2024-03-07 20:07:42.344895: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90144700 of size 2048 next 229\n",
      "2024-03-07 20:07:42.344903: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90144f00 of size 256 next 1062\n",
      "2024-03-07 20:07:42.344910: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145000 of size 256 next 1200\n",
      "2024-03-07 20:07:42.344917: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145100 of size 256 next 1069\n",
      "2024-03-07 20:07:42.344925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145200 of size 256 next 164\n",
      "2024-03-07 20:07:42.344934: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145300 of size 256 next 184\n",
      "2024-03-07 20:07:42.344941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d90145400 of size 512 next 994\n",
      "2024-03-07 20:07:42.344949: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145600 of size 256 next 1020\n",
      "2024-03-07 20:07:42.344958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145700 of size 256 next 273\n",
      "2024-03-07 20:07:42.344965: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90145800 of size 256 next 986\n",
      "2024-03-07 20:07:42.344972: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f0d90145900 of size 21248 next 170\n",
      "2024-03-07 20:07:42.344979: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9014ac00 of size 51200 next 958\n",
      "2024-03-07 20:07:42.344986: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90157400 of size 65536 next 1206\n",
      "2024-03-07 20:07:42.344993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90167400 of size 51200 next 895\n",
      "2024-03-07 20:07:42.345000: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90173c00 of size 70400 next 66\n",
      "2024-03-07 20:07:42.345021: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90184f00 of size 51200 next 1199\n",
      "2024-03-07 20:07:42.345046: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d90191700 of size 72192 next 239\n",
      "2024-03-07 20:07:42.345056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d901a3100 of size 40192 next 98\n",
      "2024-03-07 20:07:42.345064: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d901ace00 of size 91392 next 254\n",
      "2024-03-07 20:07:42.345072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d901c3300 of size 65536 next 1061\n",
      "2024-03-07 20:07:42.345082: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d901d3300 of size 78643200 next 313\n",
      "2024-03-07 20:07:42.345090: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d94cd3300 of size 78643200 next 1145\n",
      "2024-03-07 20:07:42.345097: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d997d3300 of size 100663296 next 1105\n",
      "2024-03-07 20:07:42.345106: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0d9f7d3300 of size 100663296 next 954\n",
      "2024-03-07 20:07:42.345116: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0da57d3300 of size 100663296 next 1116\n",
      "2024-03-07 20:07:42.345123: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0dab7d3300 of size 33554432 next 966\n",
      "2024-03-07 20:07:42.345130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0dad7d3300 of size 33554432 next 265\n",
      "2024-03-07 20:07:42.345137: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0daf7d3300 of size 33554432 next 312\n",
      "2024-03-07 20:07:42.345145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0db17d3300 of size 33554432 next 1000\n",
      "2024-03-07 20:07:42.345154: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0db37d3300 of size 33554432 next 47\n",
      "2024-03-07 20:07:42.345162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0db57d3300 of size 26214400 next 1115\n",
      "2024-03-07 20:07:42.345169: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0db70d3300 of size 26214400 next 1117\n",
      "2024-03-07 20:07:42.345176: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0db89d3300 of size 26214400 next 1040\n",
      "2024-03-07 20:07:42.345186: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0dba2d3300 of size 26214400 next 19\n",
      "2024-03-07 20:07:42.345193: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f0dbbbd3300 of size 31182080 next 18446744073709551615\n",
      "2024-03-07 20:07:42.345200: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2024-03-07 20:07:42.345212: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 181 Chunks of size 256 totalling 45.2KiB\n",
      "2024-03-07 20:07:42.345222: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 81 Chunks of size 512 totalling 40.5KiB\n",
      "2024-03-07 20:07:42.345230: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 9 Chunks of size 768 totalling 6.8KiB\n",
      "2024-03-07 20:07:42.345241: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 14 Chunks of size 1024 totalling 14.0KiB\n",
      "2024-03-07 20:07:42.345249: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2024-03-07 20:07:42.345257: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1792 totalling 1.8KiB\n",
      "2024-03-07 20:07:42.345266: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 2048 totalling 24.0KiB\n",
      "2024-03-07 20:07:42.345275: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2024-03-07 20:07:42.345283: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2816 totalling 2.8KiB\n",
      "2024-03-07 20:07:42.345291: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\n",
      "2024-03-07 20:07:42.345302: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 40192 totalling 471.0KiB\n",
      "2024-03-07 20:07:42.345310: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 49920 totalling 48.8KiB\n",
      "2024-03-07 20:07:42.345318: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 50688 totalling 198.0KiB\n",
      "2024-03-07 20:07:42.345326: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 16 Chunks of size 51200 totalling 800.0KiB\n",
      "2024-03-07 20:07:42.345337: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 52224 totalling 102.0KiB\n",
      "2024-03-07 20:07:42.345345: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 52736 totalling 51.5KiB\n",
      "2024-03-07 20:07:42.345353: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 55040 totalling 53.8KiB\n",
      "2024-03-07 20:07:42.345363: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 60416 totalling 59.0KiB\n",
      "2024-03-07 20:07:42.345374: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 61184 totalling 59.8KiB\n",
      "2024-03-07 20:07:42.345382: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 62464 totalling 61.0KiB\n",
      "2024-03-07 20:07:42.345392: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 64000 totalling 62.5KiB\n",
      "2024-03-07 20:07:42.345400: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 18 Chunks of size 65536 totalling 1.12MiB\n",
      "2024-03-07 20:07:42.345409: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 66048 totalling 64.5KiB\n",
      "2024-03-07 20:07:42.345419: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 67072 totalling 65.5KiB\n",
      "2024-03-07 20:07:42.345427: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 70400 totalling 68.8KiB\n",
      "2024-03-07 20:07:42.345435: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 72192 totalling 70.5KiB\n",
      "2024-03-07 20:07:42.345443: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 76544 totalling 149.5KiB\n",
      "2024-03-07 20:07:42.345451: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 78336 totalling 76.5KiB\n",
      "2024-03-07 20:07:42.345460: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 78848 totalling 77.0KiB\n",
      "2024-03-07 20:07:42.345470: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 79872 totalling 312.0KiB\n",
      "2024-03-07 20:07:42.345478: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 86784 totalling 84.8KiB\n",
      "2024-03-07 20:07:42.345486: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 91392 totalling 178.5KiB\n",
      "2024-03-07 20:07:42.345494: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 105728 totalling 103.2KiB\n",
      "2024-03-07 20:07:42.345502: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 524288 totalling 512.0KiB\n",
      "2024-03-07 20:07:42.345510: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 786432 totalling 768.0KiB\n",
      "2024-03-07 20:07:42.345518: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3932160 totalling 3.75MiB\n",
      "2024-03-07 20:07:42.345530: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 26214400 totalling 100.00MiB\n",
      "2024-03-07 20:07:42.345538: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 31182080 totalling 29.74MiB\n",
      "2024-03-07 20:07:42.345546: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 33554432 totalling 160.00MiB\n",
      "2024-03-07 20:07:42.345554: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 78643200 totalling 150.00MiB\n",
      "2024-03-07 20:07:42.345562: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 100663296 totalling 288.00MiB\n",
      "2024-03-07 20:07:42.345570: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 161483008 totalling 154.00MiB\n",
      "2024-03-07 20:07:42.345580: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1800000000 totalling 3.35GiB\n",
      "2024-03-07 20:07:42.345589: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1801112320 totalling 1.68GiB\n",
      "2024-03-07 20:07:42.345597: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 5.90GiB\n",
      "2024-03-07 20:07:42.345605: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 6368591872 memory_limit_: 6368591872 available bytes: 0 curr_region_allocation_bytes_: 12737183744\n",
      "2024-03-07 20:07:42.345618: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                      6368591872\n",
      "InUse:                      6335546112\n",
      "MaxInUse:                   6337970432\n",
      "NumAllocs:                   108247628\n",
      "MaxAllocSize:               1801112320\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2024-03-07 20:07:42.345640: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ****************************************************************************************************\n",
      "2024-03-07 20:07:42.352334: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at matmul_op_impl.h:681 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[65536,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model_8/output/MatMul/MatMul' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27/1818802773.py\", line 3, in <module>\n      train_loop(train_data, plt_data, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20',model='/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf',\n    File \"/tmp/ipykernel_27/2672725750.py\", line 211, in train_loop\n      batch_model, min_chi2, chi2_mean_list, min_loss, loss_list = train_super_epoch_choose_best(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir=train_dir,\n    File \"/tmp/ipykernel_27/2672725750.py\", line 161, in train_super_epoch_choose_best\n      dctr, model_list, loss_list = train_super_epoch(model, train_data, batch_size, repeat, train_dir = train_dir, input_dim=input_dim,\n    File \"/tmp/ipykernel_27/2672725750.py\", line 143, in train_super_epoch\n      loss_val = train(dctr, callbacks, x_train, y_train, x_val, y_val, wgt_train=wgt_train, wgt_val=wgt_val,\n    File \"/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py\", line 634, in train\n      history = dctr.fit(X_train, Y_train,\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_8/output/MatMul/MatMul'\nOOM when allocating tensor with shape[65536,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_8/output/MatMul/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8889536]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_patience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 211\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0_plt_nrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 161\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_super_epoch_choose_best\u001b[39m(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m                                   input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, Phi_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m128\u001b[39m), F_sizes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m100\u001b[39m), loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, l2_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    158\u001b[0m                                   Phi_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m), F_acts\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m), output_act\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     dctr, model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m     dctr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# loss_val = dctr.evaluate(x_val, y_val, sample_weight=pd.Series(wgt_val).to_frame('w_v'), batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py:634\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, wgt_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[1;32m    625\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m, savePath\u001b[38;5;241m=\u001b[39mcurrentPath, saveLabel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDCTR_training\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    method to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m    allows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    plots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    636\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model_8/output/MatMul/MatMul' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27/1818802773.py\", line 3, in <module>\n      train_loop(train_data, plt_data, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20',model='/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf',\n    File \"/tmp/ipykernel_27/2672725750.py\", line 211, in train_loop\n      batch_model, min_chi2, chi2_mean_list, min_loss, loss_list = train_super_epoch_choose_best(model, train_data, batch_size, repeat, epochs, super_epoch, x0_plt_nrm, train_dir=train_dir,\n    File \"/tmp/ipykernel_27/2672725750.py\", line 161, in train_super_epoch_choose_best\n      dctr, model_list, loss_list = train_super_epoch(model, train_data, batch_size, repeat, train_dir = train_dir, input_dim=input_dim,\n    File \"/tmp/ipykernel_27/2672725750.py\", line 143, in train_super_epoch\n      loss_val = train(dctr, callbacks, x_train, y_train, x_val, y_val, wgt_train=wgt_train, wgt_val=wgt_val,\n    File \"/tf/home/gdrive/_STUDIUM_/DCTR_Paper/DCTR.py\", line 634, in train\n      history = dctr.fit(X_train, Y_train,\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model_8/output/MatMul/MatMul'\nOOM when allocating tensor with shape[65536,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/model_8/output/MatMul/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_8889536]"
     ]
    }
   ],
   "source": [
    "# /tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf\n",
    "\n",
    "train_loop(train_data, plt_data, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d20',model='/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train_d10/super_epoch_16/run_2/s-16_b-65536_r-2.tf', \n",
    "           lowest_chi2 = 3, repeat=4, super_epochs=35, starting_super_epoch=20, super_patience = 3, learning_rate=0.0005, dropout = 0.2, \n",
    "           batch_sizes=[6*8192, 8*8192, 12*8192])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
