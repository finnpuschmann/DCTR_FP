{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to improve the model in the high p_T (top) areas that it curently struggles in\n",
    "\n",
    "-> fit an A*exp(b*p_t) to p_t bins\n",
    "-> multiply wgts by exp(b*p_T)/max(exp(b*p_T)) -> low p_t -> very low wgts ; high p_T -> weights +/- 1 \n",
    "        -> Problem: most weights very very small -> large loss of statistics\n",
    "            -> normalizing to mean of 1 -> some small percentage of weights get very large (>2000), but most are below 1\n",
    "\n",
    "-> IDEA: Use 10M events with these weigths and 10M with (scaled (20%)) org weights, since we don't want to lose the low p_t accuracy but also want the network to learn these high p_T events\n",
    "    For NNLO use same events with new weights, NLO use different events (since we have them)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# import system modules\n",
    "import sys\n",
    "import os\n",
    "os.system('for a in /sys/bus/pci/devices/*; do echo 0 | tee -a $a/numa_node>/dev/null; done') # get rid of NUMA node warnings: https://github.com/tensorflow/tensorflow/issues/42738\n",
    "import gc\n",
    "\n",
    "# import standard numerical modules\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# import machine learning modules\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "gpu = tf.config.list_physical_devices('GPU') # make sure GPU usage is enabled\n",
    "tf.config.experimental.set_virtual_device_configuration(gpu[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=int(7.5*1024))])\n",
    "print(gpu) \n",
    "\n",
    "sys.path.append('../') # path th DCTR.py\n",
    "import DCTR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def exponential(x, A, b):\n",
    "    value = A*np.exp(np.multiply(x,b))\n",
    "    return value\n",
    "\n",
    "def get_exponential_wgts(X, part_index, arg_index, cut_off = 1000, div = 31):\n",
    "    bins = np.linspace(min(X[:,part_index,arg_index]),cut_off, div)\n",
    "    n, bins = np.histogram(X[:,part_index,arg_index], bins=bins)\n",
    "    bin_centers = bins[:-1]+0.5*(bins[1:]-bins[:-1])\n",
    "    max_n = max(n)\n",
    "    where_max=np.squeeze(np.where(n == max_n))\n",
    "    # print(bins[where_max])\n",
    "    \n",
    "    # fit exp to histogram\n",
    "    p0 = (5e6, -0.015)\n",
    "    popt, pcov = curve_fit(exponential, bin_centers[where_max:], n[where_max:], p0)\n",
    "\n",
    "    # calculate the wgts as the inverse of the exponential and the max value as normalization\n",
    "    # only use events within cut_off\n",
    "    x_cut = [pt for pt in X[:,part_index,arg_index] if pt <= cut_off]\n",
    "    \n",
    "    max_exp = max(exponential(x_cut, 1, -1*popt[1]))\n",
    "    \n",
    "    # set wgts above cut off to 1\n",
    "    wgts = []\n",
    "    for i in range(len(x0_1)):\n",
    "        if X[i,part_index,arg_index] >= cut_off:\n",
    "            wgts.append(1)\n",
    "        else:\n",
    "            wgts.append(exponential(X[i,part_index,arg_index], 1, -1*popt[1])/max_exp)\n",
    "\n",
    "    return wgts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory with pre converted lhe files as numpy arrays\n",
    "data_dir = '../Data' # modify as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POWHEG hvq x0_1.shape:         (9553938, 3, 9)\n",
      "POWHEG hvq x0_2.shape:         (9553938, 3, 9)\n",
      "63331\n"
     ]
    }
   ],
   "source": [
    "# Load POWHEG hvq x0 datasets\n",
    "# x0_nrm for training, x0_plt and x0_plt_nrm for calculating stats used to decide which model performs best\n",
    "# only contain tt-pair; every event has order: \n",
    "    # tt-pair, top, anti-top\n",
    "# every particle has arguments: \n",
    "    # [pt, y, phi, mass, eta, E, PID, w, theta]\n",
    "    # [0 , 1, 2  , 3   , 4  , 5, 6  , 7, 8    ]\n",
    "\n",
    "# POWHEG hvq\n",
    "\n",
    "# unnormalized dataset for checking which events fall into one (or more) of the categories that should get higher weights\n",
    "x0_1 = []\n",
    "x0_1 = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/01-02_converted_lhe.npz', i=3)[:9553938] # 9553938 num of NNLO samples\n",
    "print(f'POWHEG hvq x0_1.shape:         {x0_1.shape}')\n",
    "\n",
    "x0_2 = []\n",
    "x0_2 = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/01-02_converted_lhe.npz', i=3)[int(1e7):int(1e7 + 9553938)] # 10M different samples than above\n",
    "print(f'POWHEG hvq x0_2.shape:         {x0_2.shape}')\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POWHEG hvq x0_nrm_1.shape:     (9553938, 3, 9)\n",
      "POWHEG hvq x0_nrm_1.shape:     (9553938, 3, 9)\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# same data as above, but normalized\n",
    "x0_1_nrm = []\n",
    "x0_1_nrm = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/01-02_normed_converted_lhe.npz', i=3)[:9553938] \n",
    "print(f'POWHEG hvq x0_nrm_1.shape:     {x0_1_nrm.shape}')\n",
    "\n",
    "x0_2_nrm = []\n",
    "x0_2_nrm = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/01-02_normed_converted_lhe.npz', i=3)[int(1e7):int(1e7 + 9553938)] # 10M different samples than above\n",
    "print(f'POWHEG hvq x0_nrm_1.shape:     {x0_2_nrm.shape}')\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiNNLO x1_1_nrm.shape: (9553938, 3, 9)\n",
      "MiNNLO x1_2_nrm.shape: (9553938, 3, 9)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# MiNNLO x1\n",
    "# training data\n",
    "x1_1_nrm = []\n",
    "x1_1_nrm = DCTR.load_dataset(f'{data_dir}/MiNNLO/converted_with_13TeV_NLO/normed_converted_lhe.npz', i=3)\n",
    "print(f'MiNNLO x1_1_nrm.shape: {x1_1_nrm.shape}')\n",
    "\n",
    "x1_2_nrm = x1_1_nrm.copy()\n",
    "print(f'MiNNLO x1_2_nrm.shape: {x1_2_nrm.shape}')\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POWHEG hvq x0_plt.shape:     (9553938, 3, 9)\n",
      "POWHEG hvq x0_plt_nrm.shape: (9553938, 3, 9)\n",
      "MiNNLO x1_plt.shape: (9553938, 3, 9)\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# plotting datasets (for calculating stats during super epoch)\n",
    "\n",
    "# POWHEG hvq\n",
    "# plotting data; different from training data\n",
    "x0_plt = []\n",
    "x0_plt = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/03-04_converted_lhe.npz', i=3)[:9553938]\n",
    "print(f'POWHEG hvq x0_plt.shape:     {x0_plt.shape}')\n",
    "\n",
    "\n",
    "x0_plt_nrm = [] # nrm data for calculating rwgt in statistics phase of super epoch\n",
    "x0_plt_nrm = DCTR.load_dataset(f'{data_dir}/POWHEG_hvq/13TeV/03-04_normed_converted_lhe.npz', i=3)[:9553938]\n",
    "print(f'POWHEG hvq x0_plt_nrm.shape: {x0_plt_nrm.shape}')\n",
    "\n",
    "\n",
    "# MiNNLO\n",
    "x1_plt = []\n",
    "x1_plt = DCTR.load_dataset(f'{data_dir}/MiNNLO/converted_with_13TeV_NLO/converted_lhe.npz', i=3)\n",
    "print(f'MiNNLO x1_plt.shape: {x1_plt.shape}')\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# get exponential weights\n",
    "\n",
    "x0_wgts_top = get_exponential_wgts(x0_1, 1, 0) # exp wgts for top pt\n",
    "x0_wgts_anti_top = get_exponential_wgts(x0_1, 2, 0) # exp wgts for anti-top pt\n",
    "\n",
    "x0_exp_wgts = np.add(x0_wgts_top, x0_wgts_anti_top)\n",
    "x0_nrm_exp_wgts = x0_exp_wgts/np.mean(x0_exp_wgts)\n",
    "\n",
    "# X1 MiNNLO\n",
    "x1_wgts_top = get_exponential_wgts(x1_plt, 1, 0) # exp wgts for top pt\n",
    "x1_wgts_anti_top = get_exponential_wgts(x1_plt, 2, 0) # exp wgts for anti-top pt\n",
    "\n",
    "x1_exp_wgts = np.add(x1_wgts_top, x1_wgts_anti_top)\n",
    "x1_nrm_exp_wgts = x1_exp_wgts/np.mean(x1_exp_wgts)\n",
    "\n",
    "# normalized event generator weights\n",
    "x0_1_nrm_wgts = x0_1[:,0,7]\n",
    "x0_1_nrm_wgts /= np.mean(x0_1_nrm_wgts)\n",
    "\n",
    "x0_2_nrm_wgts = x0_2[:,0,7]\n",
    "x0_2_nrm_wgts /= np.mean(x0_2_nrm_wgts)\n",
    "\n",
    "\n",
    "x1_nrm_wgts = x1_plt[:,0,7]\n",
    "x1_nrm_wgts /= np.mean(x1_nrm_wgts)\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hvq    x0_nrm.shape: (19107876, 3, 9)\n",
      "MiNNLO x1_nrm.shape: (19107876, 3, 9)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# set weights for first datasets to exp_wgt*nrm_wgts\n",
    "x0_1_nrm[:,0,7] = x0_1_nrm_wgts * x0_nrm_exp_wgts\n",
    "x1_1_nrm[:,0,7] = x1_nrm_wgts * x1_nrm_exp_wgts\n",
    "\n",
    "# set event gen wgts\n",
    "regular_multiplier = 10 # multiplier for regular events importance | 0.2x and 1x did not lead to satisfactory results; trying 10x\n",
    "x0_2_nrm[:,0,7] = regular_multiplier * x0_2_nrm_wgts\n",
    "x1_2_nrm[:,0,7] = regular_multiplier * x1_nrm_wgts\n",
    "\n",
    "# create concatenated x0 and x1 datasets with normal and exp weights\n",
    "x0_nrm = np.concatenate((x0_1_nrm, x0_2_nrm))\n",
    "x1_nrm = np.concatenate((x1_1_nrm, x1_2_nrm))\n",
    "\n",
    "x0_nrm[:,0,7] /= np.mean(x0_nrm[:,0,7]) # adjust wgts so mean is 1 across entire concated datasets incorporating above multipliers\n",
    "x1_nrm[:,0,7] /= np.mean(x1_nrm[:,0,7])\n",
    "\n",
    "print(f'hvq    x0_nrm.shape: {x0_nrm.shape}')\n",
    "print(f'MiNNLO x1_nrm.shape: {x1_nrm.shape}')\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# delete eta (pseudorapidity) and Energy -> Train only with [pt, y, phi, m, PID]\n",
    "\n",
    "# delete energy\n",
    "x0_nrm = np.delete(x0_nrm, 5, -1)\n",
    "x0_plt_nrm = np.delete(x0_plt_nrm, 5, -1)\n",
    "x1_nrm = np.delete(x1_nrm, 5, -1)\n",
    "\n",
    "# delete eta\n",
    "x0_nrm = np.delete(x0_nrm, 4, -1)\n",
    "x0_plt_nrm = np.delete(x0_plt_nrm, 4, -1)\n",
    "x1_nrm = np.delete(x1_nrm, 4, -1)\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prep arrays for training\n",
    "x_train, x_val, y_train, y_val, wgt_train, wgt_val = DCTR.prep_arrays(x0_nrm, x1_nrm, val=0.2)\n",
    "\n",
    "# bring into shape for training loop\n",
    "train_data = (x_train, y_train, x_val, y_val, wgt_train, wgt_val)\n",
    "plt_data = (x0_plt , x0_plt_nrm, x1_plt, x1_nrm_wgts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "K.clear_session() # clear gpu memory\n",
    "\n",
    "# clear temp arrays and variables from memory\n",
    "del wgt_train, wgt_val, x0_1, x0_1_nrm, x0_1_nrm_wgts, x0_2, x0_2_nrm, x0_2_nrm_wgts, x0_exp_wgts, x0_nrm, x0_nrm_exp_wgts\n",
    "del x0_plt, x0_plt_nrm, x0_wgts_anti_top, x0_wgts_top, x1_1_nrm, x1_2_nrm, x1_exp_wgts, x1_nrm, x1_nrm_exp_wgts\n",
    "del x1_nrm_wgts, x1_plt, x1_wgts_anti_top, x1_wgts_top, x_train, x_val, y_train, y_val\n",
    "\n",
    "print(gc.collect()) # gabage collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train_20240415' # where to save models during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting super_epoch 1\n",
      "\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 1 with batch_size 196608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:14:39.538964: I tensorflow/stream_executor/cuda/cuda_driver.cc:739] failed to allocate 7.50G (8053063680 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2024-04-15 13:14:39.656081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:14:59.641185: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005707376869395375.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2389 of run 0 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:19:00.006737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 1 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:23:18.232630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0005707376869395375.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2391 of run 2 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:27:37.776542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005822673439979553.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2366 of run 3 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:31:59.113945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005764737026765942.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2369 of run 4 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:36:19.736903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-196608_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2388 of run 5 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "starting run 6 of super_epoch 1 with batch_size 196608\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:40:42.383974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-196608_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2389 of run 6 of super_epoch 1 with batch_size 196608\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 196608\n",
      "in super epoch 1\n",
      "with best model ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "with chi2 105.3538 and loss 0.2369\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:49:58.654030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2390 of run 0 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:54:40.087699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2388 of run 1 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 13:59:27.418555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2383 of run 2 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:03:55.723564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2386 of run 3 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:08:17.446924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-262144_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2382 of run 4 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:12:38.180454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005764737026765942.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-262144_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2372 of run 5 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 1 with batch_size 262144\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:17:00.887887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-262144_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2385 of run 6 of super_epoch 1 with batch_size 262144\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 262144\n",
      "in super epoch 1\n",
      "with best model ./train_20240415/super_epoch_1/run_2/s-1_b-262144_r-2.tf\n",
      "with chi2 160.9736 and loss 0.2383\n",
      "starting training with batch_size: 327680 and 15 epochs\n",
      "starting with weights from model: None\n",
      "starting run 0 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:25:48.540657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_0/s-1_b-327680_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2387 of run 0 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:30:10.532826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005881192395463586.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 1 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:34:30.852097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_2/s-1_b-327680_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2384 of run 2 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:38:50.426098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_3/s-1_b-327680_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2387 of run 3 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:43:10.817630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005764737026765942.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_4/s-1_b-327680_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2381 of run 4 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:47:31.062278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_5/s-1_b-327680_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2384 of run 5 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "starting run 6 of super_epoch 1 with batch_size 327680\n",
      "reset neural network weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:51:46.579817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_1/run_6/s-1_b-327680_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2389 of run 6 of super_epoch 1 with batch_size 327680\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 327680\n",
      "in super epoch 1\n",
      "with best model ./train_20240415/super_epoch_1/run_1/s-1_b-327680_r-1.tf\n",
      "with chi2 185.2890 and loss 0.2363\n",
      "\n",
      "\n",
      "finished super_epoch 1 with 7 runs each with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tfwith chi2 105.3538 and loss 0.2369\n",
      "starting super_epoch 2\n",
      "\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2145 of run 0 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2145 of run 1 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2136 of run 2 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2127 of run 3 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-196608_r-4.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-196608_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2139 of run 4 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-196608_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-196608_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-196608_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2130 of run 5 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "starting run 6 of super_epoch 2 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-196608_r-6.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-196608_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2128 of run 6 of super_epoch 2 with batch_size 196608\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 196608\n",
      "in super epoch 2\n",
      "with best model ./train_20240415/super_epoch_2/run_4/s-2_b-196608_r-4.tf\n",
      "with chi2 53.0118 and loss 0.2139\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2140 of run 0 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-262144_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00033903527073562143.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2140 of run 1 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2166 of run 2 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2161 of run 3 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2151 of run 4 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-262144_r-5.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00033903527073562143.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-262144_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2161 of run 5 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 2 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-262144_r-6.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-262144_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2156 of run 6 of super_epoch 2 with batch_size 262144\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 262144\n",
      "in super epoch 2\n",
      "with best model ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "with chi2 51.5207 and loss 0.2151\n",
      "starting training with batch_size: 327680 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-327680_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_0/s-2_b-327680_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2154 of run 0 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-327680_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_1/s-2_b-327680_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2145 of run 1 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-327680_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_2/s-2_b-327680_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2126 of run 2 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-327680_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-327680_r-3.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00019740915158763527.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_3/s-2_b-327680_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2123 of run 3 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-327680_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_4/s-2_b-327680_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2149 of run 4 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-327680_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00019939314806833862.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_5/s-2_b-327680_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2131 of run 5 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "starting run 6 of super_epoch 2 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_1/run_4/s-1_b-196608_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-327680_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003424426191486418.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_2/run_6/s-2_b-327680_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2145 of run 6 of super_epoch 2 with batch_size 327680\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 327680\n",
      "in super epoch 2\n",
      "with best model ./train_20240415/super_epoch_2/run_6/s-2_b-327680_r-6.tf\n",
      "with chi2 76.4179 and loss 0.2145\n",
      "\n",
      "\n",
      "finished super_epoch 2 with 7 runs each with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 0.2151\n",
      "starting super_epoch 3\n",
      "\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1867 of run 0 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1887 of run 1 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 2 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1866 of run 3 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-196608_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-196608_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 4 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-196608_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-196608_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1870 of run 5 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "starting run 6 of super_epoch 3 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-196608_r-6.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-196608_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1874 of run 6 of super_epoch 3 with batch_size 196608\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 196608\n",
      "in super epoch 3\n",
      "with best model ./train_20240415/super_epoch_3/run_0/s-3_b-196608_r-0.tf\n",
      "with chi2 298.6864 and loss 0.1867\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 0 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 1 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 2 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-262144_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1886 of run 3 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-262144_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-262144_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1880 of run 4 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-262144_r-5.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-262144_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1878 of run 5 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 3 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-262144_r-6.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-262144_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 6 of super_epoch 3 with batch_size 262144\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 262144\n",
      "in super epoch 3\n",
      "with best model ./train_20240415/super_epoch_3/run_3/s-3_b-262144_r-3.tf\n",
      "with chi2 199.8985 and loss 0.1886\n",
      "starting training with batch_size: 327680 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-327680_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_0/s-3_b-327680_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 0 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-327680_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_1/s-3_b-327680_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 1 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-327680_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_2/s-3_b-327680_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1888 of run 2 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-327680_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_3/s-3_b-327680_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1886 of run 3 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-327680_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_4/s-3_b-327680_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1888 of run 4 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-327680_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-327680_r-5.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_5/s-3_b-327680_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 5 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "starting run 6 of super_epoch 3 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-327680_r-6.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_3/run_6/s-3_b-327680_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 6 of super_epoch 3 with batch_size 327680\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 327680\n",
      "in super epoch 3\n",
      "with best model ./train_20240415/super_epoch_3/run_2/s-3_b-327680_r-2.tf\n",
      "with chi2 1107.8263 and loss 0.1888\n",
      "no improvement, lowering learnng_rate to 0.001\n",
      "\n",
      "\n",
      "finished super_epoch 3 with 7 runs each with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 0.2151\n",
      "starting super_epoch 4\n",
      "\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 0 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 1 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1869 of run 2 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1870 of run 3 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-196608_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-196608_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1874 of run 4 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-196608_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-196608_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1876 of run 5 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "starting run 6 of super_epoch 4 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-196608_r-6.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-196608_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1879 of run 6 of super_epoch 4 with batch_size 196608\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 196608\n",
      "in super epoch 4\n",
      "with best model ./train_20240415/super_epoch_4/run_3/s-4_b-196608_r-3.tf\n",
      "with chi2 626.2071 and loss 0.1870\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 0 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 1 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-262144_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 2 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1879 of run 3 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-262144_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-262144_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 4 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-262144_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-262144_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 5 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 4 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-262144_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-262144_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-262144_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1893 of run 6 of super_epoch 4 with batch_size 262144\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 262144\n",
      "in super epoch 4\n",
      "with best model ./train_20240415/super_epoch_4/run_6/s-4_b-262144_r-6.tf\n",
      "with chi2 246.8381 and loss 0.1893\n",
      "starting training with batch_size: 327680 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-327680_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-327680_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_0/s-4_b-327680_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 0 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-327680_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_1/s-4_b-327680_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1885 of run 1 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-327680_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_2/s-4_b-327680_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1885 of run 2 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-327680_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_3/s-4_b-327680_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1887 of run 3 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-327680_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-327680_r-4.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_4/s-4_b-327680_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 4 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-327680_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_5/s-4_b-327680_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 5 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "starting run 6 of super_epoch 4 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-327680_r-6.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_4/run_6/s-4_b-327680_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 6 of super_epoch 4 with batch_size 327680\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 327680\n",
      "in super epoch 4\n",
      "with best model ./train_20240415/super_epoch_4/run_3/s-4_b-327680_r-3.tf\n",
      "with chi2 2327.4967 and loss 0.1887\n",
      "no improvement, lowering learnng_rate to 0.001\n",
      "\n",
      "\n",
      "finished super_epoch 4 with 7 runs each with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 0.2151\n",
      "starting super_epoch 5\n",
      "\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1875 of run 0 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1869 of run 1 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1873 of run 2 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-196608_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1887 of run 3 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-196608_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-196608_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-196608_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1896 of run 4 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-196608_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-196608_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1871 of run 5 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "starting run 6 of super_epoch 5 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-196608_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-196608_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-196608_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1890 of run 6 of super_epoch 5 with batch_size 196608\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 196608\n",
      "in super epoch 5\n",
      "with best model ./train_20240415/super_epoch_5/run_1/s-5_b-196608_r-1.tf\n",
      "with chi2 71.5261 and loss 0.1869\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1881 of run 0 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1877 of run 1 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 2 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 3 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-262144_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-262144_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1880 of run 4 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-262144_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-262144_r-5.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-262144_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1876 of run 5 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "starting run 6 of super_epoch 5 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-262144_r-6.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-262144_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 6 of super_epoch 5 with batch_size 262144\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 262144\n",
      "in super epoch 5\n",
      "with best model ./train_20240415/super_epoch_5/run_5/s-5_b-262144_r-5.tf\n",
      "with chi2 516.9240 and loss 0.1876\n",
      "starting training with batch_size: 327680 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-327680_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_0/s-5_b-327680_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 0 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-327680_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_1/s-5_b-327680_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1888 of run 1 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-327680_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-327680_r-2.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_2/s-5_b-327680_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 2 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-327680_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_3/s-5_b-327680_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1892 of run 3 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-327680_r-4.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_4/s-5_b-327680_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1887 of run 4 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-327680_r-5.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_5/s-5_b-327680_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1891 of run 5 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "starting run 6 of super_epoch 5 with batch_size 327680\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-327680_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-327680_r-6.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_5/run_6/s-5_b-327680_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 6 of super_epoch 5 with batch_size 327680\n",
      "\n",
      "calculating stats for 7 models\n",
      "\n",
      "finished 7 runs of batch_size 327680\n",
      "in super epoch 5\n",
      "with best model ./train_20240415/super_epoch_5/run_1/s-5_b-327680_r-1.tf\n",
      "with chi2 559.6434 and loss 0.1888\n",
      "no improvement, lowering learnng_rate to 0.001\n",
      "\n",
      "\n",
      "finished super_epoch 5 with 7 runs each with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 0.2151\n",
      "\n",
      "\n",
      "\n",
      "finished loop of 5 super_epochs\n",
      "with batch_sizes:[196608, 262144, 327680]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "with chi2 51.5207 and loss 0.2151\n"
     ]
    }
   ],
   "source": [
    "# start training loop\n",
    "''' train_loop() necessary arguments\n",
    "train_data, plt_data\n",
    "\n",
    "default arguments:\n",
    "model=None, lowest_chi2 = 1e6, train_dir = '/tf/home/gdrive/_STUDIUM_/DCTR_Paper/train',\n",
    "batch_sizes=[4*8192, 8*8192, 16*8192, 32*8192], repeat=5, super_epochs=35, super_patience = 5, epochs = 8, starting_super_epoch = 1, \n",
    "input_dim=5, Phi_sizes = (100,100,128), F_sizes = (128,100,100), loss = 'mse', dropout=0.0, l2_reg=0.0, \n",
    "Phi_acts=('linear', 'gelu', 'gelu'), F_acts=('gelu', 'gelu', 'linear'), output_act='sigmoid', learning_rate=0.001\n",
    "\n",
    "returns: best_model_list, lowest_chi2_list, lowest_loss_list\n",
    "'''\n",
    "best_model_list, lowest_chi2_list, _ = DCTR.train_loop(train_data, plt_data, batch_sizes=[24*8192, 32*8192, 40*8192], repeat=7, super_epochs=5,\n",
    "                                                       train_dir = train_dir, epochs=15, learning_rate=0.001)\n",
    "\n",
    "best_model = best_model_list[-1]\n",
    "lowest_chi2 = lowest_chi2_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81651\n",
      "starting super_epoch 6\n",
      "\n",
      "starting training with batch_size: 131072 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1867 of run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1866 of run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1864 of run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 131072\n",
      "in super epoch 6\n",
      "with best model ./train_20240415/super_epoch_6/run_3/s-6_b-131072_r-3.tf\n",
      "with chi2 1634.7393 and loss 0.1864\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1876 of run 0 of super_epoch 6 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1870 of run 1 of super_epoch 6 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1872 of run 2 of super_epoch 6 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1871 of run 3 of super_epoch 6 with batch_size 196608\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 196608\n",
      "in super epoch 6\n",
      "with best model ./train_20240415/super_epoch_6/run_1/s-6_b-196608_r-1.tf\n",
      "with chi2 2032.9820 and loss 0.1870\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_0/s-6_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 0 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_1/s-6_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1878 of run 1 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_2/s-6_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1879 of run 2 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_6/run_3/s-6_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1882 of run 3 of super_epoch 6 with batch_size 262144\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 262144\n",
      "in super epoch 6\n",
      "with best model ./train_20240415/super_epoch_6/run_1/s-6_b-262144_r-1.tf\n",
      "with chi2 156.4317 and loss 0.1878\n",
      "no improvement, lowering learnng_rate to 0.0007\n",
      "\n",
      "\n",
      "finished super_epoch 6 with 4 runs each with batch_sizes:[131072, 196608, 262144]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 7\n",
      "\n",
      "starting training with batch_size: 131072 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1885 of run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1867 of run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1889 of run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1877 of run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 131072\n",
      "in super epoch 7\n",
      "with best model ./train_20240415/super_epoch_7/run_2/s-7_b-131072_r-2.tf\n",
      "with chi2 2744.8065 and loss 0.1889\n",
      "starting training with batch_size: 196608 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1872 of run 0 of super_epoch 7 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-196608_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00011154777312185615.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 1 of super_epoch 7 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1870 of run 2 of super_epoch 7 with batch_size 196608\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-196608_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-196608_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1870 of run 3 of super_epoch 7 with batch_size 196608\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 196608\n",
      "in super epoch 7\n",
      "with best model ./train_20240415/super_epoch_7/run_2/s-7_b-196608_r-2.tf\n",
      "with chi2 682.0596 and loss 0.1870\n",
      "starting training with batch_size: 262144 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-262144_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-262144_r-0.tf/assets\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0001126688439399004.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_0/s-7_b-262144_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1885 of run 0 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-262144_r-1.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_1/s-7_b-262144_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1884 of run 1 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-262144_r-2.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_2/s-7_b-262144_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1883 of run 2 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 262144\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-262144_r-3.tf/assets\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00011380118667148054.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_7/run_3/s-7_b-262144_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.1877 of run 3 of super_epoch 7 with batch_size 262144\n",
      "\n",
      "calculating stats for 4 models\n",
      "\n",
      "finished 4 runs of batch_size 262144\n",
      "in super epoch 7\n",
      "with best model ./train_20240415/super_epoch_7/run_2/s-7_b-262144_r-2.tf\n",
      "with chi2 433.1657 and loss 0.1883\n",
      "no improvement, lowering learnng_rate to 0.0007\n",
      "\n",
      "\n",
      "finished super_epoch 7 with 4 runs each with batch_sizes:[131072, 196608, 262144]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 8\n",
      "\n",
      "starting training with batch_size: 131072 and 15 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m chi2_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# continue training loop with best model as starting point and smaller batch_ sizes and less repeats\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model_list, chi2_list, _ \u001b[38;5;241m=\u001b[39m \u001b[43mDCTR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0007\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_list) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# only if there was an improvement\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     best_model_list\u001b[38;5;241m.\u001b[39mappend(model_list)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:924\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# K.clear_session() # clearing before every run now\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# gc.collect() # collect garbage\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# device.reset()\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    923\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 924\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:872\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, plt_data, batch_size, repeat, epochs, super_epoch, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    869\u001b[0m x0_plt , x0_plt_nrm, x1_plt, x1_plt_wgt \u001b[38;5;241m=\u001b[39m plt_data\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:856\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    853\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/super_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    854\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_b-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 856\u001b[0m loss_val, current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m model_list\u001b[38;5;241m.\u001b[39mappend(current_model)\n\u001b[1;32m    859\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:835\u001b[0m, in \u001b[0;36mtrain_run\u001b[0;34m(model, train_data, run, super_epoch, batch_size, epochs, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, save_dir, label)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded neural network model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# train using multiprocessing to close child process every repeat to free memory from GPU \u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m best loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of super_epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with batch_size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:654\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03mmethod to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03mallows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03mplots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m#print('starting training')\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    656\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# print(f'saved model: {savePath+saveLabel}.tf')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1376\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1376\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m         epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m         step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m         _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py:1246\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1247\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1248\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m   1251\u001b[0m     original_spe)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/resource_variable_ops.py:674\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    676\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "model_list = []\n",
    "chi2_list = []\n",
    "# continue training loop with best model as starting point and smaller batch_ sizes and less repeats\n",
    "model_list, chi2_list, _ = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[16*8192, 24*8192, 32*8192], repeat=4, super_epochs=5,\n",
    "                                           starting_super_epoch=6, train_dir = train_dir, epochs=15, learning_rate=0.0007)\n",
    "\n",
    "if len(model_list) >= 1: # only if there was an improvement\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "    best_model = best_model_list[-1]\n",
    "    lowest_chi2 = lowest_chi2_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "starting super_epoch 8\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2027 of run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2023 of run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2016 of run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 65536\n",
      "in super epoch 8\n",
      "with best model ./train_20240415/super_epoch_8/run_1/s-8_b-65536_r-1.tf\n",
      "with chi2 3840.1847 and loss 0.2023\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2038 of run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2039 of run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2040 of run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 131072\n",
      "in super epoch 8\n",
      "with best model ./train_20240415/super_epoch_8/run_0/s-8_b-131072_r-0.tf\n",
      "with chi2 3287.1243 and loss 0.2038\n",
      "starting training with batch_size: 196608 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_0/s-8_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 0 of super_epoch 8 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_1/s-8_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2055 of run 1 of super_epoch 8 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_8/run_2/s-8_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 2 of super_epoch 8 with batch_size 196608\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 196608\n",
      "in super epoch 8\n",
      "with best model ./train_20240415/super_epoch_8/run_0/s-8_b-196608_r-0.tf\n",
      "with chi2 2440.7802 and loss 0.2052\n",
      "no improvement, lowering learnng_rate to 0.0005\n",
      "\n",
      "\n",
      "finished super_epoch 8 with 3 runs each with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 9\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2027 of run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2025 of run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2027 of run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 65536\n",
      "in super epoch 9\n",
      "with best model ./train_20240415/super_epoch_9/run_0/s-9_b-65536_r-0.tf\n",
      "with chi2 1839.4834 and loss 0.2027\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2035 of run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2044 of run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2038 of run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 131072\n",
      "in super epoch 9\n",
      "with best model ./train_20240415/super_epoch_9/run_0/s-9_b-131072_r-0.tf\n",
      "with chi2 738.8183 and loss 0.2035\n",
      "starting training with batch_size: 196608 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_0/s-9_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2054 of run 0 of super_epoch 9 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_1/s-9_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 1 of super_epoch 9 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_9/run_2/s-9_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2055 of run 2 of super_epoch 9 with batch_size 196608\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 196608\n",
      "in super epoch 9\n",
      "with best model ./train_20240415/super_epoch_9/run_2/s-9_b-196608_r-2.tf\n",
      "with chi2 1146.7154 and loss 0.2055\n",
      "no improvement, lowering learnng_rate to 0.0005\n",
      "\n",
      "\n",
      "finished super_epoch 9 with 3 runs each with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 10\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2026 of run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2023 of run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2030 of run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 65536\n",
      "in super epoch 10\n",
      "with best model ./train_20240415/super_epoch_10/run_0/s-10_b-65536_r-0.tf\n",
      "with chi2 1032.1906 and loss 0.2026\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00010093259916175156.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2034 of run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2039 of run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2037 of run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 131072\n",
      "in super epoch 10\n",
      "with best model ./train_20240415/super_epoch_10/run_2/s-10_b-131072_r-2.tf\n",
      "with chi2 1909.6753 and loss 0.2037\n",
      "starting training with batch_size: 196608 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_0/s-10_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2049 of run 0 of super_epoch 10 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_1/s-10_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2048 of run 1 of super_epoch 10 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_10/run_2/s-10_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 2 of super_epoch 10 with batch_size 196608\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 196608\n",
      "in super epoch 10\n",
      "with best model ./train_20240415/super_epoch_10/run_1/s-10_b-196608_r-1.tf\n",
      "with chi2 1200.5394 and loss 0.2048\n",
      "no improvement, lowering learnng_rate to 0.0005\n",
      "\n",
      "\n",
      "finished super_epoch 10 with 3 runs each with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 11\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 11 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2024 of run 0 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00010093259916175156.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2021 of run 1 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2024 of run 2 of super_epoch 11 with batch_size 65536\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 65536\n",
      "in super epoch 11\n",
      "with best model ./train_20240415/super_epoch_11/run_2/s-11_b-65536_r-2.tf\n",
      "with chi2 1297.2111 and loss 0.2024\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 11 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2041 of run 0 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2038 of run 1 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2044 of run 2 of super_epoch 11 with batch_size 131072\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 131072\n",
      "in super epoch 11\n",
      "with best model ./train_20240415/super_epoch_11/run_2/s-11_b-131072_r-2.tf\n",
      "with chi2 4160.6486 and loss 0.2044\n",
      "starting training with batch_size: 196608 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 11 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_0/s-11_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2050 of run 0 of super_epoch 11 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 11 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_1/s-11_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 1 of super_epoch 11 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 11 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_11/run_2/s-11_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2051 of run 2 of super_epoch 11 with batch_size 196608\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 196608\n",
      "in super epoch 11\n",
      "with best model ./train_20240415/super_epoch_11/run_2/s-11_b-196608_r-2.tf\n",
      "with chi2 595.5952 and loss 0.2051\n",
      "no improvement, lowering learnng_rate to 0.00035\n",
      "\n",
      "\n",
      "finished super_epoch 11 with 3 runs each with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 12\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 12 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2026 of run 0 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2020 of run 1 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2025 of run 2 of super_epoch 12 with batch_size 65536\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 65536\n",
      "in super epoch 12\n",
      "with best model ./train_20240415/super_epoch_12/run_0/s-12_b-65536_r-0.tf\n",
      "with chi2 1521.9021 and loss 0.2026\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 12 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2037 of run 0 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2039 of run 1 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2038 of run 2 of super_epoch 12 with batch_size 131072\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 131072\n",
      "in super epoch 12\n",
      "with best model ./train_20240415/super_epoch_12/run_2/s-12_b-131072_r-2.tf\n",
      "with chi2 779.6883 and loss 0.2038\n",
      "starting training with batch_size: 196608 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 12 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-196608_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-196608_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_0/s-12_b-196608_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2052 of run 0 of super_epoch 12 with batch_size 196608\n",
      "\n",
      "starting run 1 of super_epoch 12 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-196608_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_1/s-12_b-196608_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2053 of run 1 of super_epoch 12 with batch_size 196608\n",
      "\n",
      "starting run 2 of super_epoch 12 with batch_size 196608\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-196608_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-196608_r-2.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_12/run_2/s-12_b-196608_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2048 of run 2 of super_epoch 12 with batch_size 196608\n",
      "\n",
      "calculating stats for 3 models\n",
      "\n",
      "finished 3 runs of batch_size 196608\n",
      "in super epoch 12\n",
      "with best model ./train_20240415/super_epoch_12/run_1/s-12_b-196608_r-1.tf\n",
      "with chi2 1755.4670 and loss 0.2053\n",
      "no improvement, lowering learnng_rate to 0.000245\n",
      "\n",
      "\n",
      "finished super_epoch 12 with 3 runs each with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "\n",
      "\n",
      "\n",
      "finished loop of 5 super_epochs\n",
      "with batch_sizes:[65536, 131072, 196608]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "with chi2 51.5207 and loss 1.0000\n"
     ]
    }
   ],
   "source": [
    "# stopped block before early, and changed the starting super epoch to match the actually completed super epochs\n",
    "    # stopped b/c results were not great after 7 super_epochs \n",
    "        # -> balanced high pt exp weights with regular event gen weights (begore regular weights were only 20% as important)\n",
    "\n",
    "best_model = './train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf'\n",
    "lowest_chi2 = 51.5207\n",
    "\n",
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "model_list = []\n",
    "chi2_list = []\n",
    "# continue training loop with best model as starting point and smaller batch_sizes, less repeats and lower learning_rate\n",
    "model_list, chi2_list, _ = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[8*8192, 16*8192, 24*8192], repeat=3, super_epochs=5,\n",
    "                                           starting_super_epoch=8, train_dir = train_dir, epochs=25, learning_rate=0.0005)\n",
    "\n",
    "\n",
    "if len(model_list) >= 1:\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "    best_model = best_model_list[-1]\n",
    "    lowest_chi2 = lowest_chi2_list[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111668\n",
      "starting super_epoch 13\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 13 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_0/s-13_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_0/s-13_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2022 of run 0 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_1/s-13_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_1/s-13_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2025 of run 1 of super_epoch 13 with batch_size 65536\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 65536\n",
      "in super epoch 13\n",
      "with best model ./train_20240415/super_epoch_13/run_0/s-13_b-65536_r-0.tf\n",
      "with chi2 1935.8866 and loss 0.2022\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 13 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_0/s-13_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_0/s-13_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2039 of run 0 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 13 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_1/s-13_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_13/run_1/s-13_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2044 of run 1 of super_epoch 13 with batch_size 131072\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 131072\n",
      "in super epoch 13\n",
      "with best model ./train_20240415/super_epoch_13/run_1/s-13_b-131072_r-1.tf\n",
      "with chi2 3319.3011 and loss 0.2044\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 13 with 2 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 14\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 14 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_0/s-14_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_0/s-14_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2031 of run 0 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_1/s-14_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_1/s-14_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2022 of run 1 of super_epoch 14 with batch_size 65536\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 65536\n",
      "in super epoch 14\n",
      "with best model ./train_20240415/super_epoch_14/run_1/s-14_b-65536_r-1.tf\n",
      "with chi2 3859.1378 and loss 0.2022\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 14 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_0/s-14_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_0/s-14_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2039 of run 0 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 14 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_1/s-14_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_14/run_1/s-14_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2035 of run 1 of super_epoch 14 with batch_size 131072\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 131072\n",
      "in super epoch 14\n",
      "with best model ./train_20240415/super_epoch_14/run_0/s-14_b-131072_r-0.tf\n",
      "with chi2 3228.3353 and loss 0.2039\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 14 with 2 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 15\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 15 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_0/s-15_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_0/s-15_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2029 of run 0 of super_epoch 15 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 15 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_1/s-15_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_1/s-15_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2028 of run 1 of super_epoch 15 with batch_size 65536\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 65536\n",
      "in super epoch 15\n",
      "with best model ./train_20240415/super_epoch_15/run_0/s-15_b-65536_r-0.tf\n",
      "with chi2 3549.4971 and loss 0.2029\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 15 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_0/s-15_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_0/s-15_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_0/s-15_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2037 of run 0 of super_epoch 15 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 15 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_1/s-15_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_15/run_1/s-15_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2037 of run 1 of super_epoch 15 with batch_size 131072\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 131072\n",
      "in super epoch 15\n",
      "with best model ./train_20240415/super_epoch_15/run_0/s-15_b-131072_r-0.tf\n",
      "with chi2 1615.4116 and loss 0.2037\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 15 with 2 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 16\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 16 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_0/s-16_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_0/s-16_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_0/s-16_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2020 of run 0 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2017 of run 1 of super_epoch 16 with batch_size 65536\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 65536\n",
      "in super epoch 16\n",
      "with best model ./train_20240415/super_epoch_16/run_1/s-16_b-65536_r-1.tf\n",
      "with chi2 1008.2078 and loss 0.2017\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 16 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_0/s-16_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_0/s-16_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2040 of run 0 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 16 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_16/run_1/s-16_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2035 of run 1 of super_epoch 16 with batch_size 131072\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 131072\n",
      "in super epoch 16\n",
      "with best model ./train_20240415/super_epoch_16/run_0/s-16_b-131072_r-0.tf\n",
      "with chi2 3559.3289 and loss 0.2040\n",
      "no improvement, lowering learnng_rate to 0.00020999999999999998\n",
      "\n",
      "\n",
      "finished super_epoch 16 with 2 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 17\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 17 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_0/s-17_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_0/s-17_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_0/s-17_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2021 of run 0 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010297157568857074.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2020 of run 1 of super_epoch 17 with batch_size 65536\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 65536\n",
      "in super epoch 17\n",
      "with best model ./train_20240415/super_epoch_17/run_0/s-17_b-65536_r-0.tf\n",
      "with chi2 1974.5304 and loss 0.2021\n",
      "starting training with batch_size: 131072 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 17 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_0/s-17_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_0/s-17_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2041 of run 0 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 17 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010505172831472009.\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_17/run_1/s-17_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2038 of run 1 of super_epoch 17 with batch_size 131072\n",
      "\n",
      "calculating stats for 2 models\n",
      "\n",
      "finished 2 runs of batch_size 131072\n",
      "in super epoch 17\n",
      "with best model ./train_20240415/super_epoch_17/run_1/s-17_b-131072_r-1.tf\n",
      "with chi2 1171.2879 and loss 0.2038\n",
      "no improvement, lowering learnng_rate to 0.00014699999999999997\n",
      "\n",
      "\n",
      "finished super_epoch 17 with 2 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 18\n",
      "\n",
      "starting training with batch_size: 65536 and 25 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 18 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240415/super_epoch_18/run_0/s-18_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(gc\u001b[38;5;241m.\u001b[39mcollect()) \u001b[38;5;66;03m# cpu gabage collection\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_list, chi2_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mDCTR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_list) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     10\u001b[0m     best_model_list\u001b[38;5;241m.\u001b[39mappend(model_list)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:924\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# K.clear_session() # clearing before every run now\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# gc.collect() # collect garbage\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# device.reset()\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    923\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 924\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:872\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, plt_data, batch_size, repeat, epochs, super_epoch, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    869\u001b[0m x0_plt , x0_plt_nrm, x1_plt, x1_plt_wgt \u001b[38;5;241m=\u001b[39m plt_data\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:856\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    853\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/super_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    854\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_b-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 856\u001b[0m loss_val, current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m model_list\u001b[38;5;241m.\u001b[39mappend(current_model)\n\u001b[1;32m    859\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:835\u001b[0m, in \u001b[0;36mtrain_run\u001b[0;34m(model, train_data, run, super_epoch, batch_size, epochs, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, save_dir, label)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded neural network model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# train using multiprocessing to close child process every repeat to free memory from GPU \u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m best loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of super_epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with batch_size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:654\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03mmethod to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03mallows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03mplots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m#print('starting training')\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    656\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# print(f'saved model: {savePath+saveLabel}.tf')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1376\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1376\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m         epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m         step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m         _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py:1246\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1246\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1247\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1248\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m   1251\u001b[0m     original_spe)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/resource_variable_ops.py:674\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    676\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\n",
    "model_list, chi2_list, loss_list = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[8*8192, 16*8192], repeat=2, super_epochs=10, \n",
    "                                                   starting_super_epoch=13, train_dir = train_dir, epochs=25, learning_rate=0.0003, dropout=0.1)\n",
    "\n",
    "\n",
    "if len(model_list) >= 1:\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "best_model = best_model_list[-1]\n",
    "lowest_chi2 = lowest_chi2_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training above was leading nowhere\n",
    "# restarted training with '../best_model.tf' as starting point \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140545\n",
      "starting super_epoch 1\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 9.490818047197535e-06.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 5.256677286524791e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2366 of run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.302887701778672e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2367 of run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.396383029525169e-06.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 5.20437224622583e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2367 of run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.3095078328624364e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2367 of run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 9.21032224141527e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2367 of run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 65536\n",
      "in super epoch 1\n",
      "with best model ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "with chi2 25.1403 and loss 0.2367\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2370 of run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.302887701778672e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2369 of run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.3095078328624364e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2369 of run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2369 of run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_1/run_4/s-1_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2369 of run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 131072\n",
      "in super epoch 1\n",
      "with best model ./train_20240416/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "with chi2 26.9015 and loss 0.2369\n",
      "\n",
      "\n",
      "finished super_epoch 1 with 5 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tfwith chi2 25.1403 and loss 0.2367\n",
      "starting super_epoch 2\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 65536\n",
      "in super epoch 2\n",
      "with best model ./train_20240416/super_epoch_2/run_2/s-2_b-65536_r-2.tf\n",
      "with chi2 32.0494 and loss 0.2362\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_0/s-2_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.0915528441255445e-06.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.7123179077316308e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 131072\n",
      "in super epoch 2\n",
      "with best model ./train_20240416/super_epoch_2/run_4/s-2_b-131072_r-4.tf\n",
      "with chi2 31.7115 and loss 0.2362\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 2 with 5 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tfwith chi2 25.1403 and loss 0.2367\n",
      "starting super_epoch 3\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.7295270026806973e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.030336029041791e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_4/s-3_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_4/s-3_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 65536\n",
      "in super epoch 3\n",
      "with best model ./train_20240416/super_epoch_3/run_2/s-3_b-65536_r-2.tf\n",
      "with chi2 30.8865 and loss 0.2362\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_4/s-3_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_3/run_4/s-3_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 131072\n",
      "in super epoch 3\n",
      "with best model ./train_20240416/super_epoch_3/run_4/s-3_b-131072_r-4.tf\n",
      "with chi2 31.3099 and loss 0.2363\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 3 with 5 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tfwith chi2 25.1403 and loss 0.2367\n",
      "starting super_epoch 4\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.0915528441255445e-06.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.7123179077316308e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.7295270026806973e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 65536\n",
      "in super epoch 4\n",
      "with best model ./train_20240416/super_epoch_4/run_1/s-4_b-65536_r-1.tf\n",
      "with chi2 29.9372 and loss 0.2362\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_1/s-4_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 131072\n",
      "in super epoch 4\n",
      "with best model ./train_20240416/super_epoch_4/run_0/s-4_b-131072_r-0.tf\n",
      "with chi2 31.6451 and loss 0.2363\n",
      "no improvement, lowering learnng_rate to 0.0003\n",
      "\n",
      "\n",
      "finished super_epoch 4 with 5 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tfwith chi2 25.1403 and loss 0.2367\n",
      "starting super_epoch 5\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.7295270026806973e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.7295270026806973e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_3/s-5_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_3/s-5_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2362 of run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "calculating stats for 5 models\n",
      "\n",
      "finished 5 runs of batch_size 65536\n",
      "in super epoch 5\n",
      "with best model ./train_20240416/super_epoch_5/run_0/s-5_b-65536_r-0.tf\n",
      "with chi2 31.0356 and loss 0.2362\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 3.122623456874862e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.7469090835220412e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2363 of run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(gc\u001b[38;5;241m.\u001b[39mcollect()) \u001b[38;5;66;03m# cpu gabage collection\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_list, chi2_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mDCTR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_list) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     16\u001b[0m     best_model_list\u001b[38;5;241m.\u001b[39mappend(model_list)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:924\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# K.clear_session() # clearing before every run now\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# gc.collect() # collect garbage\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# device.reset()\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    923\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 924\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:872\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, plt_data, batch_size, repeat, epochs, super_epoch, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    869\u001b[0m x0_plt , x0_plt_nrm, x1_plt, x1_plt_wgt \u001b[38;5;241m=\u001b[39m plt_data\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# train and get list of model model\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m model_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# stats\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:856\u001b[0m, in \u001b[0;36mtrain_super_epoch\u001b[0;34m(model, train_data, batch_size, repeat, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, epochs, super_epoch)\u001b[0m\n\u001b[1;32m    853\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/super_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    854\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_b-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_r-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 856\u001b[0m loss_val, current_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m model_list\u001b[38;5;241m.\u001b[39mappend(current_model)\n\u001b[1;32m    859\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:835\u001b[0m, in \u001b[0;36mtrain_run\u001b[0;34m(model, train_data, run, super_epoch, batch_size, epochs, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate, save_dir, label)\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloaded neural network model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# train using multiprocessing to close child process every repeat to free memory from GPU \u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdctr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwgt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavePath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaveLabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m best loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_val\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of super_epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuper_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with batch_size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:654\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dctr, callbacks, X_train, Y_train, X_val, Y_val, wgt_train, wgt_val, epochs, batch_size, savePath, saveLabel, verbose, plot)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03mmethod to train the given dctr Neural Network with the X_train/Y_train arrays and validate the predictions with X_val and Y_val\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03mallows for passing along sample_weights for training and validation. These can be positive and/or negative. If no wgt_train or wgt_val are given, then the weights are set to 1 by default\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03mplots and saves a figure of loss and accuracy throughout the Epochs\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m#print('starting training')\u001b[39;00m\n\u001b[0;32m--> 654\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mdctr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# pd.Series makes the training initialize much, much faster than passing just the weight\u001b[39;49;00m\n\u001b[1;32m    656\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwgt_val\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_v\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m dctr\u001b[38;5;241m.\u001b[39msave(savePath\u001b[38;5;241m+\u001b[39msaveLabel\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# print(f'saved model: {savePath+saveLabel}.tf')\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dir = './train_20240416' # where to save models during training\n",
    "# using previous training as starting point\n",
    "best_model = '../best_model.tf'\n",
    "lowest_chi2 = 51.5207\n",
    "\n",
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\n",
    "model_list, chi2_list, loss_list = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[8*8192, 16*8192], repeat=5, super_epochs=10, \n",
    "                                                   starting_super_epoch=1, train_dir = train_dir, epochs=20, learning_rate=0.0003, dropout=0.15)\n",
    "\n",
    "\n",
    "if len(model_list) >= 1:\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "best_model = best_model_list[-1]\n",
    "lowest_chi2 = lowest_chi2_list[-1]\n",
    "\n",
    "# best chi2: 25.1403\n",
    "    # ./train_20240416/super_epoch_1/run_2/s-1_b-65536_r-2.tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31244\n",
      "starting super_epoch 1\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_0/s-1_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_1/s-1_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_6/s-1_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_6/s-1_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 1 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_7/s-1_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_7/s-1_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_7/s-1_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 1 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 1\n",
      "with best model ./train_20240416_b/super_epoch_1/run_4/s-1_b-65536_r-4.tf\n",
      "with chi2 14.7910 and loss 0.2374\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_0/s-1_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_1/s-1_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_2/s-1_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_3/s-1_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 3 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_4/s-1_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 4 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_5/s-1_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_6/s-1_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_6/s-1_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 1 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_7/s-1_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_1/run_7/s-1_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 1 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 1\n",
      "with best model ./train_20240416_b/super_epoch_1/run_1/s-1_b-131072_r-1.tf\n",
      "with chi2 14.4234 and loss 0.2374\n",
      "no improvement, lowering learnng_rate to 0.0002\n",
      "\n",
      "\n",
      "finished super_epoch 1 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tfwith chi2 6.0000 and loss 1.0000\n",
      "starting super_epoch 2\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_0/s-2_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.3095078328624364e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 9.396383029525169e-06.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 5.20437224622583e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 2 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 2 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 2\n",
      "with best model ./train_20240416_b/super_epoch_2/run_0/s-2_b-65536_r-0.tf\n",
      "with chi2 11.6268 and loss 0.2374\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_0/s-2_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_0/s-2_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_1/s-2_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_2/s-2_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_3/s-2_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 3 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_4/s-2_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_5/s-2_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_6/s-2_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 6 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 2 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_2/run_7/s-2_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 2 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 2\n",
      "with best model ./train_20240416_b/super_epoch_2/run_2/s-2_b-131072_r-2.tf\n",
      "with chi2 15.2482 and loss 0.2374\n",
      "no improvement, lowering learnng_rate to 0.0002\n",
      "\n",
      "\n",
      "finished super_epoch 2 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tfwith chi2 6.0000 and loss 1.0000\n",
      "starting super_epoch 3\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_0/s-3_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_2/s-3_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_2/s-3_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 3 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_7/s-3_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_7/s-3_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_7/s-3_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 3 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 3\n",
      "with best model ./train_20240416_b/super_epoch_3/run_6/s-3_b-65536_r-6.tf\n",
      "with chi2 12.1347 and loss 0.2374\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_0/s-3_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_1/s-3_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_2/s-3_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_3/s-3_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_4/s-3_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.3095078328624364e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_5/s-3_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_6/s-3_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 3 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_7/s-3_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_3/run_7/s-3_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 3 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 3\n",
      "with best model ./train_20240416_b/super_epoch_3/run_4/s-3_b-131072_r-4.tf\n",
      "with chi2 13.1281 and loss 0.2374\n",
      "no improvement, lowering learnng_rate to 0.0002\n",
      "\n",
      "\n",
      "finished super_epoch 3 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tfwith chi2 6.0000 and loss 1.0000\n",
      "starting super_epoch 4\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_3/s-4_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_3/s-4_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_4/s-4_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_6/s-4_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_6/s-4_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_6/s-4_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 4 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 9.302887701778672e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 4 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 4\n",
      "with best model ./train_20240416_b/super_epoch_4/run_2/s-4_b-65536_r-2.tf\n",
      "with chi2 13.0084 and loss 0.2374\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_0/s-4_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 0 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_1/s-4_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_2/s-4_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_3/s-4_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_4/s-4_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_5/s-4_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 5 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_6/s-4_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_6/s-4_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 4 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_4/run_7/s-4_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 4 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 4\n",
      "with best model ./train_20240416_b/super_epoch_4/run_2/s-4_b-131072_r-2.tf\n",
      "with chi2 14.9533 and loss 0.2374\n",
      "no improvement, lowering learnng_rate to 0.00014\n",
      "\n",
      "\n",
      "finished super_epoch 4 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tfwith chi2 6.0000 and loss 1.0000\n",
      "starting super_epoch 5\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 5.3095078328624364e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_2/s-5_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 3 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 5 with batch_size 65536\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 5 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 5\n",
      "with best model ./train_20240416_b/super_epoch_5/run_5/s-5_b-65536_r-5.tf\n",
      "with chi2 14.2551 and loss 0.2374\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ../best_model.tf\n",
      "starting run 0 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_0/s-5_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 0 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_1/s-5_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 1 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_2/s-5_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_2/s-5_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 2 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_3/s-5_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2375 of run 3 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_4/s-5_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 4 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_5/s-5_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 5 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_6/s-5_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 6 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 5 with batch_size 131072\n",
      "loaded neural network model: ../best_model.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 9.586202577338553e-06.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 5.3628693422069775e-06.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_5/run_7/s-5_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2374 of run 7 of super_epoch 5 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 5\n",
      "with best model ./train_20240416_b/super_epoch_5/run_4/s-5_b-131072_r-4.tf\n",
      "with chi2 13.9300 and loss 0.2374\n",
      "no improvement, lowering learnng_rate to 9.799999999999998e-05\n",
      "\n",
      "\n",
      "finished super_epoch 5 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tfwith chi2 6.0000 and loss 1.0000\n",
      "\n",
      "\n",
      "\n",
      "finished loop of 5 super_epochs\n",
      "with batch_sizes:[65536, 131072]\n",
      "best model../best_model.tf\n",
      "with chi2 6.0000 and loss 1.0000\n"
     ]
    }
   ],
   "source": [
    "# again with much more important regular events (non exp weights; from 0.2x to 1x and now to 10x)\n",
    "    # since the above training was not working well enough for the bulk\n",
    "\n",
    "train_dir = './train_20240416_b' # where to save models during training\n",
    "# using previous training as starting point\n",
    "best_model = '../best_model.tf'\n",
    "lowest_chi2 = 6 # only use new models when significant progress is made | Base model has chi2 ~2; first epoch above had 25; which is not good enough to base subsequent runs on\n",
    "\n",
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\n",
    "model_list, chi2_list, loss_list = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[8*8192, 16*8192], repeat=8, super_epochs=5, \n",
    "                                                   starting_super_epoch=1, train_dir = train_dir, epochs=20, learning_rate=0.0002, dropout=0.33333) # high dropout | maybe it will help against overfitting high pT\n",
    "\n",
    "\n",
    "if len(model_list) >= 1:\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "best_model = best_model_list[-1]\n",
    "lowest_chi2 = lowest_chi2_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81651\n",
      "starting super_epoch 6\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 0 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 1 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 2 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 3 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 4 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 5 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2324 of run 6 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 6 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 7 of super_epoch 6 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 6\n",
      "with best model ./train_20240416_b/super_epoch_6/run_1/s-6_b-65536_r-1.tf\n",
      "with chi2 322.7026 and loss 0.2330\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_0/s-6_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 0 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_1/s-6_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 1 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_2/s-6_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 2 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_3/s-6_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 3 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_4/s-6_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 4 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_5/s-6_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 5 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_6/s-6_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 6 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 6 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_6/run_7/s-6_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 7 of super_epoch 6 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 6\n",
      "with best model ./train_20240416_b/super_epoch_6/run_5/s-6_b-131072_r-5.tf\n",
      "with chi2 298.1386 and loss 0.2329\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 6 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 7\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 0 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 1 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 2 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 3 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 4 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 5 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00018778140074573457.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2331 of run 6 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 7 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00018778140074573457.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 7 of super_epoch 7 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 7\n",
      "with best model ./train_20240416_b/super_epoch_7/run_0/s-7_b-65536_r-0.tf\n",
      "with chi2 1237.4926 and loss 0.2330\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_0/s-7_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 0 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00018591295229271053.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_1/s-7_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 1 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_2/s-7_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 2 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00018778140074573457.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_3/s-7_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 3 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_4/s-7_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 4 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_5/s-7_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 5 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_6/s-7_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 6 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 7 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_7/run_7/s-7_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 7 of super_epoch 7 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 7\n",
      "with best model ./train_20240416_b/super_epoch_7/run_7/s-7_b-131072_r-7.tf\n",
      "with chi2 550.9608 and loss 0.2327\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 7 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 8\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 0 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 1 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_2/s-8_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_2/s-8_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 2 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 3 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 4 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 5 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 6 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 8 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2324 of run 7 of super_epoch 8 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 8\n",
      "with best model ./train_20240416_b/super_epoch_8/run_1/s-8_b-65536_r-1.tf\n",
      "with chi2 317.5566 and loss 0.2325\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_0/s-8_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 0 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_1/s-8_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 1 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_2/s-8_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_2/s-8_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_2/s-8_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 2 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_3/s-8_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2332 of run 3 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_4/s-8_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 4 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_5/s-8_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 5 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_6/s-8_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 6 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 8 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_8/run_7/s-8_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 7 of super_epoch 8 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 8\n",
      "with best model ./train_20240416_b/super_epoch_8/run_5/s-8_b-131072_r-5.tf\n",
      "with chi2 487.5972 and loss 0.2328\n",
      "no improvement, lowering learnng_rate to 0.0001\n",
      "\n",
      "\n",
      "finished super_epoch 8 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 9\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 0 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 1 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2324 of run 2 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-65536_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00018778140074573457.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 3 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 4 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 5 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 6 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 9 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 7 of super_epoch 9 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 9\n",
      "with best model ./train_20240416_b/super_epoch_9/run_5/s-9_b-65536_r-5.tf\n",
      "with chi2 188.6535 and loss 0.2326\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_0/s-9_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 0 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_1/s-9_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 1 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_2/s-9_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 2 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_3/s-9_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 3 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_4/s-9_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 4 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_5/s-9_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 5 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_6/s-9_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 6 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 9 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_9/run_7/s-9_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 7 of super_epoch 9 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 131072\n",
      "in super epoch 9\n",
      "with best model ./train_20240416_b/super_epoch_9/run_7/s-9_b-131072_r-7.tf\n",
      "with chi2 442.3682 and loss 0.2330\n",
      "no improvement, lowering learnng_rate to 7e-05\n",
      "\n",
      "\n",
      "finished super_epoch 9 with 8 runs each with batch_sizes:[65536, 131072]\n",
      "best model./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tfwith chi2 51.5207 and loss 1.0000\n",
      "starting super_epoch 10\n",
      "\n",
      "starting training with batch_size: 65536 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.00018778140074573457.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-65536_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 0 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00019157484057359396.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010610752506181597.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-65536_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 1 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-65536_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-65536_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-65536_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 2 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-65536_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-65536_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 3 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-65536_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-65536_r-4.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-65536_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2326 of run 4 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 5 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-65536_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-65536_r-5.tf/assets\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001935001986566931.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010717391269281507.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-65536_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 5 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 6 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-65536_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-65536_r-6.tf/assets\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00018966863863170146.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-65536_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2330 of run 6 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "starting run 7 of super_epoch 10 with batch_size 65536\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-65536_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-65536_r-7.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-65536_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2325 of run 7 of super_epoch 10 with batch_size 65536\n",
      "\n",
      "calculating stats for 8 models\n",
      "\n",
      "finished 8 runs of batch_size 65536\n",
      "in super epoch 10\n",
      "with best model ./train_20240416_b/super_epoch_10/run_0/s-10_b-65536_r-0.tf\n",
      "with chi2 1020.4052 and loss 0.2329\n",
      "starting training with batch_size: 131072 and 20 epochs\n",
      "starting with weights from model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "starting run 0 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_0/s-10_b-131072_r-0.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 0 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 1 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-131072_r-1.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-131072_r-1.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_1/s-10_b-131072_r-1.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 1 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 2 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_2/s-10_b-131072_r-2.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 2 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 3 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-131072_r-3.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-131072_r-3.tf/assets\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010933897574432195.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_3/s-10_b-131072_r-3.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2329 of run 3 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 4 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-131072_r-4.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-131072_r-4.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_4/s-10_b-131072_r-4.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 4 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 5 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-131072_r-5.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-131072_r-5.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_5/s-10_b-131072_r-5.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2327 of run 5 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 6 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-131072_r-6.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-131072_r-6.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_6/s-10_b-131072_r-6.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 6 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "starting run 7 of super_epoch 10 with batch_size 131072\n",
      "loaded neural network model: ./train_20240415/super_epoch_2/run_4/s-2_b-262144_r-4.tf\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-131072_r-7.tf/assets\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-131072_r-7.tf/assets\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00019544490496627986.\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.00010825103963725269.\n",
      "INFO:tensorflow:Assets written to: ./train_20240416_b/super_epoch_10/run_7/s-10_b-131072_r-7.tf/assets\n",
      "clearing keras session and collecting garbage\n",
      "\n",
      " best loss 0.2328 of run 7 of super_epoch 10 with batch_size 131072\n",
      "\n",
      "calculating stats for 8 models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(gc\u001b[38;5;241m.\u001b[39mcollect()) \u001b[38;5;66;03m# cpu gabage collection\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_list, chi2_list, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mDCTR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowest_chi2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowest_chi2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mstarting_super_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# low dropout\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_list) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     10\u001b[0m     best_model_list\u001b[38;5;241m.\u001b[39mappend(model_list)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:924\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_data, plt_data, model, lowest_chi2, train_dir, batch_sizes, repeat, super_epochs, super_patience, epochs, starting_super_epoch, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# K.clear_session() # clearing before every run now\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# gc.collect() # collect garbage\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# device.reset()\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting training with batch_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    923\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstarting with weights from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 924\u001b[0m     batch_model, min_chi2, chi2_mean_list, min_loss, loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_super_epoch_choose_best\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPhi_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43mPhi_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPhi_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_acts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mF_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_act\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m                                                                                               \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;66;03m# save chi2, loss for each run to disk\u001b[39;00m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chi2_mean_list)): \u001b[38;5;66;03m# one entry for each run, plus baseline (needs to be ignored) x1 as first entry\u001b[39;00m\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:876\u001b[0m, in \u001b[0;36mtrain_super_epoch_choose_best\u001b[0;34m(model, train_data, plt_data, batch_size, repeat, epochs, super_epoch, train_dir, input_dim, Phi_sizes, F_sizes, loss, dropout, l2_reg, Phi_acts, F_acts, output_act, learning_rate)\u001b[0m\n\u001b[1;32m    874\u001b[0m rwgt_list\u001b[38;5;241m=\u001b[39m get_rwgt(model_list, x0_plt_nrm)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# stats\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m mae_mean_list, chi2_mean_list, p_mean_list \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrwgt_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplt_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m min_chi2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(chi2_mean_list[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;66;03m# first is always the baseline x1, always has chi2=0 and is only used for calculating the other statistics\u001b[39;00m\n\u001b[1;32m    879\u001b[0m best_where \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(chi2_mean_list \u001b[38;5;241m==\u001b[39m min_chi2)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:781\u001b[0m, in \u001b[0;36mcalc_stats\u001b[0;34m(rwgt_list, plt_data, part_indices, arg_indices, stats_only, verbose)\u001b[0m\n\u001b[1;32m    778\u001b[0m     start \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    779\u001b[0m     stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m\n\u001b[0;32m--> 781\u001b[0m mae_list, chi2_list, p_list \u001b[38;5;241m=\u001b[39m \u001b[43mplot_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# print(chi2_list)\u001b[39;00m\n\u001b[1;32m    783\u001b[0m mae_all\u001b[38;5;241m.\u001b[39mappend(mae_list)\n",
      "File \u001b[0;32m/tf/home/gdrive/_STUDIUM_/DCTR_Paper/git/DCTR_FP/testing/../DCTR.py:1135\u001b[0m, in \u001b[0;36mplot_ratio\u001b[0;34m(args, arg_index, part_index, title, x_label, y_label, bins, start, stop, div, ratio_ylim, figsize, layout, stats_only, y_scale, verbose)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# check wheter full dataset is passed or 1D dataset, that can be plotted as is\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1135\u001b[0m     n, bins_ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     bin_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdigitize(X[:,part_index, arg_index], bins \u001b[38;5;241m=\u001b[39m bins)\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;66;03m# num_evts = len(X[:,0,0])\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;66;03m# ks_statistic, ks_p_value = stats.ks_2samp(args[0][0][:, part_index, arg_index], X[:, part_index, arg_index])\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/histograms.py:885\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    883\u001b[0m tmp_a \u001b[38;5;241m=\u001b[39m a[i:i\u001b[38;5;241m+\u001b[39mBLOCK]\n\u001b[1;32m    884\u001b[0m tmp_w \u001b[38;5;241m=\u001b[39m weights[i:i\u001b[38;5;241m+\u001b[39mBLOCK]\n\u001b[0;32m--> 885\u001b[0m sorting_index \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m sa \u001b[38;5;241m=\u001b[39m tmp_a[sorting_index]\n\u001b[1;32m    887\u001b[0m sw \u001b[38;5;241m=\u001b[39m tmp_w[sorting_index]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:1120\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session() \n",
    "print(gc.collect()) # cpu gabage collection\n",
    "\n",
    "# finish training loop with best model as starting point and smaller batch_sizes and lower learning_rate and 10% dropout\n",
    "model_list, chi2_list, loss_list = DCTR.train_loop(train_data, plt_data, model=best_model, lowest_chi2=lowest_chi2, batch_sizes=[8*8192, 16*8192], repeat=8, super_epochs=5, \n",
    "                                                   starting_super_epoch=6, train_dir = train_dir, epochs=20, learning_rate=0.0001, dropout=0.05) # low dropout\n",
    "\n",
    "\n",
    "if len(model_list) >= 1:\n",
    "    best_model_list.append(model_list)\n",
    "    lowest_chi2_list.append(chi2_list)\n",
    "\n",
    "best_model = best_model_list[-1]\n",
    "lowest_chi2 = lowest_chi2_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
